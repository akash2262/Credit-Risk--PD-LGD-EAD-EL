{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoj9wjQbuDYI"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNM3Ar9juDYK"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CldCZHBDuDYL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl4-lFhRuDYM"
      },
      "source": [
        "## Import Data\n",
        "The dataset contains all available data for more than 800,000 consumer loans issued from 2007 to 2015 by Lending Club: a large US peer-to-peer lending company. There are several different versions of this dataset. We have used a version available on kaggle.com. You can find it here: https://www.kaggle.com/wendykan/lending-club-loan-data/version/1\n",
        "We divided the data into two periods because we assume that some data are available at the moment when we need to build Expected Loss models, and some data comes from applications after. Later, we investigate whether the applications we have after we built the Probability of Default (PD) model have similar characteristics with the applications we used to build the PD model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2DSgbLdouDYM",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "95d8778a-1155-4652-ba79-a74dc01284d9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7f0b60224b64>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloan_data_backup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/loan_data_2007_2014.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m     \"\"\"\n\u001b[1;32m   1435\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loan_data_backup = pd.read_csv('/content/loan_data_2007_2014.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqGefnCwuDYN"
      },
      "outputs": [],
      "source": [
        "loan_data = loan_data_backup.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO8bRbQcuDYN"
      },
      "source": [
        "## Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOYLILkkuDYO"
      },
      "outputs": [],
      "source": [
        "loan_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztVPqrl7uDYO"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_columns = None\n",
        "#pd.options.display.max_rows = None\n",
        "# Sets the pandas dataframe options to display all columns/ rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b5cXJpLuDYO"
      },
      "outputs": [],
      "source": [
        "loan_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQEsy9y0uDYP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "loan_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "187lB6WKuDYP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "loan_data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYfvRXOquDYQ"
      },
      "outputs": [],
      "source": [
        "loan_data.columns.values\n",
        "# Displays all column names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0WQP0_XuDYQ"
      },
      "outputs": [],
      "source": [
        "loan_data.info()\n",
        "# Displays column names, complete (non-missing) cases per column, and datatype per column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGCUbjkKuDYR"
      },
      "source": [
        "## General Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IggOkqmkuDYR"
      },
      "source": [
        "### Preprocessing few continuous variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-CzMLafuDYR"
      },
      "outputs": [],
      "source": [
        "loan_data['emp_length'].unique()\n",
        "# Displays unique values of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0npvl_oeuDYS"
      },
      "outputs": [],
      "source": [
        "loan_data['emp_length_int'] = loan_data['emp_length'].str.replace('\\+ years', '')\n",
        "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('< 1 year', str(0))\n",
        "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('n/a',  str(0))\n",
        "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace(' years', '')\n",
        "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace(' year', '')\n",
        "# We store the preprocessed ‘employment length’ variable in a new variable called ‘employment length int’,\n",
        "# We assign the new ‘employment length int’ to be equal to the ‘employment length’ variable with the string ‘+ years’\n",
        "# replaced with nothing. Next, we replace the whole string ‘less than 1 year’ with the string ‘0’.\n",
        "# Then, we replace the ‘n/a’ string with the string ‘0’. Then, we replace the string ‘space years’ with nothing.\n",
        "# Finally, we replace the string ‘space year’ with nothing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukl2lD7GuDYS"
      },
      "outputs": [],
      "source": [
        "type(loan_data['emp_length_int'][0])\n",
        "# Checks the datatype of a single element of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHhOdLr2uDYT"
      },
      "outputs": [],
      "source": [
        "loan_data['emp_length_int'] = pd.to_numeric(loan_data['emp_length_int'])\n",
        "# Transforms the values to numeric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVXmGrcbuDYT"
      },
      "outputs": [],
      "source": [
        "type(loan_data['emp_length_int'][0])\n",
        "# Checks the datatype of a single element of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR9gXbcJuDYT"
      },
      "outputs": [],
      "source": [
        "loan_data['earliest_cr_line']\n",
        "# Displays a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQHTJviruDYU"
      },
      "outputs": [],
      "source": [
        "loan_data['earliest_cr_line_date'] = pd.to_datetime(loan_data['earliest_cr_line'], format = '%b-%y')\n",
        "# Extracts the date and the time from a string variable that is in a given format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QN1weisWuDYU",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "type(loan_data['earliest_cr_line_date'][0])\n",
        "# Checks the datatype of a single element of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVY0depGuDYU"
      },
      "outputs": [],
      "source": [
        "pd.to_datetime('2017-12-01') - loan_data['earliest_cr_line_date']\n",
        "# Calculates the difference between two dates and times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLA-RoJkuDYV"
      },
      "outputs": [],
      "source": [
        "# Assume we are now in December 2017\n",
        "loan_data['mths_since_earliest_cr_line'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - loan_data['earliest_cr_line_date']) / np.timedelta64(1, 'M')))\n",
        "# We calculate the difference between two dates in months, turn it to numeric datatype and round it.\n",
        "# We save the result in a new variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJnkNxonuDYV"
      },
      "outputs": [],
      "source": [
        "loan_data['mths_since_earliest_cr_line'].describe()\n",
        "# Shows some descriptive statisics for the values of a column.\n",
        "# Dates from 1969 and before are not being converted well, i.e., they have become 2069 and similar,\n",
        "# and negative differences are being calculated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vBqIynOuDYV"
      },
      "outputs": [],
      "source": [
        "loan_data.loc[: , ['earliest_cr_line', 'earliest_cr_line_date', 'mths_since_earliest_cr_line']][loan_data['mths_since_earliest_cr_line'] < 0]\n",
        "# We take three columns from the dataframe. Then, we display them only for the rows where a variable has negative value.\n",
        "# There are 2303 strange negative values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_e4ODybuDYV"
      },
      "outputs": [],
      "source": [
        "loan_data['mths_since_earliest_cr_line'][loan_data['mths_since_earliest_cr_line'] < 0] = loan_data['mths_since_earliest_cr_line'].max()\n",
        "# We set the rows that had negative differences to the maximum value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP8nBH1QuDYW"
      },
      "outputs": [],
      "source": [
        "min(loan_data['mths_since_earliest_cr_line'])\n",
        "# Calculates and shows the minimum value of a column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3cke0VWuDYW"
      },
      "source": [
        "### Homework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJVxzbj6uDYX"
      },
      "outputs": [],
      "source": [
        "loan_data['term']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nlenbhj5uDYX"
      },
      "outputs": [],
      "source": [
        "loan_data['term'].describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjzvoN2yuDYX"
      },
      "outputs": [],
      "source": [
        "loan_data['term_int'] = loan_data['term'].str.replace(' months', '')\n",
        "# We replace a string with another string, in this case, with an empty strng (i.e. with nothing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1Ta4YNGuDYY"
      },
      "outputs": [],
      "source": [
        "loan_data['term_int']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgVNjw6HuDYY"
      },
      "outputs": [],
      "source": [
        "type(loan_data['term_int'][25])\n",
        "# Checks the datatype of a single element of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfaf2u26uDYY"
      },
      "outputs": [],
      "source": [
        "loan_data['term_int'] = pd.to_numeric(loan_data['term'].str.replace(' months', ''))\n",
        "# We remplace a string from a variable with another string, in this case, with an empty strng (i.e. with nothing).\n",
        "# We turn the result to numeric datatype and save it in another variable.\n",
        "loan_data['term_int']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXC0Nq5RuDYY"
      },
      "outputs": [],
      "source": [
        "type(loan_data['term_int'][0])\n",
        "# Checks the datatype of a single element of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw1pgTm5uDYY"
      },
      "outputs": [],
      "source": [
        "loan_data['issue_d']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5652hkcPuDYY"
      },
      "outputs": [],
      "source": [
        "# Assume we are now in December 2017\n",
        "loan_data['issue_d_date'] = pd.to_datetime(loan_data['issue_d'], format = '%b-%y')\n",
        "# Extracts the date and the time from a string variable that is in a given format.\n",
        "loan_data['mths_since_issue_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - loan_data['issue_d_date']) / np.timedelta64(1, 'M')))\n",
        "# We calculate the difference between two dates in months, turn it to numeric datatype and round it.\n",
        "# We save the result in a new variable.\n",
        "loan_data['mths_since_issue_d'].describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQJzoVK8uDYY"
      },
      "source": [
        "### Preprocessing few discrete variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JxGL1LRuDYY"
      },
      "outputs": [],
      "source": [
        "loan_data.info()\n",
        "# Displays column names, complete (non-missing) cases per column, and datatype per column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzJiubIjuDYZ"
      },
      "source": [
        "We are going to preprocess the following discrete variables: grade, sub_grade, home_ownership, verification_status, loan_status, purpose, addr_state, initial_list_status. Most likely, we are not going to use sub_grade, as it overlaps with grade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohoVphuouDYZ"
      },
      "outputs": [],
      "source": [
        "pd.get_dummies(loan_data['grade'])\n",
        "# Create dummy variables from a variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flfrkul_uDYZ"
      },
      "outputs": [],
      "source": [
        "pd.get_dummies(loan_data['grade'], prefix = 'grade', prefix_sep = ':')\n",
        "# Create dummy variables from a variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCmNDwtDuDYZ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "loan_data_dummies = [pd.get_dummies(loan_data['grade'], prefix = 'grade', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['sub_grade'], prefix = 'sub_grade', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['home_ownership'], prefix = 'home_ownership', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['verification_status'], prefix = 'verification_status', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['loan_status'], prefix = 'loan_status', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['purpose'], prefix = 'purpose', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['addr_state'], prefix = 'addr_state', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['initial_list_status'], prefix = 'initial_list_status', prefix_sep = ':')]\n",
        "# We create dummy variables from all 8 original independent variables, and save them into a list.\n",
        "# Note that we are using a particular naming convention for all variables: original variable name, colon, category name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6akhkDfuDYZ"
      },
      "outputs": [],
      "source": [
        "loan_data_dummies = pd.concat(loan_data_dummies, axis = 1)\n",
        "# We concatenate the dummy variables and this turns them into a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRI3ywC0uDYZ"
      },
      "outputs": [],
      "source": [
        "type(loan_data_dummies)\n",
        "# Returns the type of the variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRAuRW5DuDYZ"
      },
      "outputs": [],
      "source": [
        "loan_data = pd.concat([loan_data, loan_data_dummies], axis = 1)\n",
        "# Concatenates two dataframes.\n",
        "# Here we concatenate the dataframe with original data with the dataframe with dummy variables, along the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JK1zU5m9uDYZ"
      },
      "outputs": [],
      "source": [
        "loan_data.columns.values\n",
        "# Displays all column names."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_vDNS89uDYZ"
      },
      "source": [
        "### Check for missing values and clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKDulPXOuDYZ",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "loan_data.isnull()\n",
        "# It returns 'False' if a value is not missing and 'True' if a value is missing, for each value in a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4Q_W-dGuDYa"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_rows = None\n",
        "# Sets the pandas dataframe options to display all columns/ rows.\n",
        "loan_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVx4bjRUuDYa"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_rows = 100\n",
        "# Sets the pandas dataframe options to display 100 columns/ rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "is8Eu3afuDYa"
      },
      "outputs": [],
      "source": [
        "# 'Total revolving high credit/ credit limit', so it makes sense that the missing values are equal to funded_amnt.\n",
        "loan_data['total_rev_hi_lim'].fillna(loan_data['funded_amnt'], inplace=True)\n",
        "# We fill the missing values with the values of another variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3R2gvt3uDYa"
      },
      "outputs": [],
      "source": [
        "loan_data['total_rev_hi_lim'].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zssQO3EuDYa"
      },
      "source": [
        "### Homework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzXJiw1DuDYb"
      },
      "outputs": [],
      "source": [
        "loan_data['annual_inc'].fillna(loan_data['annual_inc'].mean(), inplace=True)\n",
        "# We fill the missing values with the mean value of the non-missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD89bH7puDYb"
      },
      "outputs": [],
      "source": [
        "loan_data['mths_since_earliest_cr_line'].fillna(0, inplace=True)\n",
        "loan_data['acc_now_delinq'].fillna(0, inplace=True)\n",
        "loan_data['total_acc'].fillna(0, inplace=True)\n",
        "loan_data['pub_rec'].fillna(0, inplace=True)\n",
        "loan_data['open_acc'].fillna(0, inplace=True)\n",
        "loan_data['inq_last_6mths'].fillna(0, inplace=True)\n",
        "loan_data['delinq_2yrs'].fillna(0, inplace=True)\n",
        "loan_data['emp_length_int'].fillna(0, inplace=True)\n",
        "# We fill the missing values with zeroes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnSe0pVAuDYb"
      },
      "source": [
        "# PD model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtIn00XGuDYb"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEqpBXdluDYb"
      },
      "source": [
        "### Dependent Variable. Good/ Bad (Default) Definition. Default and Non-default Accounts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtI7eiaouDYb"
      },
      "outputs": [],
      "source": [
        "loan_data['loan_status'].unique()\n",
        "# Displays unique values of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOOHc41-uDYb",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "loan_data['loan_status'].value_counts()\n",
        "# Calculates the number of observations for each unique value of a variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpgjubbMuDYb"
      },
      "outputs": [],
      "source": [
        "loan_data['loan_status'].value_counts() / loan_data['loan_status'].count()\n",
        "# We divide the number of observations for each unique value of a variable by the total number of observations.\n",
        "# Thus, we get the proportion of observations for each unique value of a variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ1lskL2uDYc",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Good/ Bad Definition\n",
        "loan_data['good_bad'] = np.where(loan_data['loan_status'].isin(['Charged Off', 'Default',\n",
        "                                                       'Does not meet the credit policy. Status:Charged Off',\n",
        "                                                       'Late (31-120 days)']), 0, 1)\n",
        "# We create a new variable that has the value of '0' if a condition is met, and the value of '1' if it is not met."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw6-NrSsuDYc"
      },
      "outputs": [],
      "source": [
        "loan_data['good_bad']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP8LnRpmuDYc"
      },
      "source": [
        "### Splitting Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8UZ1JxnuDYc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Imports the libraries we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-oVizYwuDYc"
      },
      "outputs": [],
      "source": [
        "train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'])\n",
        "# Takes a set of inputs and a set of targets as arguments. Splits the inputs and the targets into four dataframes:\n",
        "# Inputs - Train, Inputs - Test, Targets - Train, Targets - Test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGhULkEduDYc"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_train, loan_data_inputs_test, loan_data_targets_train, loan_data_targets_test = train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'])\n",
        "# We split two dataframes with inputs and targets, each into a train and test dataframe, and store them in variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VkC8TtvuDYc"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_train.shape\n",
        "# Displays the size of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aq0PNcmuDYc"
      },
      "outputs": [],
      "source": [
        "loan_data_targets_train.shape\n",
        "# Displays the size of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFhzzJT8uDYc"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_test.shape\n",
        "# Displays the size of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM4tI0GfuDYd"
      },
      "outputs": [],
      "source": [
        "loan_data_targets_test.shape\n",
        "# Displays the size of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vorrsdjyuDYd"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_train, loan_data_inputs_test, loan_data_targets_train, loan_data_targets_test = train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'], test_size = 0.2, random_state = 42)\n",
        "# We split two dataframes with inputs and targets, each into a train and test dataframe, and store them in variables.\n",
        "# This time we set the size of the test dataset to be 20%.\n",
        "# Respectively, the size of the train dataset becomes 80%.\n",
        "# We also set a specific random state.\n",
        "# This would allow us to perform the exact same split multimple times.\n",
        "# This means, to assign the exact same observations to the train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZipAOG9uDYd"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_train.shape\n",
        "# Displays the size of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud_v8WxhuDYd"
      },
      "outputs": [],
      "source": [
        "loan_data_targets_train.shape\n",
        "# Displays the size of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA17O2niuDYd"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_test.shape\n",
        "# Displays the size of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj9pnUXJuDYd"
      },
      "outputs": [],
      "source": [
        "loan_data_targets_test.shape\n",
        "# Displays the size of the dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THaMp1niuDYd"
      },
      "source": [
        "### Data Preparation: An Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhisOdZ1uDYd"
      },
      "outputs": [],
      "source": [
        "#####\n",
        "#df_inputs_prepr = loan_data_inputs_train\n",
        "#df_targets_prepr = loan_data_targets_train\n",
        "#####\n",
        "df_inputs_prepr = loan_data_inputs_test\n",
        "df_targets_prepr = loan_data_targets_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUpvum-8uDYd"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr['grade'].unique()\n",
        "# Displays unique values of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqE00sSjuDYd"
      },
      "outputs": [],
      "source": [
        "df1 = pd.concat([df_inputs_prepr['grade'], df_targets_prepr], axis = 1)\n",
        "# Concatenates two dataframes along the columns.\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcw8PYU1uDYe"
      },
      "outputs": [],
      "source": [
        "df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].count()\n",
        "# Groups the data according to a criterion contained in one column.\n",
        "# Does not turn the names of the values of the criterion as indexes.\n",
        "# Aggregates the data in another column, using a selected function.\n",
        "# In this specific case, we group by the column with index 0 and we aggregate the values of the column with index 1.\n",
        "# More specifically, we count them.\n",
        "# In other words, we count the values in the column with index 1 for each value of the column with index 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twSreIAMuDYe"
      },
      "outputs": [],
      "source": [
        "df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].mean()\n",
        "# Groups the data according to a criterion contained in one column.\n",
        "# Does not turn the names of the values of the criterion as indexes.\n",
        "# Aggregates the data in another column, using a selected function.\n",
        "# Here we calculate the mean of the values in the column with index 1 for each value of the column with index 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElvEem9RuDYe"
      },
      "outputs": [],
      "source": [
        "df1 = pd.concat([df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].count(),\n",
        "                df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].mean()], axis = 1)\n",
        "# Concatenates two dataframes along the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu-YFxepuDYe"
      },
      "outputs": [],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3McT94EuDYe"
      },
      "outputs": [],
      "source": [
        "df1 = df1.iloc[:, [0, 1, 3]]\n",
        "# Selects only columns with specific indexes.\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VDzeuSWuDYe"
      },
      "outputs": [],
      "source": [
        "df1.columns = [df1.columns.values[0], 'n_obs', 'prop_good']\n",
        "# Changes the names of the columns of a dataframe.\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99yD9qjauDYe"
      },
      "outputs": [],
      "source": [
        "df1['prop_n_obs'] = df1['n_obs'] / df1['n_obs'].sum()\n",
        "# We divide the values of one column by he values of another column and save the result in a new variable.\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgESRBI-uDYe"
      },
      "outputs": [],
      "source": [
        "df1['n_good'] = df1['prop_good'] * df1['n_obs']\n",
        "# We multiply the values of one column by he values of another column and save the result in a new variable.\n",
        "df1['n_bad'] = (1 - df1['prop_good']) * df1['n_obs']\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYV_Pfc4uDYf"
      },
      "outputs": [],
      "source": [
        "df1['prop_n_good'] = df1['n_good'] / df1['n_good'].sum()\n",
        "df1['prop_n_bad'] = df1['n_bad'] / df1['n_bad'].sum()\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqClGgu5uDYf"
      },
      "outputs": [],
      "source": [
        "df1['WoE'] = np.log(df1['prop_n_good'] / df1['prop_n_bad'])\n",
        "# We take the natural logarithm of a variable and save the result in a nex variable.\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIhQME9IuDYf",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df1 = df1.sort_values(['WoE'])\n",
        "# Sorts a dataframe by the values of a given column.\n",
        "df1 = df1.reset_index(drop = True)\n",
        "# We reset the index of a dataframe and overwrite it.\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHMeGGfguDYf",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df1['diff_prop_good'] = df1['prop_good'].diff().abs()\n",
        "# We take the difference between two subsequent values of a column. Then, we take the absolute value of the result.\n",
        "df1['diff_WoE'] = df1['WoE'].diff().abs()\n",
        "# We take the difference between two subsequent values of a column. Then, we take the absolute value of the result.\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nYPIxW5uDYg"
      },
      "outputs": [],
      "source": [
        "df1['IV'] = (df1['prop_n_good'] - df1['prop_n_bad']) * df1['WoE']\n",
        "df1['IV'] = df1['IV'].sum()\n",
        "# We sum all values of a given column.\n",
        "df1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daNJFy7zuDYg"
      },
      "source": [
        "### Preprocessing Discrete Variables: Automating Calculaions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaAhiIH0uDYg"
      },
      "outputs": [],
      "source": [
        "# WoE function for discrete unordered variables\n",
        "def woe_discrete(df, discrete_variabe_name, good_bad_variable_df):\n",
        "    df = pd.concat([df[discrete_variabe_name], good_bad_variable_df], axis = 1)\n",
        "    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n",
        "                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n",
        "    df = df.iloc[:, [0, 1, 3]]\n",
        "    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']\n",
        "    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()\n",
        "    df['n_good'] = df['prop_good'] * df['n_obs']\n",
        "    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']\n",
        "    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()\n",
        "    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()\n",
        "    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])\n",
        "    df = df.sort_values(['WoE'])\n",
        "    df = df.reset_index(drop = True)\n",
        "    df['diff_prop_good'] = df['prop_good'].diff().abs()\n",
        "    df['diff_WoE'] = df['WoE'].diff().abs()\n",
        "    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']\n",
        "    df['IV'] = df['IV'].sum()\n",
        "    return df\n",
        "# Here we combine all of the operations above in a function.\n",
        "# The function takes 3 arguments: a dataframe, a string, and a dataframe. The function returns a dataframe as a result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACT2tJAtuDYh",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# 'grade'\n",
        "df_temp = woe_discrete(df_inputs_prepr, 'grade', df_targets_prepr)\n",
        "# We execute the function we defined with the necessary arguments: a dataframe, a string, and a dataframe.\n",
        "# We store the result in a dataframe.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGTKG9vXuDYh"
      },
      "source": [
        "### Preprocessing Discrete Variables: Visualizing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJUGqv_fuDYh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Imports the libraries we need.\n",
        "sns.set()\n",
        "# We set the default style of the graphs to the seaborn style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tzr7fAjuDYh"
      },
      "outputs": [],
      "source": [
        "# Below we define a function that takes 2 arguments: a dataframe and a number.\n",
        "# The number parameter has a default value of 0.\n",
        "# This means that if we call the function and omit the number parameter, it will be executed with it having a value of 0.\n",
        "# The function displays a graph.\n",
        "def plot_by_woe(df_WoE, rotation_of_x_axis_labels = 0):\n",
        "    x = np.array(df_WoE.iloc[:, 0].apply(str))\n",
        "    # Turns the values of the column with index 0 to strings, makes an array from these strings, and passes it to variable x.\n",
        "    y = df_WoE['WoE']\n",
        "    # Selects a column with label 'WoE' and passes it to variable y.\n",
        "    plt.figure(figsize=(18, 6))\n",
        "    # Sets the graph size to width 18 x height 6.\n",
        "    plt.plot(x, y, marker = 'o', linestyle = '--', color = 'k')\n",
        "    # Plots the datapoints with coordiantes variable x on the x-axis and variable y on the y-axis.\n",
        "    # Sets the marker for each datapoint to a circle, the style line between the points to dashed, and the color to black.\n",
        "    plt.xlabel(df_WoE.columns[0])\n",
        "    # Names the x-axis with the name of the column with index 0.\n",
        "    plt.ylabel('Weight of Evidence')\n",
        "    # Names the y-axis 'Weight of Evidence'.\n",
        "    plt.title(str('Weight of Evidence by ' + df_WoE.columns[0]))\n",
        "    # Names the grapth 'Weight of Evidence by ' the name of the column with index 0.\n",
        "    plt.xticks(rotation = rotation_of_x_axis_labels)\n",
        "    # Rotates the labels of the x-axis a predefined number of degrees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G01g1wLTuDYi"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)\n",
        "# We execute the function we defined with the necessary arguments: a dataframe.\n",
        "# We omit the number argument, which means the function will use its default value, 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSeonz8tuDYi"
      },
      "source": [
        "### Preprocessing Discrete Variables: Creating Dummy Variables, Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DkaKl5JuDYi"
      },
      "outputs": [],
      "source": [
        "# 'home_ownership'\n",
        "df_temp = woe_discrete(df_inputs_prepr, 'home_ownership', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzlbnNPmuDYi"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzo5yZK7zWbp"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99Oc00U2uDYi"
      },
      "outputs": [],
      "source": [
        "# There are many categories with very few observations and many categories with very different \"good\" %.\n",
        "# Therefore, we create a new discrete variable where we combine some of the categories.\n",
        "# 'OTHERS' and 'NONE' are riskiest but are very few. 'RENT' is the next riskiest.\n",
        "# 'ANY' are least risky but are too few. Conceptually, they belong to the same category. Also, their inclusion would not change anything.\n",
        "# We combine them in one category, 'RENT_OTHER_NONE_ANY'.\n",
        "# We end up with 3 categories: 'RENT_OTHER_NONE_ANY', 'OWN', 'MORTGAGE'.\n",
        "df_inputs_prepr['home_ownership:RENT_OTHER_NONE_ANY'] = sum([df_inputs_prepr['home_ownership:RENT'], df_inputs_prepr['home_ownership:OTHER'],\n",
        "                                                      df_inputs_prepr['home_ownership:NONE'],df_inputs_prepr['home_ownership:ANY']])\n",
        "# 'RENT_OTHER_NONE_ANY' will be the reference category.\n",
        "\n",
        "# Alternatively:\n",
        "#loan_data.loc['home_ownership' in ['RENT', 'OTHER', 'NONE', 'ANY'], 'home_ownership:RENT_OTHER_NONE_ANY'] = 1\n",
        "#loan_data.loc['home_ownership' not in ['RENT', 'OTHER', 'NONE', 'ANY'], 'home_ownership:RENT_OTHER_NONE_ANY'] = 0\n",
        "#loan_data.loc['loan_status' not in ['OWN'], 'home_ownership:OWN'] = 1\n",
        "#loan_data.loc['loan_status' not in ['OWN'], 'home_ownership:OWN'] = 0\n",
        "#loan_data.loc['loan_status' not in ['MORTGAGE'], 'home_ownership:MORTGAGE'] = 1\n",
        "#loan_data.loc['loan_status' not in ['MORTGAGE'], 'home_ownership:MORTGAGE'] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-XeXewnuDYi"
      },
      "source": [
        "### Preprocessing Discrete Variables: Creating Dummy Variables, Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz2JqgdiuDYi"
      },
      "outputs": [],
      "source": [
        "# 'addr_state'\n",
        "df_inputs_prepr['addr_state'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15adJTrSuDYi",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df_temp = woe_discrete(df_inputs_prepr, 'addr_state', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DOJ4bRnuDYj"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LngeJ4kXuDYj"
      },
      "outputs": [],
      "source": [
        "if ['addr_state:ND'] in df_inputs_prepr.columns.values:\n",
        "    pass\n",
        "else:\n",
        "    df_inputs_prepr['addr_state:ND'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ3PburJuDYj"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp.iloc[2: -2, : ])\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urYGp2nOuDYj"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp.iloc[6: -6, : ])\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqA9IGHLuDYj"
      },
      "outputs": [],
      "source": [
        "# We create the following categories:\n",
        "# 'ND' 'NE' 'IA' NV' 'FL' 'HI' 'AL'\n",
        "# 'NM' 'VA'\n",
        "# 'NY'\n",
        "# 'OK' 'TN' 'MO' 'LA' 'MD' 'NC'\n",
        "# 'CA'\n",
        "# 'UT' 'KY' 'AZ' 'NJ'\n",
        "# 'AR' 'MI' 'PA' 'OH' 'MN'\n",
        "# 'RI' 'MA' 'DE' 'SD' 'IN'\n",
        "# 'GA' 'WA' 'OR'\n",
        "# 'WI' 'MT'\n",
        "# 'TX'\n",
        "# 'IL' 'CT'\n",
        "# 'KS' 'SC' 'CO' 'VT' 'AK' 'MS'\n",
        "# 'WV' 'NH' 'WY' 'DC' 'ME' 'ID'\n",
        "\n",
        "# 'IA_NV_HI_ID_AL_FL' will be the reference category.\n",
        "\n",
        "df_inputs_prepr['addr_state:ND_NE_IA_NV_FL_HI_AL'] = sum([df_inputs_prepr['addr_state:ND'], df_inputs_prepr['addr_state:NE'],\n",
        "                                              df_inputs_prepr['addr_state:IA'], df_inputs_prepr['addr_state:NV'],\n",
        "                                              df_inputs_prepr['addr_state:FL'], df_inputs_prepr['addr_state:HI'],\n",
        "                                                          df_inputs_prepr['addr_state:AL']])\n",
        "\n",
        "df_inputs_prepr['addr_state:NM_VA'] = sum([df_inputs_prepr['addr_state:NM'], df_inputs_prepr['addr_state:VA']])\n",
        "\n",
        "df_inputs_prepr['addr_state:OK_TN_MO_LA_MD_NC'] = sum([df_inputs_prepr['addr_state:OK'], df_inputs_prepr['addr_state:TN'],\n",
        "                                              df_inputs_prepr['addr_state:MO'], df_inputs_prepr['addr_state:LA'],\n",
        "                                              df_inputs_prepr['addr_state:MD'], df_inputs_prepr['addr_state:NC']])\n",
        "\n",
        "df_inputs_prepr['addr_state:UT_KY_AZ_NJ'] = sum([df_inputs_prepr['addr_state:UT'], df_inputs_prepr['addr_state:KY'],\n",
        "                                              df_inputs_prepr['addr_state:AZ'], df_inputs_prepr['addr_state:NJ']])\n",
        "\n",
        "df_inputs_prepr['addr_state:AR_MI_PA_OH_MN'] = sum([df_inputs_prepr['addr_state:AR'], df_inputs_prepr['addr_state:MI'],\n",
        "                                              df_inputs_prepr['addr_state:PA'], df_inputs_prepr['addr_state:OH'],\n",
        "                                              df_inputs_prepr['addr_state:MN']])\n",
        "\n",
        "df_inputs_prepr['addr_state:RI_MA_DE_SD_IN'] = sum([df_inputs_prepr['addr_state:RI'], df_inputs_prepr['addr_state:MA'],\n",
        "                                              df_inputs_prepr['addr_state:DE'], df_inputs_prepr['addr_state:SD'],\n",
        "                                              df_inputs_prepr['addr_state:IN']])\n",
        "\n",
        "df_inputs_prepr['addr_state:GA_WA_OR'] = sum([df_inputs_prepr['addr_state:GA'], df_inputs_prepr['addr_state:WA'],\n",
        "                                              df_inputs_prepr['addr_state:OR']])\n",
        "\n",
        "df_inputs_prepr['addr_state:WI_MT'] = sum([df_inputs_prepr['addr_state:WI'], df_inputs_prepr['addr_state:MT']])\n",
        "\n",
        "df_inputs_prepr['addr_state:IL_CT'] = sum([df_inputs_prepr['addr_state:IL'], df_inputs_prepr['addr_state:CT']])\n",
        "\n",
        "df_inputs_prepr['addr_state:KS_SC_CO_VT_AK_MS'] = sum([df_inputs_prepr['addr_state:KS'], df_inputs_prepr['addr_state:SC'],\n",
        "                                              df_inputs_prepr['addr_state:CO'], df_inputs_prepr['addr_state:VT'],\n",
        "                                              df_inputs_prepr['addr_state:AK'], df_inputs_prepr['addr_state:MS']])\n",
        "\n",
        "df_inputs_prepr['addr_state:WV_NH_WY_DC_ME_ID'] = sum([df_inputs_prepr['addr_state:WV'], df_inputs_prepr['addr_state:NH'],\n",
        "                                              df_inputs_prepr['addr_state:WY'], df_inputs_prepr['addr_state:DC'],\n",
        "                                              df_inputs_prepr['addr_state:ME'], df_inputs_prepr['addr_state:ID']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLN929D0uDYj"
      },
      "source": [
        "### Preprocessing Discrete Variables: Homework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MkwVydluDYj"
      },
      "outputs": [],
      "source": [
        "# 'verification_status'\n",
        "df_temp = woe_discrete(df_inputs_prepr, 'verification_status', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxiGfpj0uDYj",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4NihIlVuDYk"
      },
      "outputs": [],
      "source": [
        "# 'purpose'\n",
        "df_temp = woe_discrete(df_inputs_prepr, 'purpose', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKrANCuQuDYk"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTPR6xGZuDYk"
      },
      "outputs": [],
      "source": [
        "# We combine 'educational', 'small_business', 'wedding', 'renewable_energy', 'moving', 'house' in one category: 'educ__sm_b__wedd__ren_en__mov__house'.\n",
        "# We combine 'other', 'medical', 'vacation' in one category: 'oth__med__vacation'.\n",
        "# We combine 'major_purchase', 'car', 'home_improvement' in one category: 'major_purch__car__home_impr'.\n",
        "# We leave 'debt_consolidtion' in a separate category.\n",
        "# We leave 'credit_card' in a separate category.\n",
        "# 'educ__sm_b__wedd__ren_en__mov__house' will be the reference category.\n",
        "df_inputs_prepr['purpose:educ__sm_b__wedd__ren_en__mov__house'] = sum([df_inputs_prepr['purpose:educational'], df_inputs_prepr['purpose:small_business'],\n",
        "                                                                 df_inputs_prepr['purpose:wedding'], df_inputs_prepr['purpose:renewable_energy'],\n",
        "                                                                 df_inputs_prepr['purpose:moving'], df_inputs_prepr['purpose:house']])\n",
        "df_inputs_prepr['purpose:oth__med__vacation'] = sum([df_inputs_prepr['purpose:other'], df_inputs_prepr['purpose:medical'],\n",
        "                                             df_inputs_prepr['purpose:vacation']])\n",
        "df_inputs_prepr['purpose:major_purch__car__home_impr'] = sum([df_inputs_prepr['purpose:major_purchase'], df_inputs_prepr['purpose:car'],\n",
        "                                                        df_inputs_prepr['purpose:home_improvement']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKVpxjeauDYk"
      },
      "outputs": [],
      "source": [
        "# 'initial_list_status'\n",
        "df_temp = woe_discrete(df_inputs_prepr, 'initial_list_status', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DHjElhkuDYk"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hNwDTqHuDYk"
      },
      "source": [
        "### Preprocessing Continuous Variables: Automating Calculations and Visualizing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lwXBnP-uDYk"
      },
      "outputs": [],
      "source": [
        "# WoE function for ordered discrete and continuous variables\n",
        "def woe_ordered_continuous(df, discrete_variabe_name, good_bad_variable_df):\n",
        "    df = pd.concat([df[discrete_variabe_name], good_bad_variable_df], axis = 1)\n",
        "    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n",
        "                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n",
        "    df = df.iloc[:, [0, 1, 3]]\n",
        "    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']\n",
        "    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()\n",
        "    df['n_good'] = df['prop_good'] * df['n_obs']\n",
        "    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']\n",
        "    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()\n",
        "    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()\n",
        "    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])\n",
        "    #df = df.sort_values(['WoE'])\n",
        "    #df = df.reset_index(drop = True)\n",
        "    df['diff_prop_good'] = df['prop_good'].diff().abs()\n",
        "    df['diff_WoE'] = df['WoE'].diff().abs()\n",
        "    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']\n",
        "    df['IV'] = df['IV'].sum()\n",
        "    return df\n",
        "# Here we define a function similar to the one above, ...\n",
        "# ... with one slight difference: we order the results by the values of a different column.\n",
        "# The function takes 3 arguments: a dataframe, a string, and a dataframe. The function returns a dataframe as a result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIj3Weg_uDYk"
      },
      "source": [
        "### Preprocessing Continuous Variables: Creating Dummy Variables, Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u50_YEYCuDYk",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# term\n",
        "df_inputs_prepr['term_int'].unique()\n",
        "# There are only two unique values, 36 and 60."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS8svb69uDYl",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'term_int', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HUqA63MuDYl",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob4azfzquDYl"
      },
      "outputs": [],
      "source": [
        "# Leave as is.\n",
        "# '60' will be the reference category.\n",
        "df_inputs_prepr['term:36'] = np.where((df_inputs_prepr['term_int'] == 36), 1, 0)\n",
        "df_inputs_prepr['term:60'] = np.where((df_inputs_prepr['term_int'] == 60), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAo7VsFMuDYl"
      },
      "outputs": [],
      "source": [
        "# emp_length_int\n",
        "df_inputs_prepr['emp_length_int'].unique()\n",
        "# Has only 11 levels: from 0 to 10. Hence, we turn it into a factor with 11 levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm1zNdhNuDYl"
      },
      "outputs": [],
      "source": [
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'emp_length_int', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKqiLyIWuDYl"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pND4FrpquDYl"
      },
      "outputs": [],
      "source": [
        "# We create the following categories: '0', '1', '2 - 4', '5 - 6', '7 - 9', '10'\n",
        "# '0' will be the reference category\n",
        "df_inputs_prepr['emp_length:0'] = np.where(df_inputs_prepr['emp_length_int'].isin([0]), 1, 0)\n",
        "df_inputs_prepr['emp_length:1'] = np.where(df_inputs_prepr['emp_length_int'].isin([1]), 1, 0)\n",
        "df_inputs_prepr['emp_length:2-4'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(2, 5)), 1, 0)\n",
        "df_inputs_prepr['emp_length:5-6'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(5, 7)), 1, 0)\n",
        "df_inputs_prepr['emp_length:7-9'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(7, 10)), 1, 0)\n",
        "df_inputs_prepr['emp_length:10'] = np.where(df_inputs_prepr['emp_length_int'].isin([10]), 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmXaKnPAuDYl"
      },
      "source": [
        "### Preprocessing Continuous Variables: Creating Dummy Variables, Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypfU00ATuDYl"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr['mths_since_issue_d'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obLJtkJvuDYl"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr['mths_since_issue_d_factor'] = pd.cut(df_inputs_prepr['mths_since_issue_d'], 50)\n",
        "# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HU08oiAuDYl"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr['mths_since_issue_d_factor']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrEV-5swuDYm"
      },
      "outputs": [],
      "source": [
        "# mths_since_issue_d\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'mths_since_issue_d_factor', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Aw1TKD-uDYm"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)\n",
        "# We plot the weight of evidence values.\n",
        "# We have to rotate the labels because we cannot read them otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD8y1gvIuDYm"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)\n",
        "# We plot the weight of evidence values, rotating the labels 90 degrees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu5AD7xnuDYm"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp.iloc[3: , : ], 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hnmb91WQuDYm"
      },
      "outputs": [],
      "source": [
        "# We create the following categories:\n",
        "# < 38, 38 - 39, 40 - 41, 42 - 48, 49 - 52, 53 - 64, 65 - 84, > 84.\n",
        "df_inputs_prepr['mths_since_issue_d:<38'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(38)), 1, 0)\n",
        "df_inputs_prepr['mths_since_issue_d:38-39'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(38, 40)), 1, 0)\n",
        "df_inputs_prepr['mths_since_issue_d:40-41'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(40, 42)), 1, 0)\n",
        "df_inputs_prepr['mths_since_issue_d:42-48'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(42, 49)), 1, 0)\n",
        "df_inputs_prepr['mths_since_issue_d:49-52'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(49, 53)), 1, 0)\n",
        "df_inputs_prepr['mths_since_issue_d:53-64'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(53, 65)), 1, 0)\n",
        "df_inputs_prepr['mths_since_issue_d:65-84'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(65, 85)), 1, 0)\n",
        "df_inputs_prepr['mths_since_issue_d:>84'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(85, int(df_inputs_prepr['mths_since_issue_d'].max()))), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGMW4ZaWuDYm",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# int_rate\n",
        "df_inputs_prepr['int_rate_factor'] = pd.cut(df_inputs_prepr['int_rate'], 50)\n",
        "# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBmIIOIvuDYm"
      },
      "outputs": [],
      "source": [
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'int_rate_factor', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0yKBQLduDYm"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xab-btPhuDYm"
      },
      "outputs": [],
      "source": [
        "# '< 9.548', '9.548 - 12.025', '12.025 - 15.74', '15.74 - 20.281', '> 20.281'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sXplEP7uDYm",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr['int_rate:<9.548'] = np.where((df_inputs_prepr['int_rate'] <= 9.548), 1, 0)\n",
        "df_inputs_prepr['int_rate:9.548-12.025'] = np.where((df_inputs_prepr['int_rate'] > 9.548) & (df_inputs_prepr['int_rate'] <= 12.025), 1, 0)\n",
        "df_inputs_prepr['int_rate:12.025-15.74'] = np.where((df_inputs_prepr['int_rate'] > 12.025) & (df_inputs_prepr['int_rate'] <= 15.74), 1, 0)\n",
        "df_inputs_prepr['int_rate:15.74-20.281'] = np.where((df_inputs_prepr['int_rate'] > 15.74) & (df_inputs_prepr['int_rate'] <= 20.281), 1, 0)\n",
        "df_inputs_prepr['int_rate:>20.281'] = np.where((df_inputs_prepr['int_rate'] > 20.281), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEBgfVG_uDYn"
      },
      "outputs": [],
      "source": [
        "# funded_amnt\n",
        "df_inputs_prepr['funded_amnt_factor'] = pd.cut(df_inputs_prepr['funded_amnt'], 50)\n",
        "# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'funded_amnt_factor', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHtQxr7buDYn"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u28OvcyUuDYn"
      },
      "source": [
        "### Data Preparation: Continuous Variables, Part 1 and 2: Homework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VxO5fWzuDYn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# mths_since_earliest_cr_line\n",
        "df_inputs_prepr['mths_since_earliest_cr_line_factor'] = pd.cut(df_inputs_prepr['mths_since_earliest_cr_line'], 50)\n",
        "# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'mths_since_earliest_cr_line_factor', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoRQ77SEuDYn"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WswLGEEPuDYn"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp.iloc[6: , : ], 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1V6FawTuDYn"
      },
      "outputs": [],
      "source": [
        "# We create the following categories:\n",
        "# < 140, # 141 - 164, # 165 - 247, # 248 - 270, # 271 - 352, # > 352\n",
        "df_inputs_prepr['mths_since_earliest_cr_line:<140'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(140)), 1, 0)\n",
        "df_inputs_prepr['mths_since_earliest_cr_line:141-164'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(140, 165)), 1, 0)\n",
        "df_inputs_prepr['mths_since_earliest_cr_line:165-247'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(165, 248)), 1, 0)\n",
        "df_inputs_prepr['mths_since_earliest_cr_line:248-270'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(248, 271)), 1, 0)\n",
        "df_inputs_prepr['mths_since_earliest_cr_line:271-352'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(271, 353)), 1, 0)\n",
        "df_inputs_prepr['mths_since_earliest_cr_line:>352'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(353, int(df_inputs_prepr['mths_since_earliest_cr_line'].max()))), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLyO-UBsuDYn"
      },
      "outputs": [],
      "source": [
        "# delinq_2yrs\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'delinq_2yrs', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEblqA4XuDYn"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20zkGslvuDYn"
      },
      "outputs": [],
      "source": [
        "# Categories: 0, 1-3, >=4\n",
        "df_inputs_prepr['delinq_2yrs:0'] = np.where((df_inputs_prepr['delinq_2yrs'] == 0), 1, 0)\n",
        "df_inputs_prepr['delinq_2yrs:1-3'] = np.where((df_inputs_prepr['delinq_2yrs'] >= 1) & (df_inputs_prepr['delinq_2yrs'] <= 3), 1, 0)\n",
        "df_inputs_prepr['delinq_2yrs:>=4'] = np.where((df_inputs_prepr['delinq_2yrs'] >= 9), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmbBetPRuDYn"
      },
      "outputs": [],
      "source": [
        "# inq_last_6mths\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'inq_last_6mths', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSLmFBWpuDYo"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iehi4KEwuDYo"
      },
      "outputs": [],
      "source": [
        "# Categories: 0, 1 - 2, 3 - 6, > 6\n",
        "df_inputs_prepr['inq_last_6mths:0'] = np.where((df_inputs_prepr['inq_last_6mths'] == 0), 1, 0)\n",
        "df_inputs_prepr['inq_last_6mths:1-2'] = np.where((df_inputs_prepr['inq_last_6mths'] >= 1) & (df_inputs_prepr['inq_last_6mths'] <= 2), 1, 0)\n",
        "df_inputs_prepr['inq_last_6mths:3-6'] = np.where((df_inputs_prepr['inq_last_6mths'] >= 3) & (df_inputs_prepr['inq_last_6mths'] <= 6), 1, 0)\n",
        "df_inputs_prepr['inq_last_6mths:>6'] = np.where((df_inputs_prepr['inq_last_6mths'] > 6), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTejpJXWuDYo",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# open_acc\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'open_acc', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKB_lLyxuDYo"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MJRHzLCuDYo"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp.iloc[ : 40, :], 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtNHtRj4uDYo"
      },
      "outputs": [],
      "source": [
        "# Categories: '0', '1-3', '4-12', '13-17', '18-22', '23-25', '26-30', '>30'\n",
        "df_inputs_prepr['open_acc:0'] = np.where((df_inputs_prepr['open_acc'] == 0), 1, 0)\n",
        "df_inputs_prepr['open_acc:1-3'] = np.where((df_inputs_prepr['open_acc'] >= 1) & (df_inputs_prepr['open_acc'] <= 3), 1, 0)\n",
        "df_inputs_prepr['open_acc:4-12'] = np.where((df_inputs_prepr['open_acc'] >= 4) & (df_inputs_prepr['open_acc'] <= 12), 1, 0)\n",
        "df_inputs_prepr['open_acc:13-17'] = np.where((df_inputs_prepr['open_acc'] >= 13) & (df_inputs_prepr['open_acc'] <= 17), 1, 0)\n",
        "df_inputs_prepr['open_acc:18-22'] = np.where((df_inputs_prepr['open_acc'] >= 18) & (df_inputs_prepr['open_acc'] <= 22), 1, 0)\n",
        "df_inputs_prepr['open_acc:23-25'] = np.where((df_inputs_prepr['open_acc'] >= 23) & (df_inputs_prepr['open_acc'] <= 25), 1, 0)\n",
        "df_inputs_prepr['open_acc:26-30'] = np.where((df_inputs_prepr['open_acc'] >= 26) & (df_inputs_prepr['open_acc'] <= 30), 1, 0)\n",
        "df_inputs_prepr['open_acc:>=31'] = np.where((df_inputs_prepr['open_acc'] >= 31), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCriRyCjuDYo"
      },
      "outputs": [],
      "source": [
        "# pub_rec\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'pub_rec', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux2WUjFxuDYo"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmS0_hGAuDYo"
      },
      "outputs": [],
      "source": [
        "# Categories '0-2', '3-4', '>=5'\n",
        "df_inputs_prepr['pub_rec:0-2'] = np.where((df_inputs_prepr['pub_rec'] >= 0) & (df_inputs_prepr['pub_rec'] <= 2), 1, 0)\n",
        "df_inputs_prepr['pub_rec:3-4'] = np.where((df_inputs_prepr['pub_rec'] >= 3) & (df_inputs_prepr['pub_rec'] <= 4), 1, 0)\n",
        "df_inputs_prepr['pub_rec:>=5'] = np.where((df_inputs_prepr['pub_rec'] >= 5), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ospNyW2suDYo"
      },
      "outputs": [],
      "source": [
        "# total_acc\n",
        "df_inputs_prepr['total_acc_factor'] = pd.cut(df_inputs_prepr['total_acc'], 50)\n",
        "# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'total_acc_factor', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8-4Cv6WuDYp"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B9kD5omuDYp"
      },
      "outputs": [],
      "source": [
        "# Categories: '<=27', '28-51', '>51'\n",
        "df_inputs_prepr['total_acc:<=27'] = np.where((df_inputs_prepr['total_acc'] <= 27), 1, 0)\n",
        "df_inputs_prepr['total_acc:28-51'] = np.where((df_inputs_prepr['total_acc'] >= 28) & (df_inputs_prepr['total_acc'] <= 51), 1, 0)\n",
        "df_inputs_prepr['total_acc:>=52'] = np.where((df_inputs_prepr['total_acc'] >= 52), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HnaxT15uDYp"
      },
      "outputs": [],
      "source": [
        "# acc_now_delinq\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'acc_now_delinq', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E65kBxG3uDYp"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IzWaDTnuDYp"
      },
      "outputs": [],
      "source": [
        "# Categories: '0', '>=1'\n",
        "df_inputs_prepr['acc_now_delinq:0'] = np.where((df_inputs_prepr['acc_now_delinq'] == 0), 1, 0)\n",
        "df_inputs_prepr['acc_now_delinq:>=1'] = np.where((df_inputs_prepr['acc_now_delinq'] >= 1), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIroIB46uDYp"
      },
      "outputs": [],
      "source": [
        "# total_rev_hi_lim\n",
        "df_inputs_prepr['total_rev_hi_lim_factor'] = pd.cut(df_inputs_prepr['total_rev_hi_lim'], 2000)\n",
        "# Here we do fine-classing: using the 'cut' method, we split the variable into 2000 categories by its values.\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'total_rev_hi_lim_factor', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioCDq4WQuDYp"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp.iloc[: 50, : ], 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaR4-o3RuDYq"
      },
      "outputs": [],
      "source": [
        "# Categories\n",
        "# '<=5K', '5K-10K', '10K-20K', '20K-30K', '30K-40K', '40K-55K', '55K-95K', '>95K'\n",
        "df_inputs_prepr['total_rev_hi_lim:<=5K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] <= 5000), 1, 0)\n",
        "df_inputs_prepr['total_rev_hi_lim:5K-10K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 5000) & (df_inputs_prepr['total_rev_hi_lim'] <= 10000), 1, 0)\n",
        "df_inputs_prepr['total_rev_hi_lim:10K-20K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 10000) & (df_inputs_prepr['total_rev_hi_lim'] <= 20000), 1, 0)\n",
        "df_inputs_prepr['total_rev_hi_lim:20K-30K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 20000) & (df_inputs_prepr['total_rev_hi_lim'] <= 30000), 1, 0)\n",
        "df_inputs_prepr['total_rev_hi_lim:30K-40K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 30000) & (df_inputs_prepr['total_rev_hi_lim'] <= 40000), 1, 0)\n",
        "df_inputs_prepr['total_rev_hi_lim:40K-55K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 40000) & (df_inputs_prepr['total_rev_hi_lim'] <= 55000), 1, 0)\n",
        "df_inputs_prepr['total_rev_hi_lim:55K-95K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 55000) & (df_inputs_prepr['total_rev_hi_lim'] <= 95000), 1, 0)\n",
        "df_inputs_prepr['total_rev_hi_lim:>95K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 95000), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oH_lgs6uDYq"
      },
      "outputs": [],
      "source": [
        "# installment\n",
        "df_inputs_prepr['installment_factor'] = pd.cut(df_inputs_prepr['installment'], 50)\n",
        "# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'installment_factor', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvyLvVQHuDYq"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeKKa3vsuDYq"
      },
      "source": [
        "### Preprocessing Continuous Variables: Creating Dummy Variables, Part 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWkKtODLuDYq"
      },
      "outputs": [],
      "source": [
        "# annual_inc\n",
        "df_inputs_prepr['annual_inc_factor'] = pd.cut(df_inputs_prepr['annual_inc'], 50)\n",
        "# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'annual_inc_factor', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qid-7p2HuDYr"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr['annual_inc_factor'] = pd.cut(df_inputs_prepr['annual_inc'], 100)\n",
        "# Here we do fine-classing: using the 'cut' method, we split the variable into 100 categories by its values.\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'annual_inc_factor', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PAf_DlhuDYr",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Initial examination shows that there are too few individuals with large income and too many with small income.\n",
        "# Hence, we are going to have one category for more than 150K, and we are going to apply our approach to determine\n",
        "# the categories of everyone with 140k or less.\n",
        "df_inputs_prepr_temp = df_inputs_prepr.loc[df_inputs_prepr['annual_inc'] <= 140000, : ]\n",
        "#loan_data_temp = loan_data_temp.reset_index(drop = True)\n",
        "#df_inputs_prepr_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os60NM7zuDYr",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr_temp[\"annual_inc_factor\"] = pd.cut(df_inputs_prepr_temp['annual_inc'], 50)\n",
        "# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'annual_inc_factor', df_targets_prepr[df_inputs_prepr_temp.index])\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbBtXEmUuDYr"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m3GGQmTuDYr"
      },
      "outputs": [],
      "source": [
        "# WoE is monotonically decreasing with income, so we split income in 10 equal categories, each with width of 15k.\n",
        "df_inputs_prepr['annual_inc:<20K'] = np.where((df_inputs_prepr['annual_inc'] <= 20000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:20K-30K'] = np.where((df_inputs_prepr['annual_inc'] > 20000) & (df_inputs_prepr['annual_inc'] <= 30000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:30K-40K'] = np.where((df_inputs_prepr['annual_inc'] > 30000) & (df_inputs_prepr['annual_inc'] <= 40000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:40K-50K'] = np.where((df_inputs_prepr['annual_inc'] > 40000) & (df_inputs_prepr['annual_inc'] <= 50000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:50K-60K'] = np.where((df_inputs_prepr['annual_inc'] > 50000) & (df_inputs_prepr['annual_inc'] <= 60000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:60K-70K'] = np.where((df_inputs_prepr['annual_inc'] > 60000) & (df_inputs_prepr['annual_inc'] <= 70000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:70K-80K'] = np.where((df_inputs_prepr['annual_inc'] > 70000) & (df_inputs_prepr['annual_inc'] <= 80000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:80K-90K'] = np.where((df_inputs_prepr['annual_inc'] > 80000) & (df_inputs_prepr['annual_inc'] <= 90000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:90K-100K'] = np.where((df_inputs_prepr['annual_inc'] > 90000) & (df_inputs_prepr['annual_inc'] <= 100000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:100K-120K'] = np.where((df_inputs_prepr['annual_inc'] > 100000) & (df_inputs_prepr['annual_inc'] <= 120000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:120K-140K'] = np.where((df_inputs_prepr['annual_inc'] > 120000) & (df_inputs_prepr['annual_inc'] <= 140000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:>140K'] = np.where((df_inputs_prepr['annual_inc'] > 140000), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrkxkxjauDYr",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# mths_since_last_delinq\n",
        "# We have to create one category for missing values and do fine and coarse classing for the rest.\n",
        "df_inputs_prepr_temp = df_inputs_prepr[pd.notnull(df_inputs_prepr['mths_since_last_delinq'])]\n",
        "df_inputs_prepr_temp['mths_since_last_delinq_factor'] = pd.cut(df_inputs_prepr_temp['mths_since_last_delinq'], 50)\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'mths_since_last_delinq_factor', df_targets_prepr[df_inputs_prepr_temp.index])\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW9NmwdYuDYr"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09Cni14buDYr"
      },
      "outputs": [],
      "source": [
        "# Categories: Missing, 0-3, 4-30, 31-56, >=57\n",
        "df_inputs_prepr['mths_since_last_delinq:Missing'] = np.where((df_inputs_prepr['mths_since_last_delinq'].isnull()), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_delinq:0-3'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 0) & (df_inputs_prepr['mths_since_last_delinq'] <= 3), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_delinq:4-30'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 4) & (df_inputs_prepr['mths_since_last_delinq'] <= 30), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_delinq:31-56'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 31) & (df_inputs_prepr['mths_since_last_delinq'] <= 56), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_delinq:>=57'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 57), 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAtPV0KOuDYr"
      },
      "source": [
        "### Preprocessing Continuous Variables: Creating Dummy Variables, Part 3: Homework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVsPYWdtuDYr"
      },
      "outputs": [],
      "source": [
        "# dti\n",
        "df_inputs_prepr['dti_factor'] = pd.cut(df_inputs_prepr['dti'], 100)\n",
        "# Here we do fine-classing: using the 'cut' method, we split the variable into 100 categories by its values.\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'dti_factor', df_targets_prepr)\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jy4iM6luDYs"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "596XbDutuDYs"
      },
      "outputs": [],
      "source": [
        "# Similarly to income, initial examination shows that most values are lower than 200.\n",
        "# Hence, we are going to have one category for more than 35, and we are going to apply our approach to determine\n",
        "# the categories of everyone with 150k or less.\n",
        "df_inputs_prepr_temp = df_inputs_prepr.loc[df_inputs_prepr['dti'] <= 35, : ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejqQuBZEuDYs",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr_temp['dti_factor'] = pd.cut(df_inputs_prepr_temp['dti'], 50)\n",
        "# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'dti_factor', df_targets_prepr[df_inputs_prepr_temp.index])\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE6oYaHNuDYy",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epZLmBMEuDYy"
      },
      "outputs": [],
      "source": [
        "# Categories:\n",
        "df_inputs_prepr['dti:<=1.4'] = np.where((df_inputs_prepr['dti'] <= 1.4), 1, 0)\n",
        "df_inputs_prepr['dti:1.4-3.5'] = np.where((df_inputs_prepr['dti'] > 1.4) & (df_inputs_prepr['dti'] <= 3.5), 1, 0)\n",
        "df_inputs_prepr['dti:3.5-7.7'] = np.where((df_inputs_prepr['dti'] > 3.5) & (df_inputs_prepr['dti'] <= 7.7), 1, 0)\n",
        "df_inputs_prepr['dti:7.7-10.5'] = np.where((df_inputs_prepr['dti'] > 7.7) & (df_inputs_prepr['dti'] <= 10.5), 1, 0)\n",
        "df_inputs_prepr['dti:10.5-16.1'] = np.where((df_inputs_prepr['dti'] > 10.5) & (df_inputs_prepr['dti'] <= 16.1), 1, 0)\n",
        "df_inputs_prepr['dti:16.1-20.3'] = np.where((df_inputs_prepr['dti'] > 16.1) & (df_inputs_prepr['dti'] <= 20.3), 1, 0)\n",
        "df_inputs_prepr['dti:20.3-21.7'] = np.where((df_inputs_prepr['dti'] > 20.3) & (df_inputs_prepr['dti'] <= 21.7), 1, 0)\n",
        "df_inputs_prepr['dti:21.7-22.4'] = np.where((df_inputs_prepr['dti'] > 21.7) & (df_inputs_prepr['dti'] <= 22.4), 1, 0)\n",
        "df_inputs_prepr['dti:22.4-35'] = np.where((df_inputs_prepr['dti'] > 22.4) & (df_inputs_prepr['dti'] <= 35), 1, 0)\n",
        "df_inputs_prepr['dti:>35'] = np.where((df_inputs_prepr['dti'] > 35), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwgvMUVouDYy"
      },
      "outputs": [],
      "source": [
        "# mths_since_last_record\n",
        "# We have to create one category for missing values and do fine and coarse classing for the rest.\n",
        "df_inputs_prepr_temp = df_inputs_prepr[pd.notnull(df_inputs_prepr['mths_since_last_record'])]\n",
        "#sum(loan_data_temp['mths_since_last_record'].isnull())\n",
        "df_inputs_prepr_temp['mths_since_last_record_factor'] = pd.cut(df_inputs_prepr_temp['mths_since_last_record'], 50)\n",
        "# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'mths_since_last_record_factor', df_targets_prepr[df_inputs_prepr_temp.index])\n",
        "# We calculate weight of evidence.\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJzKVID7uDYz"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)\n",
        "# We plot the weight of evidence values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBpyxw0yuDYz"
      },
      "outputs": [],
      "source": [
        "# Categories: 'Missing', '0-2', '3-20', '21-31', '32-80', '81-86', '>86'\n",
        "df_inputs_prepr['mths_since_last_record:Missing'] = np.where((df_inputs_prepr['mths_since_last_record'].isnull()), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_record:0-2'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 0) & (df_inputs_prepr['mths_since_last_record'] <= 2), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_record:3-20'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 3) & (df_inputs_prepr['mths_since_last_record'] <= 20), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_record:21-31'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 21) & (df_inputs_prepr['mths_since_last_record'] <= 31), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_record:32-80'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 32) & (df_inputs_prepr['mths_since_last_record'] <= 80), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_record:81-86'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 81) & (df_inputs_prepr['mths_since_last_record'] <= 86), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_record:>86'] = np.where((df_inputs_prepr['mths_since_last_record'] > 86), 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ-3B5UkuDYz"
      },
      "source": [
        "### Preprocessing the Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-vueh__uDYz"
      },
      "outputs": [],
      "source": [
        "#####\n",
        "#loan_data_inputs_train = df_inputs_prepr\n",
        "#####\n",
        "loan_data_inputs_test = df_inputs_prepr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgldv42euDYz"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_train.to_csv('loan_data_inputs_train.csv')\n",
        "loan_data_targets_train.to_csv('loan_data_targets_train.csv')\n",
        "loan_data_inputs_test.to_csv('loan_data_inputs_test.csv')\n",
        "loan_data_targets_test.to_csv('loan_data_targets_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLtvepRtvgIf"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_train = pd.read_csv('/content/loan_data_inputs_train.csv', index_col = 0)\n",
        "loan_data_targets_train = pd.read_csv('/content/loan_data_targets_train.csv', index_col = 0, header = None)\n",
        "loan_data_inputs_test = pd.read_csv('/content/loan_data_inputs_test.csv', index_col = 0)\n",
        "loan_data_targets_test = pd.read_csv('/content/loan_data_targets_test.csv', index_col = 0, header = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmaMf6S6vjqE"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Buwu8BCLv-2M"
      },
      "outputs": [],
      "source": [
        "loan_data_targets_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DKdcVVHwBQv"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBo_8_0IwC6y"
      },
      "outputs": [],
      "source": [
        "loan_data_targets_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km9BILfswEYO"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKMJ6LOdwF18"
      },
      "outputs": [],
      "source": [
        "loan_data_targets_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr5wNZY7vWqC"
      },
      "source": [
        "### Selecting the Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc_8hm9UxkZg"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIHuUld-wHXR"
      },
      "outputs": [],
      "source": [
        "# Here we select a limited set of input variables in a new dataframe.\n",
        "inputs_train_with_ref_cat = loan_data_inputs_train.loc[: , ['grade:A',\n",
        "'grade:B',\n",
        "'grade:C',\n",
        "'grade:D',\n",
        "'grade:E',\n",
        "'grade:F',\n",
        "'grade:G',\n",
        "'home_ownership:RENT_OTHER_NONE_ANY',\n",
        "'home_ownership:OWN',\n",
        "'home_ownership:MORTGAGE',\n",
        "'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
        "'addr_state:NM_VA',\n",
        "'addr_state:NY',\n",
        "'addr_state:OK_TN_MO_LA_MD_NC',\n",
        "'addr_state:CA',\n",
        "'addr_state:UT_KY_AZ_NJ',\n",
        "'addr_state:AR_MI_PA_OH_MN',\n",
        "'addr_state:RI_MA_DE_SD_IN',\n",
        "'addr_state:GA_WA_OR',\n",
        "'addr_state:WI_MT',\n",
        "'addr_state:TX',\n",
        "'addr_state:IL_CT',\n",
        "'addr_state:KS_SC_CO_VT_AK_MS',\n",
        "'addr_state:WV_NH_WY_DC_ME_ID',\n",
        "'verification_status:Not Verified',\n",
        "'verification_status:Source Verified',\n",
        "'verification_status:Verified',\n",
        "'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
        "'purpose:credit_card',\n",
        "'purpose:debt_consolidation',\n",
        "'purpose:oth__med__vacation',\n",
        "'purpose:major_purch__car__home_impr',\n",
        "'initial_list_status:f',\n",
        "'initial_list_status:w',\n",
        "'term:36',\n",
        "'term:60',\n",
        "'emp_length:0',\n",
        "'emp_length:1',\n",
        "'emp_length:2-4',\n",
        "'emp_length:5-6',\n",
        "'emp_length:7-9',\n",
        "'emp_length:10',\n",
        "'mths_since_issue_d:<38',\n",
        "'mths_since_issue_d:38-39',\n",
        "'mths_since_issue_d:40-41',\n",
        "'mths_since_issue_d:42-48',\n",
        "'mths_since_issue_d:49-52',\n",
        "'mths_since_issue_d:53-64',\n",
        "'mths_since_issue_d:65-84',\n",
        "'mths_since_issue_d:>84',\n",
        "'int_rate:<9.548',\n",
        "'int_rate:9.548-12.025',\n",
        "'int_rate:12.025-15.74',\n",
        "'int_rate:15.74-20.281',\n",
        "'int_rate:>20.281',\n",
        "'mths_since_earliest_cr_line:<140',\n",
        "'mths_since_earliest_cr_line:141-164',\n",
        "'mths_since_earliest_cr_line:165-247',\n",
        "'mths_since_earliest_cr_line:248-270',\n",
        "'mths_since_earliest_cr_line:271-352',\n",
        "'mths_since_earliest_cr_line:>352',\n",
        "'delinq_2yrs:0',\n",
        "'delinq_2yrs:1-3',\n",
        "'delinq_2yrs:>=4',\n",
        "'inq_last_6mths:0',\n",
        "'inq_last_6mths:1-2',\n",
        "'inq_last_6mths:3-6',\n",
        "'inq_last_6mths:>6',\n",
        "'open_acc:0',\n",
        "'open_acc:1-3',\n",
        "'open_acc:4-12',\n",
        "'open_acc:13-17',\n",
        "'open_acc:18-22',\n",
        "'open_acc:23-25',\n",
        "'open_acc:26-30',\n",
        "'open_acc:>=31',\n",
        "'pub_rec:0-2',\n",
        "'pub_rec:3-4',\n",
        "'pub_rec:>=5',\n",
        "'total_acc:<=27',\n",
        "'total_acc:28-51',\n",
        "'total_acc:>=52',\n",
        "'acc_now_delinq:0',\n",
        "'acc_now_delinq:>=1',\n",
        "'total_rev_hi_lim:<=5K',\n",
        "'total_rev_hi_lim:5K-10K',\n",
        "'total_rev_hi_lim:10K-20K',\n",
        "'total_rev_hi_lim:20K-30K',\n",
        "'total_rev_hi_lim:30K-40K',\n",
        "'total_rev_hi_lim:40K-55K',\n",
        "'total_rev_hi_lim:55K-95K',\n",
        "'total_rev_hi_lim:>95K',\n",
        "'annual_inc:<20K',\n",
        "'annual_inc:20K-30K',\n",
        "'annual_inc:30K-40K',\n",
        "'annual_inc:40K-50K',\n",
        "'annual_inc:50K-60K',\n",
        "'annual_inc:60K-70K',\n",
        "'annual_inc:70K-80K',\n",
        "'annual_inc:80K-90K',\n",
        "'annual_inc:90K-100K',\n",
        "'annual_inc:100K-120K',\n",
        "'annual_inc:120K-140K',\n",
        "'annual_inc:>140K',\n",
        "'dti:<=1.4',\n",
        "'dti:1.4-3.5',\n",
        "'dti:3.5-7.7',\n",
        "'dti:7.7-10.5',\n",
        "'dti:10.5-16.1',\n",
        "'dti:16.1-20.3',\n",
        "'dti:20.3-21.7',\n",
        "'dti:21.7-22.4',\n",
        "'dti:22.4-35',\n",
        "'dti:>35',\n",
        "'mths_since_last_delinq:Missing',\n",
        "'mths_since_last_delinq:0-3',\n",
        "'mths_since_last_delinq:4-30',\n",
        "'mths_since_last_delinq:31-56',\n",
        "'mths_since_last_delinq:>=57',\n",
        "'mths_since_last_record:Missing',\n",
        "'mths_since_last_record:0-2',\n",
        "'mths_since_last_record:3-20',\n",
        "'mths_since_last_record:21-31',\n",
        "'mths_since_last_record:32-80',\n",
        "'mths_since_last_record:81-86',\n",
        "'mths_since_last_record:>=86',\n",
        "]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEMU8h8-vWqE"
      },
      "outputs": [],
      "source": [
        "# Here we store the names of the reference category dummy variables in a list.\n",
        "ref_categories = ['grade:G',\n",
        "'home_ownership:RENT_OTHER_NONE_ANY',\n",
        "'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
        "'verification_status:Verified',\n",
        "'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
        "'initial_list_status:f',\n",
        "'term:60',\n",
        "'emp_length:0',\n",
        "'mths_since_issue_d:>84',\n",
        "'int_rate:>20.281',\n",
        "'mths_since_earliest_cr_line:<140',\n",
        "'delinq_2yrs:>=4',\n",
        "'inq_last_6mths:>6',\n",
        "'open_acc:0',\n",
        "'pub_rec:0-2',\n",
        "'total_acc:<=27',\n",
        "'acc_now_delinq:0',\n",
        "'total_rev_hi_lim:<=5K',\n",
        "'annual_inc:<20K',\n",
        "'dti:>35',\n",
        "'mths_since_last_delinq:0-3',\n",
        "'mths_since_last_record:0-2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5hbQ1y4vWqE",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "inputs_train = inputs_train_with_ref_cat.drop(ref_categories, axis = 1)\n",
        "# From the dataframe with input variables, we drop the variables with variable names in the list with reference categories.\n",
        "inputs_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPnsWh_WvWqE"
      },
      "source": [
        "# PD Model Estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQPT8hrevWqE"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gl1dOnXvWqE"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzVQo0T1whGD"
      },
      "outputs": [],
      "source": [
        "reg = LogisticRegression()\n",
        "# We create an instance of an object from the 'LogisticRegression' class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IztP0lxw4Xw"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_rows = None\n",
        "# Sets the pandas dataframe options to display all columns/ rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giSZ9kAaw6jO"
      },
      "outputs": [],
      "source": [
        "reg.fit(inputs_train, loan_data_targets_train)\n",
        "# Estimates the coefficients of the object from the 'LogisticRegression' class\n",
        "# with inputs (independent variables) contained in the first dataframe\n",
        "# and targets (dependent variables) contained in the second dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNzr3QFrw9jf"
      },
      "outputs": [],
      "source": [
        "reg.intercept_\n",
        "# Displays the intercept contain in the estimated (\"fitted\") object from the 'LogisticRegression' class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_U9NkcXtxFgD"
      },
      "outputs": [],
      "source": [
        "reg.coef_\n",
        "# Displays the coefficients contained in the estimated (\"fitted\") object from the 'LogisticRegression' class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x_N7heJxHbM"
      },
      "outputs": [],
      "source": [
        "feature_name = inputs_train.columns.values\n",
        "# Stores the names of the columns of a dataframe in a variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glByJYnTxImJ"
      },
      "outputs": [],
      "source": [
        "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
        "# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\n",
        "summary_table['Coefficients'] = np.transpose(reg.coef_)\n",
        "# Creates a new column in the dataframe, called 'Coefficients',\n",
        "# with row values the transposed coefficients from the 'LogisticRegression' object.\n",
        "summary_table.index = summary_table.index + 1\n",
        "# Increases the index of every row of the dataframe with 1.\n",
        "summary_table.loc[0] = ['Intercept', reg.intercept_[0]]\n",
        "# Assigns values of the row with index 0 of the dataframe.\n",
        "summary_table = summary_table.sort_index()\n",
        "# Sorts the dataframe by index.\n",
        "summary_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neg9rSwcvWqH"
      },
      "source": [
        "## Build a Logistic Regression Model with P-Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fEK8u_qxK6R"
      },
      "outputs": [],
      "source": [
        "# P values for sklearn logistic regression.\n",
        "\n",
        "# Class to display p-values for logistic regression in sklearn.\n",
        "\n",
        "from sklearn import linear_model\n",
        "import scipy.stats as stat\n",
        "\n",
        "class LogisticRegression_with_p_values:\n",
        "\n",
        "    def __init__(self,*args,**kwargs):#,**kwargs):\n",
        "        self.model = linear_model.LogisticRegression(*args,**kwargs)#,**args)\n",
        "\n",
        "    def fit(self,X,y):\n",
        "        self.model.fit(X,y)\n",
        "\n",
        "        #### Get p-values for the fitted model ####\n",
        "        denom = (2.0 * (1.0 + np.cosh(self.model.decision_function(X))))\n",
        "        denom = np.tile(denom,(X.shape[1],1)).T\n",
        "        F_ij = np.dot((X / denom).T,X) ## Fisher Information Matrix\n",
        "        Cramer_Rao = np.linalg.inv(F_ij) ## Inverse Information Matrix\n",
        "        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n",
        "        z_scores = self.model.coef_[0] / sigma_estimates # z-score for eaach model coefficient\n",
        "        p_values = [stat.norm.sf(abs(x)) * 2 for x in z_scores] ### two tailed test for p-values\n",
        "\n",
        "        self.coef_ = self.model.coef_\n",
        "        self.intercept_ = self.model.intercept_\n",
        "        #self.z_scores = z_scores\n",
        "        self.p_values = p_values\n",
        "        #self.sigma_estimates = sigma_estimates\n",
        "        #self.F_ij = F_ij"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYYJMiwexO6n"
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model\n",
        "import scipy.stats as stat\n",
        "\n",
        "class LogisticRegression_with_p_values:\n",
        "\n",
        "    def __init__(self,*args,**kwargs):\n",
        "        self.model = linear_model.LogisticRegression(*args,**kwargs)\n",
        "\n",
        "    def fit(self,X,y):\n",
        "        self.model.fit(X,y)\n",
        "        denom = (2.0 * (1.0 + np.cosh(self.model.decision_function(X))))\n",
        "        denom = np.tile(denom,(X.shape[1],1)).T\n",
        "        F_ij = np.dot((X / denom).T,X)\n",
        "        Cramer_Rao = np.linalg.inv(F_ij)\n",
        "        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n",
        "        z_scores = self.model.coef_[0] / sigma_estimates\n",
        "        p_values = [stat.norm.sf(abs(x)) * 2 for x in z_scores]\n",
        "        self.coef_ = self.model.coef_\n",
        "        self.intercept_ = self.model.intercept_\n",
        "        self.p_values = p_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUGhpx6RxQkH"
      },
      "outputs": [],
      "source": [
        "reg = LogisticRegression_with_p_values()\n",
        "# We create an instance of an object from the newly created 'LogisticRegression_with_p_values()' class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwjZtx_zxRyy"
      },
      "outputs": [],
      "source": [
        "reg.fit(inputs_train, loan_data_targets_train)\n",
        "# Estimates the coefficients of the object from the 'LogisticRegression' class\n",
        "# with inputs (independent variables) contained in the first dataframe\n",
        "# and targets (dependent variables) contained in the second dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKWlzU0qxS59"
      },
      "outputs": [],
      "source": [
        "# Same as above.\n",
        "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
        "summary_table['Coefficients'] = np.transpose(reg.coef_)\n",
        "summary_table.index = summary_table.index + 1\n",
        "summary_table.loc[0] = ['Intercept', reg.intercept_[0]]\n",
        "summary_table = summary_table.sort_index()\n",
        "summary_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uyb1NVpxUtm"
      },
      "outputs": [],
      "source": [
        "# This is a list.\n",
        "p_values = reg.p_values\n",
        "# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uoe50WWSxWpe"
      },
      "outputs": [],
      "source": [
        "# Add the intercept for completeness.\n",
        "p_values = np.append(np.nan, np.array(p_values))\n",
        "# We add the value 'NaN' in the beginning of the variable with p-values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xx0dMlB0xXw0"
      },
      "outputs": [],
      "source": [
        "summary_table['p_values'] = p_values\n",
        "# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU5SuKI_xZAb"
      },
      "outputs": [],
      "source": [
        "summary_table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYsshT6jxawm"
      },
      "outputs": [],
      "source": [
        "# We are going to remove some features, the coefficients for all or almost all of the dummy variables for which,\n",
        "# are not tatistically significant.\n",
        "\n",
        "# We do that by specifying another list of dummy variables as reference categories, and a list of variables to remove.\n",
        "# Then, we are going to drop the two datasets from the original list of dummy variables.\n",
        "\n",
        "# Variables\n",
        "inputs_train_with_ref_cat = loan_data_inputs_train.loc[: , ['grade:A',\n",
        "'grade:B',\n",
        "'grade:C',\n",
        "'grade:D',\n",
        "'grade:E',\n",
        "'grade:F',\n",
        "'grade:G',\n",
        "'home_ownership:RENT_OTHER_NONE_ANY',\n",
        "'home_ownership:OWN',\n",
        "'home_ownership:MORTGAGE',\n",
        "'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
        "'addr_state:NM_VA',\n",
        "'addr_state:NY',\n",
        "'addr_state:OK_TN_MO_LA_MD_NC',\n",
        "'addr_state:CA',\n",
        "'addr_state:UT_KY_AZ_NJ',\n",
        "'addr_state:AR_MI_PA_OH_MN',\n",
        "'addr_state:RI_MA_DE_SD_IN',\n",
        "'addr_state:GA_WA_OR',\n",
        "'addr_state:WI_MT',\n",
        "'addr_state:TX',\n",
        "'addr_state:IL_CT',\n",
        "'addr_state:KS_SC_CO_VT_AK_MS',\n",
        "'addr_state:WV_NH_WY_DC_ME_ID',\n",
        "'verification_status:Not Verified',\n",
        "'verification_status:Source Verified',\n",
        "'verification_status:Verified',\n",
        "'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
        "'purpose:credit_card',\n",
        "'purpose:debt_consolidation',\n",
        "'purpose:oth__med__vacation',\n",
        "'purpose:major_purch__car__home_impr',\n",
        "'initial_list_status:f',\n",
        "'initial_list_status:w',\n",
        "'term:36',\n",
        "'term:60',\n",
        "'emp_length:0',\n",
        "'emp_length:1',\n",
        "'emp_length:2-4',\n",
        "'emp_length:5-6',\n",
        "'emp_length:7-9',\n",
        "'emp_length:10',\n",
        "'mths_since_issue_d:<38',\n",
        "'mths_since_issue_d:38-39',\n",
        "'mths_since_issue_d:40-41',\n",
        "'mths_since_issue_d:42-48',\n",
        "'mths_since_issue_d:49-52',\n",
        "'mths_since_issue_d:53-64',\n",
        "'mths_since_issue_d:65-84',\n",
        "'mths_since_issue_d:>84',\n",
        "'int_rate:<9.548',\n",
        "'int_rate:9.548-12.025',\n",
        "'int_rate:12.025-15.74',\n",
        "'int_rate:15.74-20.281',\n",
        "'int_rate:>20.281',\n",
        "'mths_since_earliest_cr_line:<140',\n",
        "'mths_since_earliest_cr_line:141-164',\n",
        "'mths_since_earliest_cr_line:165-247',\n",
        "'mths_since_earliest_cr_line:248-270',\n",
        "'mths_since_earliest_cr_line:271-352',\n",
        "'mths_since_earliest_cr_line:>352',\n",
        "'inq_last_6mths:0',\n",
        "'inq_last_6mths:1-2',\n",
        "'inq_last_6mths:3-6',\n",
        "'inq_last_6mths:>6',\n",
        "'acc_now_delinq:0',\n",
        "'acc_now_delinq:>=1',\n",
        "'annual_inc:<20K',\n",
        "'annual_inc:20K-30K',\n",
        "'annual_inc:30K-40K',\n",
        "'annual_inc:40K-50K',\n",
        "'annual_inc:50K-60K',\n",
        "'annual_inc:60K-70K',\n",
        "'annual_inc:70K-80K',\n",
        "'annual_inc:80K-90K',\n",
        "'annual_inc:90K-100K',\n",
        "'annual_inc:100K-120K',\n",
        "'annual_inc:120K-140K',\n",
        "'annual_inc:>140K',\n",
        "'dti:<=1.4',\n",
        "'dti:1.4-3.5',\n",
        "'dti:3.5-7.7',\n",
        "'dti:7.7-10.5',\n",
        "'dti:10.5-16.1',\n",
        "'dti:16.1-20.3',\n",
        "'dti:20.3-21.7',\n",
        "'dti:21.7-22.4',\n",
        "'dti:22.4-35',\n",
        "'dti:>35',\n",
        "'mths_since_last_delinq:Missing',\n",
        "'mths_since_last_delinq:0-3',\n",
        "'mths_since_last_delinq:4-30',\n",
        "'mths_since_last_delinq:31-56',\n",
        "'mths_since_last_delinq:>=57',\n",
        "'mths_since_last_record:Missing',\n",
        "'mths_since_last_record:0-2',\n",
        "'mths_since_last_record:3-20',\n",
        "'mths_since_last_record:21-31',\n",
        "'mths_since_last_record:32-80',\n",
        "'mths_since_last_record:81-86',\n",
        "'mths_since_last_record:>=86',\n",
        "]]\n",
        "\n",
        "ref_categories = ['grade:G',\n",
        "'home_ownership:RENT_OTHER_NONE_ANY',\n",
        "'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
        "'verification_status:Verified',\n",
        "'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
        "'initial_list_status:f',\n",
        "'term:60',\n",
        "'emp_length:0',\n",
        "'mths_since_issue_d:>84',\n",
        "'int_rate:>20.281',\n",
        "'mths_since_earliest_cr_line:<140',\n",
        "'inq_last_6mths:>6',\n",
        "'acc_now_delinq:0',\n",
        "'annual_inc:<20K',\n",
        "'dti:>35',\n",
        "'mths_since_last_delinq:0-3',\n",
        "'mths_since_last_record:0-2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M068zc7Lxcfi"
      },
      "outputs": [],
      "source": [
        "inputs_train = inputs_train_with_ref_cat.drop(ref_categories, axis = 1)\n",
        "inputs_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8mOIl1jxddZ"
      },
      "outputs": [],
      "source": [
        "# Here we run a new model.\n",
        "reg2 = LogisticRegression_with_p_values()\n",
        "reg2.fit(inputs_train, loan_data_targets_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5flCU__xehi"
      },
      "outputs": [],
      "source": [
        "feature_name = inputs_train.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFrO6aJBxwJe"
      },
      "outputs": [],
      "source": [
        "# Same as above.\n",
        "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
        "summary_table['Coefficients'] = np.transpose(reg2.coef_)\n",
        "summary_table.index = summary_table.index + 1\n",
        "summary_table.loc[0] = ['Intercept', reg2.intercept_[0]]\n",
        "summary_table = summary_table.sort_index()\n",
        "summary_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Efsug93KxyJx"
      },
      "outputs": [],
      "source": [
        "# We add the 'p_values' here, just as we did before.\n",
        "p_values = reg2.p_values\n",
        "p_values = np.append(np.nan,np.array(p_values))\n",
        "summary_table['p_values'] = p_values\n",
        "summary_table\n",
        "# Here we get the results for our final PD model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvM-WbCpxzmR"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLY_ojCnx0x5"
      },
      "outputs": [],
      "source": [
        "pickle.dump(reg2, open('pd_model.sav', 'wb'))\n",
        "# Here we export our model to a 'SAV' file with file name 'pd_model.sav'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byGcU3Prx4di"
      },
      "source": [
        "# PD Model Validation (Test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ktv4Qqzx60m"
      },
      "source": [
        "### Out-of-sample validation (test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVlQpwR5x6Xq"
      },
      "outputs": [],
      "source": [
        "# Here, from the dataframe with inputs for testing, we keep the same variables that we used in our final PD model.\n",
        "inputs_test_with_ref_cat = loan_data_inputs_test.loc[: , ['grade:A',\n",
        "'grade:B',\n",
        "'grade:C',\n",
        "'grade:D',\n",
        "'grade:E',\n",
        "'grade:F',\n",
        "'grade:G',\n",
        "'home_ownership:RENT_OTHER_NONE_ANY',\n",
        "'home_ownership:OWN',\n",
        "'home_ownership:MORTGAGE',\n",
        "'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
        "'addr_state:NM_VA',\n",
        "'addr_state:NY',\n",
        "'addr_state:OK_TN_MO_LA_MD_NC',\n",
        "'addr_state:CA',\n",
        "'addr_state:UT_KY_AZ_NJ',\n",
        "'addr_state:AR_MI_PA_OH_MN',\n",
        "'addr_state:RI_MA_DE_SD_IN',\n",
        "'addr_state:GA_WA_OR',\n",
        "'addr_state:WI_MT',\n",
        "'addr_state:TX',\n",
        "'addr_state:IL_CT',\n",
        "'addr_state:KS_SC_CO_VT_AK_MS',\n",
        "'addr_state:WV_NH_WY_DC_ME_ID',\n",
        "'verification_status:Not Verified',\n",
        "'verification_status:Source Verified',\n",
        "'verification_status:Verified',\n",
        "'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
        "'purpose:credit_card',\n",
        "'purpose:debt_consolidation',\n",
        "'purpose:oth__med__vacation',\n",
        "'purpose:major_purch__car__home_impr',\n",
        "'initial_list_status:f',\n",
        "'initial_list_status:w',\n",
        "'term:36',\n",
        "'term:60',\n",
        "'emp_length:0',\n",
        "'emp_length:1',\n",
        "'emp_length:2-4',\n",
        "'emp_length:5-6',\n",
        "'emp_length:7-9',\n",
        "'emp_length:10',\n",
        "'mths_since_issue_d:<38',\n",
        "'mths_since_issue_d:38-39',\n",
        "'mths_since_issue_d:40-41',\n",
        "'mths_since_issue_d:42-48',\n",
        "'mths_since_issue_d:49-52',\n",
        "'mths_since_issue_d:53-64',\n",
        "'mths_since_issue_d:65-84',\n",
        "'mths_since_issue_d:>84',\n",
        "'int_rate:<9.548',\n",
        "'int_rate:9.548-12.025',\n",
        "'int_rate:12.025-15.74',\n",
        "'int_rate:15.74-20.281',\n",
        "'int_rate:>20.281',\n",
        "'mths_since_earliest_cr_line:<140',\n",
        "'mths_since_earliest_cr_line:141-164',\n",
        "'mths_since_earliest_cr_line:165-247',\n",
        "'mths_since_earliest_cr_line:248-270',\n",
        "'mths_since_earliest_cr_line:271-352',\n",
        "'mths_since_earliest_cr_line:>352',\n",
        "'inq_last_6mths:0',\n",
        "'inq_last_6mths:1-2',\n",
        "'inq_last_6mths:3-6',\n",
        "'inq_last_6mths:>6',\n",
        "'acc_now_delinq:0',\n",
        "'acc_now_delinq:>=1',\n",
        "'annual_inc:<20K',\n",
        "'annual_inc:20K-30K',\n",
        "'annual_inc:30K-40K',\n",
        "'annual_inc:40K-50K',\n",
        "'annual_inc:50K-60K',\n",
        "'annual_inc:60K-70K',\n",
        "'annual_inc:70K-80K',\n",
        "'annual_inc:80K-90K',\n",
        "'annual_inc:90K-100K',\n",
        "'annual_inc:100K-120K',\n",
        "'annual_inc:120K-140K',\n",
        "'annual_inc:>140K',\n",
        "'dti:<=1.4',\n",
        "'dti:1.4-3.5',\n",
        "'dti:3.5-7.7',\n",
        "'dti:7.7-10.5',\n",
        "'dti:10.5-16.1',\n",
        "'dti:16.1-20.3',\n",
        "'dti:20.3-21.7',\n",
        "'dti:21.7-22.4',\n",
        "'dti:22.4-35',\n",
        "'dti:>35',\n",
        "'mths_since_last_delinq:Missing',\n",
        "'mths_since_last_delinq:0-3',\n",
        "'mths_since_last_delinq:4-30',\n",
        "'mths_since_last_delinq:31-56',\n",
        "'mths_since_last_delinq:>=57',\n",
        "'mths_since_last_record:Missing',\n",
        "'mths_since_last_record:0-2',\n",
        "'mths_since_last_record:3-20',\n",
        "'mths_since_last_record:21-31',\n",
        "'mths_since_last_record:32-80',\n",
        "'mths_since_last_record:81-86',\n",
        "'mths_since_last_record:>=86',\n",
        "]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otdSIZcuyAzG"
      },
      "outputs": [],
      "source": [
        "# And here, in the list below, we keep the variable names for the reference categories,\n",
        "# only for the variables we used in our final PD model.\n",
        "ref_categories = ['grade:G',\n",
        "'home_ownership:RENT_OTHER_NONE_ANY',\n",
        "'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
        "'verification_status:Verified',\n",
        "'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
        "'initial_list_status:f',\n",
        "'term:60',\n",
        "'emp_length:0',\n",
        "'mths_since_issue_d:>84',\n",
        "'int_rate:>20.281',\n",
        "'mths_since_earliest_cr_line:<140',\n",
        "'inq_last_6mths:>6',\n",
        "'acc_now_delinq:0',\n",
        "'annual_inc:<20K',\n",
        "'dti:>35',\n",
        "'mths_since_last_delinq:0-3',\n",
        "'mths_since_last_record:0-2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfh4Nrq6yCI9"
      },
      "outputs": [],
      "source": [
        "inputs_test = inputs_test_with_ref_cat.drop(ref_categories, axis = 1)\n",
        "inputs_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwXFsnV5yDd5"
      },
      "outputs": [],
      "source": [
        "y_hat_test = reg2.model.predict(inputs_test)\n",
        "# Calculates the predicted values for the dependent variable (targets)\n",
        "# based on the values of the independent variables (inputs) supplied as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL6Izpf4yEpq"
      },
      "outputs": [],
      "source": [
        "y_hat_test\n",
        "# This is an array of predicted discrete classess (in this case, 0s and 1s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ6uZKtZyFsp"
      },
      "outputs": [],
      "source": [
        "y_hat_test_proba = reg2.model.predict_proba(inputs_test)\n",
        "# Calculates the predicted probability values for the dependent variable (targets)\n",
        "# based on the values of the independent variables (inputs) supplied as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw0tt2I8yHzc"
      },
      "outputs": [],
      "source": [
        "y_hat_test_proba\n",
        "# This is an array of arrays of predicted class probabilities for all classes.\n",
        "# In this case, the first value of every sub-array is the probability for the observation to belong to the first class, i.e. 0,\n",
        "# and the second value is the probability for the observation to belong to the first class, i.e. 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_Nz2883yJ1L"
      },
      "outputs": [],
      "source": [
        "y_hat_test_proba[:][:,1]\n",
        "# Here we take all the arrays in the array, and from each array, we take all rows, and only the element with index 1,\n",
        "# that is, the second element.\n",
        "# In other words, we take only the probabilities for being 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8UrZlo7yLgJ"
      },
      "outputs": [],
      "source": [
        "y_hat_test_proba = y_hat_test_proba[: ][: , 1]\n",
        "# We store these probabilities in a variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLNyeOciyPu1"
      },
      "outputs": [],
      "source": [
        "y_hat_test_proba\n",
        "# This variable contains an array of probabilities of being 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCyfOaqtyRk_"
      },
      "outputs": [],
      "source": [
        "loan_data_targets_test_temp = loan_data_targets_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrI3cTXqyTFc"
      },
      "outputs": [],
      "source": [
        "loan_data_targets_test_temp.reset_index(drop = True, inplace = True)\n",
        "# We reset the index of a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvnwG5mVyURH"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs = pd.concat([loan_data_targets_test_temp, pd.DataFrame(y_hat_test_proba)], axis = 1)\n",
        "# Concatenates two dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPV4iXpRyVWn"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSWAAsZ9yWVs"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs.columns = ['loan_data_targets_test', 'y_hat_test_proba']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du35pyryyYrL"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs.index = loan_data_inputs_test.index\n",
        "# Makes the index of one dataframe equal to the index of another dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdmm2E1lyaTR"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xguMi_ebydFh"
      },
      "source": [
        "### Accuracy and Area under the Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGN0d49ydrI"
      },
      "outputs": [],
      "source": [
        "tr = 0.9\n",
        "# We create a new column with an indicator,\n",
        "# where every observation that has predicted probability greater than the threshold has a value of 1,\n",
        "# and every observation that has predicted probability lower than the threshold has a value of 0.\n",
        "df_actual_predicted_probs['y_hat_test'] = np.where(df_actual_predicted_probs['y_hat_test_proba'] > tr, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPMOU8izyfyz"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted'])\n",
        "# Creates a cross-table where the actual values are displayed by rows and the predicted values by columns.\n",
        "# This table is known as a Confusion Matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97sztXenye-o"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]\n",
        "# Here we divide each value of the table by the total number of observations,\n",
        "# thus getting percentages, or, rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M9TEm8zyp2I"
      },
      "outputs": [],
      "source": [
        "(pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[0, 0] + (pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[1, 1]\n",
        "# Here we calculate Accuracy of the model, which is the sum of the diagonal rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44AkUqGgyrYI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlA3abRiyvPH"
      },
      "outputs": [],
      "source": [
        "roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])\n",
        "# Returns the Receiver Operating Characteristic (ROC) Curve from a set of actual values and their predicted probabilities.\n",
        "# As a result, we get three arrays: the false positive rates, the true positive rates, and the thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sPt2Yvjyw_d"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])\n",
        "# Here we store each of the three arrays in a separate variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTWJYD-2yyPX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8iUY2f0yze8"
      },
      "outputs": [],
      "source": [
        "plt.plot(fpr, tpr)\n",
        "# We plot the false positive rate along the x-axis and the true positive rate along the y-axis,\n",
        "# thus plotting the ROC curve.\n",
        "plt.plot(fpr, fpr, linestyle = '--', color = 'k')\n",
        "# We plot a seconary diagonal line, with dashed line style and black color.\n",
        "plt.xlabel('False positive rate')\n",
        "# We name the x-axis \"False positive rate\".\n",
        "plt.ylabel('True positive rate')\n",
        "# We name the x-axis \"True positive rate\".\n",
        "plt.title('ROC curve')\n",
        "# We name the graph \"ROC curve\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8bZlYdIy1EY"
      },
      "outputs": [],
      "source": [
        "AUROC = roc_auc_score(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])\n",
        "# Calculates the Area Under the Receiver Operating Characteristic Curve (AUROC)\n",
        "# from a set of actual values and their predicted probabilities.\n",
        "AUROC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZpZqg0Yy3D-"
      },
      "source": [
        "*italicised text*### Gini and Kolmogorov-Smirnov"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtXezIFNy6fM"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs = df_actual_predicted_probs.sort_values('y_hat_test_proba')\n",
        "# Sorts a dataframe by the values of a specific column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_ukBcsey8su"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnxU4EVBy-QB"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kygn6n9y_hj"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs = df_actual_predicted_probs.reset_index()\n",
        "# We reset the index of a dataframe and overwrite it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j64STFwEzA17"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piKA5KN0zFTY"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs['Cumulative N Population'] = df_actual_predicted_probs.index + 1\n",
        "# We calculate the cumulative number of all observations.\n",
        "# We use the new index for that. Since indexing in ython starts from 0, we add 1 to each index.\n",
        "df_actual_predicted_probs['Cumulative N Good'] = df_actual_predicted_probs['loan_data_targets_test'].cumsum()\n",
        "# We calculate cumulative number of 'good', which is the cumulative sum of the column with actual observations.\n",
        "df_actual_predicted_probs['Cumulative N Bad'] = df_actual_predicted_probs['Cumulative N Population'] - df_actual_predicted_probs['loan_data_targets_test'].cumsum()\n",
        "# We calculate cumulative number of 'bad', which is\n",
        "# the difference between the cumulative number of all observations and cumulative number of 'good' for each row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PK8Q__jzG54"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LWJB3V9zICa"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs['Cumulative Perc Population'] = df_actual_predicted_probs['Cumulative N Population'] / (df_actual_predicted_probs.shape[0])\n",
        "# We calculate the cumulative percentage of all observations.\n",
        "df_actual_predicted_probs['Cumulative Perc Good'] = df_actual_predicted_probs['Cumulative N Good'] / df_actual_predicted_probs['loan_data_targets_test'].sum()\n",
        "# We calculate cumulative percentage of 'good'.\n",
        "df_actual_predicted_probs['Cumulative Perc Bad'] = df_actual_predicted_probs['Cumulative N Bad'] / (df_actual_predicted_probs.shape[0] - df_actual_predicted_probs['loan_data_targets_test'].sum())\n",
        "# We calculate the cumulative percentage of 'bad'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ht97EfnzU3G"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA-X7NKmzW6q"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs.tail()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7DxFoOUzYM2"
      },
      "outputs": [],
      "source": [
        "# Plot Gini\n",
        "plt.plot(df_actual_predicted_probs['Cumulative Perc Population'], df_actual_predicted_probs['Cumulative Perc Bad'])\n",
        "# We plot the cumulative percentage of all along the x-axis and the cumulative percentage 'good' along the y-axis,\n",
        "# thus plotting the Gini curve.\n",
        "plt.plot(df_actual_predicted_probs['Cumulative Perc Population'], df_actual_predicted_probs['Cumulative Perc Population'], linestyle = '--', color = 'k')\n",
        "# We plot a seconary diagonal line, with dashed line style and black color.\n",
        "plt.xlabel('Cumulative % Population')\n",
        "# We name the x-axis \"Cumulative % Population\".\n",
        "plt.ylabel('Cumulative % Bad')\n",
        "# We name the y-axis \"Cumulative % Bad\".\n",
        "plt.title('Gini')\n",
        "# We name the graph \"Gini\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDcWX6RvzZ2f"
      },
      "outputs": [],
      "source": [
        "Gini = AUROC * 2 - 1\n",
        "# Here we calculate Gini from AUROC.\n",
        "Gini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tuxtk5qzayX"
      },
      "outputs": [],
      "source": [
        "# Plot KS\n",
        "plt.plot(df_actual_predicted_probs['y_hat_test_proba'], df_actual_predicted_probs['Cumulative Perc Bad'], color = 'r')\n",
        "# We plot the predicted (estimated) probabilities along the x-axis and the cumulative percentage 'bad' along the y-axis,\n",
        "# colored in red.\n",
        "plt.plot(df_actual_predicted_probs['y_hat_test_proba'], df_actual_predicted_probs['Cumulative Perc Good'], color = 'b')\n",
        "# We plot the predicted (estimated) probabilities along the x-axis and the cumulative percentage 'good' along the y-axis,\n",
        "# colored in red.\n",
        "plt.xlabel('Estimated Probability for being Good')\n",
        "# We name the x-axis \"Estimated Probability for being Good\".\n",
        "plt.ylabel('Cumulative %')\n",
        "# We name the y-axis \"Cumulative %\".\n",
        "plt.title('Kolmogorov-Smirnov')\n",
        "# We name the graph \"Kolmogorov-Smirnov\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQtIUuKczcYO"
      },
      "outputs": [],
      "source": [
        "KS = max(df_actual_predicted_probs['Cumulative Perc Bad'] - df_actual_predicted_probs['Cumulative Perc Good'])\n",
        "# We calculate KS from the data. It is the maximum of the difference between the cumulative percentage of 'bad'\n",
        "# and the cumulative percentage of 'good'.\n",
        "KS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i5AeoLw0hDw"
      },
      "source": [
        "# Applying the PD Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAFt2byG0hDw"
      },
      "source": [
        "### Calculating PD of individual accounts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc4f69480v3N"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_columns = None\n",
        "# Sets the pandas dataframe options to display all columns/ rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDo5wqJn0w_V"
      },
      "outputs": [],
      "source": [
        "inputs_test_with_ref_cat.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04gxOS4W0x8D"
      },
      "outputs": [],
      "source": [
        "summary_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rs4tb3JX0zAS"
      },
      "outputs": [],
      "source": [
        "y_hat_test_proba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50WGdgpi0hDx"
      },
      "source": [
        "### Creating a Scorecard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuRq_3Jc0hDx"
      },
      "outputs": [],
      "source": [
        "summary_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atH8OuSd0hDx"
      },
      "outputs": [],
      "source": [
        "ref_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb5lnm7q1AOY"
      },
      "outputs": [],
      "source": [
        "df_ref_categories = pd.DataFrame(ref_categories, columns = ['Feature name'])\n",
        "# We create a new dataframe with one column. Its values are the values from the 'reference_categories' list.\n",
        "# We name it 'Feature name'.\n",
        "df_ref_categories['Coefficients'] = 0\n",
        "# We create a second column, called 'Coefficients', which contains only 0 values.\n",
        "df_ref_categories['p_values'] = np.nan\n",
        "# We create a third column, called 'p_values', with contains only NaN values.\n",
        "df_ref_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUy3E05z1Bi7"
      },
      "outputs": [],
      "source": [
        "df_scorecard = pd.concat([summary_table, df_ref_categories])\n",
        "# Concatenates two dataframes.\n",
        "df_scorecard = df_scorecard.reset_index()\n",
        "# We reset the index of a dataframe.\n",
        "df_scorecard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BT_qm7E81DFg"
      },
      "outputs": [],
      "source": [
        "df_scorecard['Original feature name'] = df_scorecard['Feature name'].str.split(':').str[0]\n",
        "# We create a new column, called 'Original feature name', which contains the value of the 'Feature name' column,\n",
        "# up to the column symbol.\n",
        "df_scorecard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjxd5IWA1EaE"
      },
      "outputs": [],
      "source": [
        "min_score = 300\n",
        "max_score = 850"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7YBzxWr1FpN"
      },
      "outputs": [],
      "source": [
        "df_scorecard.groupby('Original feature name')['Coefficients'].min()\n",
        "# Groups the data by the values of the 'Original feature name' column.\n",
        "# Aggregates the data in the 'Coefficients' column, calculating their minimum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx5Q87Gs1G7y"
      },
      "outputs": [],
      "source": [
        "min_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].min().sum()\n",
        "# Up to the 'min()' method everything is the same as in te line above.\n",
        "# Then, we aggregate further and sum all the minimum values.\n",
        "min_sum_coef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6r8n3cx1IFM"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_scorecard.groupby('Original feature name')['Coefficients'].max()\n",
        "# Groups the data by the values of the 'Original feature name' column.\n",
        "# Aggregates the data in the 'Coefficients' column, calculating their maximum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnnnLYs81JgH"
      },
      "outputs": [],
      "source": [
        "max_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].max().sum()\n",
        "# Up to the 'min()' method everything is the same as in te line above.\n",
        "# Then, we aggregate further and sum all the maximum values.\n",
        "max_sum_coef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYys9veF1K35"
      },
      "outputs": [],
      "source": [
        "df_scorecard['Score - Calculation'] = df_scorecard['Coefficients'] * (max_score - min_score) / (max_sum_coef - min_sum_coef)\n",
        "# We multiply the value of the 'Coefficients' column by the ration of the differences between\n",
        "# maximum score and minimum score and maximum sum of coefficients and minimum sum of cefficients.\n",
        "df_scorecard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "himaVBdj1McN"
      },
      "outputs": [],
      "source": [
        "df_scorecard['Score - Calculation'][0] = ((df_scorecard['Coefficients'][0] - min_sum_coef) / (max_sum_coef - min_sum_coef)) * (max_score - min_score) + min_score\n",
        "# We divide the difference of the value of the 'Coefficients' column and the minimum sum of coefficients by\n",
        "# the difference of the maximum sum of coefficients and the minimum sum of coefficients.\n",
        "# Then, we multiply that by the difference between the maximum score and the minimum score.\n",
        "# Then, we add minimum score.\n",
        "df_scorecard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyE2BZq31Omr"
      },
      "outputs": [],
      "source": [
        "df_scorecard['Score - Preliminary'] = df_scorecard['Score - Calculation'].round()\n",
        "# We round the values of the 'Score - Calculation' column.\n",
        "df_scorecard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv_CCuUK1PgN"
      },
      "outputs": [],
      "source": [
        "min_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].min().sum()\n",
        "# Groups the data by the values of the 'Original feature name' column.\n",
        "# Aggregates the data in the 'Coefficients' column, calculating their minimum.\n",
        "# Sums all minimum values.\n",
        "min_sum_score_prel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uswjJQP61Qcb"
      },
      "outputs": [],
      "source": [
        "max_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].max().sum()\n",
        "# Groups the data by the values of the 'Original feature name' column.\n",
        "# Aggregates the data in the 'Coefficients' column, calculating their maximum.\n",
        "# Sums all maximum values.\n",
        "max_sum_score_prel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVyVP3Yn1SuJ"
      },
      "outputs": [],
      "source": [
        "# One has to be subtracted from the maximum score for one original variable. Which one? We'll evaluate based on differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niEDxOLO1Ts9"
      },
      "outputs": [],
      "source": [
        "df_scorecard['Difference'] = df_scorecard['Score - Preliminary'] - df_scorecard['Score - Calculation']\n",
        "df_scorecard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuh-eoAw1UtB"
      },
      "outputs": [],
      "source": [
        "df_scorecard['Score - Final'] = df_scorecard['Score - Preliminary']\n",
        "df_scorecard['Score - Final'][77] = 16\n",
        "df_scorecard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCglnya41ifZ"
      },
      "outputs": [],
      "source": [
        "min_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Final'].min().sum()\n",
        "# Groups the data by the values of the 'Original feature name' column.\n",
        "# Aggregates the data in the 'Coefficients' column, calculating their minimum.\n",
        "# Sums all minimum values.\n",
        "min_sum_score_prel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_BWAWAq1kc2"
      },
      "outputs": [],
      "source": [
        "max_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Final'].max().sum()\n",
        "# Groups the data by the values of the 'Original feature name' column.\n",
        "# Aggregates the data in the 'Coefficients' column, calculating their maximum.\n",
        "# Sums all maximum values.\n",
        "max_sum_score_prel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onAdLabZ0hD0"
      },
      "source": [
        "### Caclulating Credit Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH0hlneV1mI9"
      },
      "outputs": [],
      "source": [
        "inputs_test_with_ref_cat.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzHmyQs_1ptE"
      },
      "outputs": [],
      "source": [
        "df_scorecard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6syfWow1qxx"
      },
      "outputs": [],
      "source": [
        "inputs_test_with_ref_cat_w_intercept = inputs_test_with_ref_cat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QZok2uo1r3C"
      },
      "outputs": [],
      "source": [
        "inputs_test_with_ref_cat_w_intercept.insert(0, 'Intercept', 1)\n",
        "# We insert a column in the dataframe, with an index of 0, that is, in the beginning of the dataframe.\n",
        "# The name of that column is 'Intercept', and its values are 1s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM-rhkSh1tiM"
      },
      "outputs": [],
      "source": [
        "inputs_test_with_ref_cat_w_intercept.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypOR9tbZ1xXY"
      },
      "outputs": [],
      "source": [
        "inputs_test_with_ref_cat_w_intercept = inputs_test_with_ref_cat_w_intercept[df_scorecard['Feature name'].values]\n",
        "# Here, from the 'inputs_test_with_ref_cat_w_intercept' dataframe, we keep only the columns with column names,\n",
        "# exactly equal to the row values of the 'Feature name' column from the 'df_scorecard' dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrXb20jD1yh4"
      },
      "outputs": [],
      "source": [
        "inputs_test_with_ref_cat_w_intercept.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuhJSGAL1zjq"
      },
      "outputs": [],
      "source": [
        "scorecard_scores = df_scorecard['Score - Final']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32iz8pu-10sz"
      },
      "outputs": [],
      "source": [
        "inputs_test_with_ref_cat_w_intercept.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J5EbNiS11q2"
      },
      "outputs": [],
      "source": [
        "scorecard_scores.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMl5eWHU13I6"
      },
      "outputs": [],
      "source": [
        "scorecard_scores = scorecard_scores.values.reshape(102, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_HUNu7A2QK1"
      },
      "outputs": [],
      "source": [
        "scorecard_scores.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M2V6AlT2RKx"
      },
      "outputs": [],
      "source": [
        "y_scores = inputs_test_with_ref_cat_w_intercept.dot(scorecard_scores)\n",
        "# Here we multiply the values of each row of the dataframe by the values of each column of the variable,\n",
        "# which is an argument of the 'dot' method, and sum them. It's essentially the sum of the products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPFFSCoT2SK-"
      },
      "outputs": [],
      "source": [
        "y_scores.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXeHhx242TUj"
      },
      "outputs": [],
      "source": [
        "y_scores.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcmIZnoY0hD1"
      },
      "source": [
        "### From Credit Score to PD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SuGZfLG2VaY"
      },
      "outputs": [],
      "source": [
        "sum_coef_from_score = ((y_scores - min_score) / (max_score - min_score)) * (max_sum_coef - min_sum_coef) + min_sum_coef\n",
        "# We divide the difference between the scores and the minimum score by\n",
        "# the difference between the maximum score and the minimum score.\n",
        "# Then, we multiply that by the difference between the maximum sum of coefficients and the minimum sum of coefficients.\n",
        "# Then, we add the minimum sum of coefficients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkD1w3n-2Yzz"
      },
      "outputs": [],
      "source": [
        "y_hat_proba_from_score = np.exp(sum_coef_from_score) / (np.exp(sum_coef_from_score) + 1)\n",
        "# Here we divide an exponent raised to sum of coefficients from score by\n",
        "# an exponent raised to sum of coefficients from score plus one.\n",
        "y_hat_proba_from_score.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sQDTFSZ2aom"
      },
      "outputs": [],
      "source": [
        "y_hat_test_proba[0: 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8Ug0TUf2bsQ"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs['y_hat_test_proba'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp9Z7HpA0hD2"
      },
      "source": [
        "### Setting Cut-offs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkJuxiga2dps"
      },
      "outputs": [],
      "source": [
        "# We need the confusion matrix again.\n",
        "#np.where(np.squeeze(np.array(loan_data_targets_test)) == np.where(y_hat_test_proba >= tr, 1, 0), 1, 0).sum() / loan_data_targets_test.shape[0]\n",
        "tr = 0.9\n",
        "df_actual_predicted_probs['y_hat_test'] = np.where(df_actual_predicted_probs['y_hat_test_proba'] > tr, 1, 0)\n",
        "#df_actual_predicted_probs['loan_data_targets_test'] == np.where(df_actual_predicted_probs['y_hat_test_proba'] >= tr, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE_BkiGM2nKR"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxEfY5Lc2okJ"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFwsAzvq2quD"
      },
      "outputs": [],
      "source": [
        "(pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[0, 0] + (pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCP-tRt52sO5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwXrtrnG2tfk"
      },
      "outputs": [],
      "source": [
        "roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMNx0O4G2ugg"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2kagR4J2vnI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en6iADJi2w0g"
      },
      "outputs": [],
      "source": [
        "plt.plot(fpr, tpr)\n",
        "plt.plot(fpr, fpr, linestyle = '--', color = 'k')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAEDJALV2zYu"
      },
      "outputs": [],
      "source": [
        "thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYXQ9TNB20_h"
      },
      "outputs": [],
      "source": [
        "thresholds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P6goi7W22fs"
      },
      "outputs": [],
      "source": [
        "df_cutoffs = pd.concat([pd.DataFrame(thresholds), pd.DataFrame(fpr), pd.DataFrame(tpr)], axis = 1)\n",
        "# We concatenate 3 dataframes along the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lrQhAFi23Yl"
      },
      "outputs": [],
      "source": [
        "df_cutoffs.columns = ['thresholds', 'fpr', 'tpr']\n",
        "# We name the columns of the dataframe 'thresholds', 'fpr', and 'tpr'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ-g3t-G24im"
      },
      "outputs": [],
      "source": [
        "df_cutoffs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMRwu58N25ha"
      },
      "outputs": [],
      "source": [
        "df_cutoffs['thresholds'][0] = 1 - 1 / np.power(10, 16)\n",
        "# Let the first threshold (the value of the thresholds column with index 0) be equal to a number, very close to 1\n",
        "# but smaller than 1, say 1 - 1 / 10 ^ 16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb90ONJA27Gx"
      },
      "outputs": [],
      "source": [
        "df_cutoffs['Score'] = ((np.log(df_cutoffs['thresholds'] / (1 - df_cutoffs['thresholds'])) - min_sum_coef) * ((max_score - min_score) / (max_sum_coef - min_sum_coef)) + min_score).round()\n",
        "# The score corresponsing to each threshold equals:\n",
        "# The the difference between the natural logarithm of the ratio of the threshold and 1 minus the threshold and\n",
        "# the minimum sum of coefficients\n",
        "# multiplied by\n",
        "# the sum of the minimum score and the ratio of the difference between the maximum score and minimum score and\n",
        "# the difference between the maximum sum of coefficients and the minimum sum of coefficients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l8Jzjz-2900"
      },
      "outputs": [],
      "source": [
        "df_cutoffs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbxoJRsX2-1E"
      },
      "outputs": [],
      "source": [
        "df_cutoffs['Score'][0] = max_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOz0Mvs13CVW"
      },
      "outputs": [],
      "source": [
        "df_cutoffs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfZkvuC93DSX"
      },
      "outputs": [],
      "source": [
        "df_cutoffs.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjEKJiM63EqU"
      },
      "outputs": [],
      "source": [
        "df_cutoffs['N Approved'] = df_cutoffs['thresholds'].apply(n_approved)\n",
        "# Assuming that all credit applications above a given probability of being 'good' will be approved,\n",
        "# when we apply the 'n_approved' function to a threshold, it will return the number of approved applications.\n",
        "# Thus, here we calculate the number of approved appliations for al thresholds.\n",
        "df_cutoffs['N Rejected'] = df_actual_predicted_probs['y_hat_test_proba'].shape[0] - df_cutoffs['N Approved']\n",
        "# Then, we calculate the number of rejected applications for each threshold.\n",
        "# It is the difference between the total number of applications and the approved applications for that threshold.\n",
        "df_cutoffs['Approval Rate'] = df_cutoffs['N Approved'] / df_actual_predicted_probs['y_hat_test_proba'].shape[0]\n",
        "# Approval rate equalts the ratio of the approved applications and all applications.\n",
        "df_cutoffs['Rejection Rate'] = 1 - df_cutoffs['Approval Rate']\n",
        "# Rejection rate equals one minus approval rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r805EIri3Gf_"
      },
      "outputs": [],
      "source": [
        "df_cutoffs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8XHsD2d3H07"
      },
      "outputs": [],
      "source": [
        "df_cutoffs.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edqxzlyi3JVK"
      },
      "outputs": [],
      "source": [
        "df_cutoffs.iloc[5000: 6200, ]\n",
        "# Here we display the dataframe with cutoffs form line with index 5000 to line with index 6200."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX34C4Re3Kfo"
      },
      "outputs": [],
      "source": [
        "df_cutoffs.iloc[1000: 2000, ]\n",
        "# Here we display the dataframe with cutoffs form line with index 1000 to line with index 2000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fgeyAQo3L3r"
      },
      "outputs": [],
      "source": [
        "inputs_train_with_ref_cat.to_csv('inputs_train_with_ref_cat.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TaRr7lN3M2d"
      },
      "outputs": [],
      "source": [
        "df_scorecard.to_csv('df_scorecard.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJS41ERf301W"
      },
      "outputs": [],
      "source": [
        "# Import Train and Test Data.\n",
        "loan_data_inputs_train = pd.read_csv('loan_data_inputs_train.csv', index_col = 0)\n",
        "loan_data_targets_train = pd.read_csv('loan_data_targets_train.csv', index_col = 0, header = None)\n",
        "loan_data_inputs_test = pd.read_csv('loan_data_inputs_test.csv', index_col = 0)\n",
        "loan_data_targets_test = pd.read_csv('loan_data_targets_test.csv', index_col = 0, header = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY_TaRqe32Me"
      },
      "outputs": [],
      "source": [
        "# Here we import the new data.\n",
        "loan_data_backup = pd.read_csv('loan_data_2015.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twzM-B_e34bt"
      },
      "outputs": [],
      "source": [
        "### Explore Data\n",
        "loan_data = loan_data_backup.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pvzkEmP38L_"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_columns = None\n",
        "#pd.options.display.max_rows = None\n",
        "# Sets the pandas dataframe options to display all columns/ rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Vwt3EhS38XE"
      },
      "outputs": [],
      "source": [
        "loan_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7GEaRJs3-sv"
      },
      "outputs": [],
      "source": [
        "loan_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjKxbXKG30ln"
      },
      "source": [
        ">>> The code from here to the other line starting with '>>>' is copied from the Data Preparation notebook, with minor adjustments. We have to perform the exact same data preprocessing, fine-classing, and coarse classing on the new data, in order to be able to calculate statistics for the exact same variables to the ones we used for training and testing the PD model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnblcORO4RWS"
      },
      "source": [
        "### Preprocessing few continuous variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKwqk5lp4UFf"
      },
      "source": [
        "## General Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjYjOdA44TZe"
      },
      "outputs": [],
      "source": [
        "loan_data['emp_length'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xF_FiK_u4RB9"
      },
      "outputs": [],
      "source": [
        "loan_data['emp_length_int'] = loan_data['emp_length'].str.replace('\\+ years', '')\n",
        "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('< 1 year', str(0))\n",
        "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('n/a',  str(0))\n",
        "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace(' years', '')\n",
        "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace(' year', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RJzq7SR4N90"
      },
      "outputs": [],
      "source": [
        "type(loan_data['emp_length_int'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E-MCZR74ZVr"
      },
      "outputs": [],
      "source": [
        "loan_data['emp_length_int'] = pd.to_numeric(loan_data['emp_length_int'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h256r-T24ano"
      },
      "outputs": [],
      "source": [
        "type(loan_data['emp_length_int'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIUOY4Jj4sa3"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Earliest credit line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNI_DMvh4r0_"
      },
      "outputs": [],
      "source": [
        "loan_data['earliest_cr_line']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O38ST6rf4x89"
      },
      "outputs": [],
      "source": [
        "loan_data['earliest_cr_line_date'] = pd.to_datetime(loan_data['earliest_cr_line'], format = '%b-%y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgJQjiAT4y_B"
      },
      "outputs": [],
      "source": [
        "type(loan_data['earliest_cr_line_date'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISiWldgk40YA"
      },
      "outputs": [],
      "source": [
        "pd.to_datetime('2018-12-01') - loan_data['earliest_cr_line_date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtHKkthn43o-"
      },
      "outputs": [],
      "source": [
        "# Assume we are now in December 2017\n",
        "loan_data['mths_since_earliest_cr_line'] = round(pd.to_numeric((pd.to_datetime('2018-12-01') - loan_data['earliest_cr_line_date']) / np.timedelta64(1, 'M')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkLeQ-os45_d"
      },
      "outputs": [],
      "source": [
        "loan_data['mths_since_earliest_cr_line'].describe()\n",
        "# Dates from 1969 and before are not being converted well, i.e., they have become 2069 and similar, and negative differences are being calculated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tm3xKdzW47hE"
      },
      "outputs": [],
      "source": [
        "# There are 2303 such values.\n",
        "loan_data.loc[: , ['earliest_cr_line', 'earliest_cr_line_date', 'mths_since_earliest_cr_line']][loan_data['mths_since_earliest_cr_line'] < 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wqeqIMK49RV"
      },
      "outputs": [],
      "source": [
        "# We set all these negative differences to the maximum.\n",
        "loan_data['mths_since_earliest_cr_line'][loan_data['mths_since_earliest_cr_line'] < 0] = loan_data['mths_since_earliest_cr_line'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3jxsVlV4_LP"
      },
      "outputs": [],
      "source": [
        "min(loan_data['mths_since_earliest_cr_line'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-egceGA5B--"
      },
      "source": [
        "Term"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DeNeiBg5FMF"
      },
      "outputs": [],
      "source": [
        "loan_data['term']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w49WIKH5GY6"
      },
      "outputs": [],
      "source": [
        "loan_data['term'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM1O0-hv5Hsh"
      },
      "outputs": [],
      "source": [
        "loan_data['term_int'] = loan_data['term'].str.replace(' months', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7Irc8eu5IwO"
      },
      "outputs": [],
      "source": [
        "loan_data['term_int']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ScAzNUS5KGi"
      },
      "outputs": [],
      "source": [
        "type(loan_data['term_int'])\n",
        "type(loan_data['term_int'][25])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTh9WpoB5LIY"
      },
      "outputs": [],
      "source": [
        "loan_data['term_int'] = pd.to_numeric(loan_data['term'].str.replace(' months', ''))\n",
        "loan_data['term_int']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4PcNAEX5MAk"
      },
      "outputs": [],
      "source": [
        "type(loan_data['term_int'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfYmuDe35RVd"
      },
      "source": [
        "Time since the loan was funded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwbBtN9J5Ozu"
      },
      "outputs": [],
      "source": [
        "loan_data['issue_d']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpINqNVC5ZYp"
      },
      "outputs": [],
      "source": [
        "# Assume we are now in December 2017\n",
        "loan_data['issue_d_date'] = pd.to_datetime(loan_data['issue_d'], format = '%b-%y')\n",
        "loan_data['mths_since_issue_d'] = round(pd.to_numeric((pd.to_datetime('2018-12-01') - loan_data['issue_d_date']) / np.timedelta64(1, 'M')))\n",
        "loan_data['mths_since_issue_d'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKnpvsKN5cy7"
      },
      "source": [
        "Data preparation: preprocessing discrete variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVEVJ6_5_X5N"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXC-17fe_dR3"
      },
      "outputs": [],
      "source": [
        "loan_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEZ19giC_fGu"
      },
      "outputs": [],
      "source": [
        "# loan_data['grade_factor'] = loan_data['grade'].astype('category')\n",
        "#grade\n",
        "#sub_grade\n",
        "#home_ownership\n",
        "#verification_status\n",
        "#loan_status\n",
        "#purpose\n",
        "#addr_state\n",
        "#initial_list_status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK_rVibz_iDR"
      },
      "outputs": [],
      "source": [
        "pd.get_dummies(loan_data['grade'], prefix = 'grade', prefix_sep = ':')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FvNQ5zd_jUy"
      },
      "outputs": [],
      "source": [
        "loan_data_dummies = [pd.get_dummies(loan_data['grade'], prefix = 'grade', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['sub_grade'], prefix = 'sub_grade', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['home_ownership'], prefix = 'home_ownership', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['verification_status'], prefix = 'verification_status', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['loan_status'], prefix = 'loan_status', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['purpose'], prefix = 'purpose', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['addr_state'], prefix = 'addr_state', prefix_sep = ':'),\n",
        "                     pd.get_dummies(loan_data['initial_list_status'], prefix = 'initial_list_status', prefix_sep = ':')]\n",
        "loan_data_dummies = pd.concat(loan_data_dummies, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft7fnVc5_kpX"
      },
      "outputs": [],
      "source": [
        "type(loan_data_dummies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RRb5IFN_mDV"
      },
      "outputs": [],
      "source": [
        "loan_data_dummies.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kr3plH_r_nOx"
      },
      "outputs": [],
      "source": [
        "loan_data_dummies.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am6UeHtT_ozl"
      },
      "outputs": [],
      "source": [
        "loan_data = pd.concat([loan_data, loan_data_dummies], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n8vgeTS_qYn"
      },
      "outputs": [],
      "source": [
        "loan_data.columns.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPNCxqjY_1TP"
      },
      "source": [
        "*italicised text*# Data preparation: check for missing values and clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWIkQxSW_yje"
      },
      "outputs": [],
      "source": [
        "loan_data.isnull()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLMmicMW_4XH"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_rows = None\n",
        "loan_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKyQ7V0z_5uV"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_rows = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBZ-b9oS_61X"
      },
      "source": [
        "# loan_data$total_rev_hi_lim - There are 70276 missing values here.\n",
        "# 'Total revolving high credit/credit limit', so it makes sense that the missing values are equal to funded_amnt.\n",
        "\n",
        "# loan_data$acc_now_delinq\n",
        "# loan_data$total_acc\n",
        "# loan_data$pub_rec\n",
        "# loan_data$open_acc\n",
        "# loan_data$inq_last_6mths\n",
        "# loan_data$delinq_2yrs\n",
        "# loan_data$mths_since_earliest_cr_line\n",
        "# - There are 29 missing values in all of these columns. They are likely the same observations.\n",
        "# An eyeballing examination of the dataset confirms that.\n",
        "# All of these are with loan_status 'Does not meet the credit policy. Status:Fully Paid'.\n",
        "# We impute these values.\n",
        "\n",
        "# loan_data$annual_inc\n",
        "# - There are 4 missing values in all of these columns.\n",
        "\n",
        "# loan_data$mths_since_last_record\n",
        "# loan_data$mths_since_last_delinq\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNmkYgVl_8Nu"
      },
      "outputs": [],
      "source": [
        "# 'Total revolving high credit/credit limit', so it makes sense that the missing values are equal to funded_amnt.\n",
        "loan_data['total_rev_hi_lim'].fillna(loan_data['funded_amnt'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V--M1LMp_-hY"
      },
      "outputs": [],
      "source": [
        "loan_data['mths_since_earliest_cr_line'].fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tjz_uWne__0k"
      },
      "outputs": [],
      "source": [
        "loan_data['acc_now_delinq'].fillna(0, inplace=True)\n",
        "loan_data['total_acc'].fillna(0, inplace=True)\n",
        "loan_data['pub_rec'].fillna(0, inplace=True)\n",
        "loan_data['open_acc'].fillna(0, inplace=True)\n",
        "loan_data['inq_last_6mths'].fillna(0, inplace=True)\n",
        "loan_data['delinq_2yrs'].fillna(0, inplace=True)\n",
        "loan_data['emp_length_int'].fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9RPghO4ABLp"
      },
      "outputs": [],
      "source": [
        "loan_data['annual_inc'].fillna(loan_data['annual_inc'].mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIY7WkzyADry"
      },
      "source": [
        "# PD model: Data preparation: Good/ Bad (DV for the PD model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PszYGGD-AFQn"
      },
      "outputs": [],
      "source": [
        "loan_data['loan_status'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gEZB4INAGaT"
      },
      "outputs": [],
      "source": [
        "loan_data['loan_status'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqDnYKzGAHlH"
      },
      "outputs": [],
      "source": [
        "loan_data['loan_status'].value_counts() / loan_data['loan_status'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8zYKMFQAIlQ"
      },
      "outputs": [],
      "source": [
        "# Good/ Bad Definition\n",
        "loan_data['good_bad'] = np.where(loan_data['loan_status'].isin(['Charged Off', 'Default',\n",
        "                                                       'Does not meet the credit policy. Status:Charged Off',\n",
        "                                                       'Late (31-120 days)']), 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLE-hYSGAKQE"
      },
      "outputs": [],
      "source": [
        "#loan_data['good_bad'].sum()/loan_data['loan_status'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_0QM4wkALln"
      },
      "outputs": [],
      "source": [
        "loan_data['good_bad']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zut9RkQLAaK-"
      },
      "source": [
        "# PD model: Data Preparation: Splitting Data\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wru57uyFAa1D"
      },
      "outputs": [],
      "source": [
        "# loan_data_inputs_train, loan_data_inputs_test, loan_data_targets_train, loan_data_targets_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKI_mOW1Ac3a"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkn4ykvaAeKQ"
      },
      "outputs": [],
      "source": [
        "# Here we don't split data into training and test\n",
        "#train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'])\n",
        "#loan_data_inputs_train, loan_data_inputs_test, loan_data_targets_train, loan_data_targets_test = train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'])\n",
        "#loan_data_inputs_train.shape\n",
        "#loan_data_targets_train.shape\n",
        "#loan_data_inputs_test.shape\n",
        "#loan_data_targets_test.shape\n",
        "#loan_data_inputs_train, loan_data_inputs_test, loan_data_targets_train, loan_data_targets_test = train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'], test_size = 0.2, random_state = 42)\n",
        "#loan_data_inputs_train.shape\n",
        "#loan_data_targets_train.shape\n",
        "#loan_data_inputs_test.shape\n",
        "#loan_data_targets_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSaBPmHSBLPs"
      },
      "source": [
        "# PD model: Data Preparation: Discrete Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHwb7UKxBNaQ"
      },
      "outputs": [],
      "source": [
        "loan_data.drop('good_bad', axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg_09RujBOlL"
      },
      "outputs": [],
      "source": [
        "loan_data['good_bad']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnRdAHOOBPht"
      },
      "outputs": [],
      "source": [
        "#####\n",
        "df_inputs_prepr = loan_data.drop('good_bad', axis = 1)\n",
        "df_targets_prepr = loan_data['good_bad']\n",
        "#####\n",
        "#df_inputs_prepr = loan_data_inputs_test\n",
        "##df_targets_prepr = loan_data_targets_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sanZDArEBQrJ"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr['grade'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_3JxwCABSFq"
      },
      "outputs": [],
      "source": [
        "df1 = pd.concat([df_inputs_prepr['grade'], df_targets_prepr], axis = 1)\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vwap28BJBTan"
      },
      "outputs": [],
      "source": [
        "df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bhf6RURzBVSj"
      },
      "outputs": [],
      "source": [
        "df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02UwkRLMBWaJ"
      },
      "outputs": [],
      "source": [
        "df1 = pd.concat([df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].count(),\n",
        "                df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].mean()], axis = 1)\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KQKKMViBcih"
      },
      "outputs": [],
      "source": [
        "df1 = df1.iloc[:, [0, 1, 3]]\n",
        "df1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_e9g2l3BfoP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBbpe7ENBffV"
      },
      "outputs": [],
      "source": [
        "df1.columns = [df1.columns.values[0], 'n_obs', 'prop_good']\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipM2XUBDBgg4"
      },
      "outputs": [],
      "source": [
        "df1['prop_n_obs'] = df1['n_obs'] / df1['n_obs'].sum()\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HJrNCo3Bhyr"
      },
      "outputs": [],
      "source": [
        "df1['n_good'] = df1['prop_good'] * df1['n_obs']\n",
        "df1['n_bad'] = (1 - df1['prop_good']) * df1['n_obs']\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-sUM6ZhBjjB"
      },
      "outputs": [],
      "source": [
        "df1['prop_n_good'] = df1['n_good'] / df1['n_good'].sum()\n",
        "df1['prop_n_bad'] = df1['n_bad'] / df1['n_bad'].sum()\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2K2KyIoBklC"
      },
      "outputs": [],
      "source": [
        "df1['WoE'] = np.log(df1['prop_n_good'] / df1['prop_n_bad'])\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWl_7w9DBlru"
      },
      "outputs": [],
      "source": [
        "df1 = df1.sort_values(['WoE'])\n",
        "df1 = df1.reset_index(drop = True)\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuq5LeVqBmv0"
      },
      "outputs": [],
      "source": [
        "df1['diff_prop_good'] = df1['prop_good'].diff().abs()\n",
        "df1['diff_WoE'] = df1['WoE'].diff().abs()\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__LqT6bdBn_u"
      },
      "outputs": [],
      "source": [
        "df1['IV'] = (df1['prop_n_good'] - df1['prop_n_bad']) * df1['WoE']\n",
        "df1['IV'] = df1['IV'].sum()\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HzSf1lfBpWM"
      },
      "outputs": [],
      "source": [
        "# WoE function for discrete unordered variables\n",
        "def woe_discrete(df, discrete_variabe_name, good_bad_variable_df):\n",
        "    df = pd.concat([df[discrete_variabe_name], good_bad_variable_df], axis = 1)\n",
        "    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n",
        "                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n",
        "    df = df.iloc[:, [0, 1, 3]]\n",
        "    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']\n",
        "    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()\n",
        "    df['n_good'] = df['prop_good'] * df['n_obs']\n",
        "    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']\n",
        "    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()\n",
        "    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()\n",
        "    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])\n",
        "    df = df.sort_values(['WoE'])\n",
        "    df = df.reset_index(drop = True)\n",
        "    df['diff_prop_good'] = df['prop_good'].diff().abs()\n",
        "    df['diff_WoE'] = df['WoE'].diff().abs()\n",
        "    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']\n",
        "    df['IV'] = df['IV'].sum()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3bFiHzWBqBL"
      },
      "outputs": [],
      "source": [
        "# 'grade', 'home_ownership', 'verification_status',\n",
        "# 'purpose', 'addr_state', 'initial_list_status'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWb2hEYlByST"
      },
      "outputs": [],
      "source": [
        "# 'grade'\n",
        "df_temp = woe_discrete(df_inputs_prepr, 'grade', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAUQiBI8BzQF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0lpIooyB0eR"
      },
      "outputs": [],
      "source": [
        "def plot_by_woe(df_WoE, rotation_of_x_axis_labels = 0):\n",
        "    #x = df_WoE.iloc[:, 0]\n",
        "    x = np.array(df_WoE.iloc[:, 0].apply(str))\n",
        "    y = df_WoE['WoE']\n",
        "    plt.figure(figsize=(18, 6))\n",
        "    plt.plot(x, y, marker = 'o', linestyle = '--', color = 'k')\n",
        "    plt.xlabel(df_WoE.columns[0])\n",
        "    plt.ylabel('Weight of Evidence')\n",
        "    plt.title(str('Weight of Evidence by ' + df_WoE.columns[0]))\n",
        "    plt.xticks(rotation = rotation_of_x_axis_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEhJtqh9B19Z"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzsvbYTTB28A"
      },
      "outputs": [],
      "source": [
        "# Leave as is.\n",
        "# 'G' will be the reference category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ16hzVyB3-q"
      },
      "outputs": [],
      "source": [
        "# 'home_ownership'\n",
        "df_temp = woe_discrete(df_inputs_prepr, 'home_ownership', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkkTTnmUB5bG"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtuigAp8B6qR"
      },
      "outputs": [],
      "source": [
        "# There are many categories with very few observations and many categories with very different \"good\" %.\n",
        "# Therefore, we create a new discrete variable where we combine some of the categories.\n",
        "# 'OTHERS' and 'NONE' are riskiest but are very few. 'RENT' is the next riskiest.\n",
        "# 'ANY' are least risky but are too few. Conceptually, they belong to the same category. Also, their inclusion would not change anything.\n",
        "# We combine them in one category, 'RENT_OTHER_NONE_ANY'.\n",
        "# We end up with 3 categories: 'RENT_OTHER_NONE_ANY', 'OWN', 'MORTGAGE'.\n",
        "df_inputs_prepr['home_ownership:RENT_OTHER_NONE_ANY'] = sum([df_inputs_prepr['home_ownership:RENT'], df_inputs_prepr['home_ownership:OTHER'],\n",
        "                                                      df_inputs_prepr['home_ownership:NONE'],df_inputs_prepr['home_ownership:ANY']])\n",
        "# 'RENT_OTHER_NONE_ANY' will be the reference category.\n",
        "\n",
        "# Alternatively:\n",
        "#loan_data.loc['home_ownership' in ['RENT', 'OTHER', 'NONE', 'ANY'], 'home_ownership:RENT_OTHER_NONE_ANY'] = 1\n",
        "#loan_data.loc['home_ownership' not in ['RENT', 'OTHER', 'NONE', 'ANY'], 'home_ownership:RENT_OTHER_NONE_ANY'] = 0\n",
        "#loan_data.loc['loan_status' not in ['OWN'], 'home_ownership:OWN'] = 1\n",
        "#loan_data.loc['loan_status' not in ['OWN'], 'home_ownership:OWN'] = 0\n",
        "#loan_data.loc['loan_status' not in ['MORTGAGE'], 'home_ownership:MORTGAGE'] = 1\n",
        "#loan_data.loc['loan_status' not in ['MORTGAGE'], 'home_ownership:MORTGAGE'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SP8wq7EYB7cR"
      },
      "outputs": [],
      "source": [
        "loan_data['home_ownership'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmJCFOmYB-YW"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr['home_ownership:RENT_OTHER_NONE_ANY'] = sum([df_inputs_prepr['home_ownership:RENT'], df_inputs_prepr['home_ownership:ANY']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CouFp9ssB_nf"
      },
      "outputs": [],
      "source": [
        "# 'addr_state'\n",
        "df_inputs_prepr['addr_state'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0Q3eQHSCZtV"
      },
      "outputs": [],
      "source": [
        "#df_inputs_prepr['addr_state:ND'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnbnLJo4ChM0"
      },
      "outputs": [],
      "source": [
        "if ['addr_state:ND'] in df_inputs_prepr.columns.values:\n",
        "    pass\n",
        "else:\n",
        "    df_inputs_prepr['addr_state:ND'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6cfionjCiRm"
      },
      "outputs": [],
      "source": [
        "if ['addr_state:ID'] in df_inputs_prepr.columns.values:\n",
        "    pass\n",
        "else:\n",
        "    df_inputs_prepr['addr_state:ID'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWHFKi9FClO3"
      },
      "outputs": [],
      "source": [
        "if ['addr_state:IA'] in df_inputs_prepr.columns.values:\n",
        "    pass\n",
        "else:\n",
        "    df_inputs_prepr['addr_state:IA'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HdxHgO4Cmby"
      },
      "outputs": [],
      "source": [
        "df_temp = woe_discrete(df_inputs_prepr, 'addr_state', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "san2WDiyCnY3"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp3rN1OOCofP"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp.iloc[2: -2, : ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi4Y7Q_ZCp50"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp.iloc[6: -6, : ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRds9xUTCrat"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8QjpQKYCszX"
      },
      "outputs": [],
      "source": [
        "# We create the following categories:\n",
        "# 'ND' 'NE' 'IA' NV' 'FL' 'HI' 'AL'\n",
        "# 'NM' 'VA'\n",
        "# 'NY'\n",
        "# 'OK' 'TN' 'MO' 'LA' 'MD' 'NC'\n",
        "# 'CA'\n",
        "# 'UT' 'KY' 'AZ' 'NJ'\n",
        "# 'AR' 'MI' 'PA' 'OH' 'MN'\n",
        "# 'RI' 'MA' 'DE' 'SD' 'IN'\n",
        "# 'GA' 'WA' 'OR'\n",
        "# 'WI' 'MT'\n",
        "# 'TX'\n",
        "# 'IL' 'CT'\n",
        "# 'KS' 'SC' 'CO' 'VT' 'AK' 'MS'\n",
        "# 'WV' 'NH' 'WY' 'DC' 'ME' 'ID'\n",
        "\n",
        "# 'IA_NV_HI_ID_AL_FL' will be the reference category.\n",
        "\n",
        "df_inputs_prepr['addr_state:ND_NE_IA_NV_FL_HI_AL'] = sum([df_inputs_prepr['addr_state:ND'], df_inputs_prepr['addr_state:NE'],\n",
        "                                              df_inputs_prepr['addr_state:IA'], df_inputs_prepr['addr_state:NV'],\n",
        "                                              df_inputs_prepr['addr_state:FL'], df_inputs_prepr['addr_state:HI'],\n",
        "                                                          df_inputs_prepr['addr_state:AL']])\n",
        "\n",
        "df_inputs_prepr['addr_state:NM_VA'] = sum([df_inputs_prepr['addr_state:NM'], df_inputs_prepr['addr_state:VA']])\n",
        "\n",
        "df_inputs_prepr['addr_state:OK_TN_MO_LA_MD_NC'] = sum([df_inputs_prepr['addr_state:OK'], df_inputs_prepr['addr_state:TN'],\n",
        "                                              df_inputs_prepr['addr_state:MO'], df_inputs_prepr['addr_state:LA'],\n",
        "                                              df_inputs_prepr['addr_state:MD'], df_inputs_prepr['addr_state:NC']])\n",
        "\n",
        "df_inputs_prepr['addr_state:UT_KY_AZ_NJ'] = sum([df_inputs_prepr['addr_state:UT'], df_inputs_prepr['addr_state:KY'],\n",
        "                                              df_inputs_prepr['addr_state:AZ'], df_inputs_prepr['addr_state:NJ']])\n",
        "\n",
        "df_inputs_prepr['addr_state:AR_MI_PA_OH_MN'] = sum([df_inputs_prepr['addr_state:AR'], df_inputs_prepr['addr_state:MI'],\n",
        "                                              df_inputs_prepr['addr_state:PA'], df_inputs_prepr['addr_state:OH'],\n",
        "                                              df_inputs_prepr['addr_state:MN']])\n",
        "\n",
        "df_inputs_prepr['addr_state:RI_MA_DE_SD_IN'] = sum([df_inputs_prepr['addr_state:RI'], df_inputs_prepr['addr_state:MA'],\n",
        "                                              df_inputs_prepr['addr_state:DE'], df_inputs_prepr['addr_state:SD'],\n",
        "                                              df_inputs_prepr['addr_state:IN']])\n",
        "\n",
        "df_inputs_prepr['addr_state:GA_WA_OR'] = sum([df_inputs_prepr['addr_state:GA'], df_inputs_prepr['addr_state:WA'],\n",
        "                                              df_inputs_prepr['addr_state:OR']])\n",
        "\n",
        "df_inputs_prepr['addr_state:WI_MT'] = sum([df_inputs_prepr['addr_state:WI'], df_inputs_prepr['addr_state:MT']])\n",
        "\n",
        "df_inputs_prepr['addr_state:IL_CT'] = sum([df_inputs_prepr['addr_state:IL'], df_inputs_prepr['addr_state:CT']])\n",
        "\n",
        "df_inputs_prepr['addr_state:KS_SC_CO_VT_AK_MS'] = sum([df_inputs_prepr['addr_state:KS'], df_inputs_prepr['addr_state:SC'],\n",
        "                                              df_inputs_prepr['addr_state:CO'], df_inputs_prepr['addr_state:VT'],\n",
        "                                              df_inputs_prepr['addr_state:AK'], df_inputs_prepr['addr_state:MS']])\n",
        "\n",
        "df_inputs_prepr['addr_state:WV_NH_WY_DC_ME_ID'] = sum([df_inputs_prepr['addr_state:WV'], df_inputs_prepr['addr_state:NH'],\n",
        "                                              df_inputs_prepr['addr_state:WY'], df_inputs_prepr['addr_state:DC'],\n",
        "                                              df_inputs_prepr['addr_state:ME'], df_inputs_prepr['addr_state:ID']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XSUWeBkCufc"
      },
      "outputs": [],
      "source": [
        "# 'verification_status'\n",
        "df_temp = woe_discrete(df_inputs_prepr, 'verification_status', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iDpKKliCvez"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aeDPdyYCwtB"
      },
      "outputs": [],
      "source": [
        "# Leave as is.\n",
        "# 'Verified' will be the reference category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_5PeV_rCxjG"
      },
      "outputs": [],
      "source": [
        "# 'purpose'\n",
        "df_temp = woe_discrete(df_inputs_prepr, 'purpose', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjM27DIICzL-"
      },
      "outputs": [],
      "source": [
        "#plt.figure(figsize=(15, 5))\n",
        "#sns.pointplot(x = 'purpose', y = 'WoE', data = df_temp, figsize = (5, 15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xB2RzY5MC0h_"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NHXXAqVC1dM"
      },
      "outputs": [],
      "source": [
        "# We combine 'educational', 'small_business', 'wedding', 'renewable_energy', 'moving', 'house' in one category: 'educ__sm_b__wedd__ren_en__mov__house'.\n",
        "# We combine 'other', 'medical', 'vacation' in one category: 'oth__med__vacation'.\n",
        "# We combine 'major_purchase', 'car', 'home_improvement' in one category: 'major_purch__car__home_impr'.\n",
        "# We leave 'debt_consolidtion' in a separate category.\n",
        "# We leave 'credit_card' in a separate category.\n",
        "# 'educ__sm_b__wedd__ren_en__mov__house' will be the reference category.\n",
        "df_inputs_prepr['purpose:educ__sm_b__wedd__ren_en__mov__house'] = sum([df_inputs_prepr['purpose:educational'], df_inputs_prepr['purpose:small_business'],\n",
        "                                                                 df_inputs_prepr['purpose:wedding'], df_inputs_prepr['purpose:renewable_energy'],\n",
        "                                                                 df_inputs_prepr['purpose:moving'], df_inputs_prepr['purpose:house']])\n",
        "df_inputs_prepr['purpose:oth__med__vacation'] = sum([df_inputs_prepr['purpose:other'], df_inputs_prepr['purpose:medical'],\n",
        "                                             df_inputs_prepr['purpose:vacation']])\n",
        "df_inputs_prepr['purpose:major_purch__car__home_impr'] = sum([df_inputs_prepr['purpose:major_purchase'], df_inputs_prepr['purpose:car'],\n",
        "                                                        df_inputs_prepr['purpose:home_improvement']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1urZy3diC3DD"
      },
      "outputs": [],
      "source": [
        "# 'initial_list_status'\n",
        "df_temp = woe_discrete(df_inputs_prepr, 'initial_list_status', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M9mU_HsC4EV"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7TI5BNlC4_J"
      },
      "outputs": [],
      "source": [
        "# Leave as is.\n",
        "# 'f' will be the reference category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSJfLGsjC6pB"
      },
      "source": [
        "# PD model: Data Preparation: Continuous Variables, Part 1\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyqXPvUkC-Fj"
      },
      "outputs": [],
      "source": [
        "# WoE function for ordered discrete and continuous variables\n",
        "def woe_ordered_continuous(df, discrete_variabe_name, good_bad_variable_df):\n",
        "    df = pd.concat([df[discrete_variabe_name], good_bad_variable_df], axis = 1)\n",
        "    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n",
        "                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n",
        "    df = df.iloc[:, [0, 1, 3]]\n",
        "    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']\n",
        "    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()\n",
        "    df['n_good'] = df['prop_good'] * df['n_obs']\n",
        "    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']\n",
        "    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()\n",
        "    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()\n",
        "    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])\n",
        "    #df = df.sort_values(['WoE'])\n",
        "    #df = df.reset_index(drop = True)\n",
        "    df['diff_prop_good'] = df['prop_good'].diff().abs()\n",
        "    df['diff_WoE'] = df['WoE'].diff().abs()\n",
        "    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']\n",
        "    df['IV'] = df['IV'].sum()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxpB8emUDA8s"
      },
      "outputs": [],
      "source": [
        "# term\n",
        "df_inputs_prepr['term_int'].unique()\n",
        "# There are only two unique values, 36 and 60."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U9CdMLjDCGD"
      },
      "outputs": [],
      "source": [
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'term_int', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INlDFVTZDYpS"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLENFL72DaKZ"
      },
      "outputs": [],
      "source": [
        "# Leave as is.\n",
        "# '60' will be the reference category.\n",
        "df_inputs_prepr['term:36'] = np.where((df_inputs_prepr['term_int'] == 36), 1, 0)\n",
        "df_inputs_prepr['term:60'] = np.where((df_inputs_prepr['term_int'] == 60), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65xBATkjDbTs"
      },
      "outputs": [],
      "source": [
        "# emp_length_int\n",
        "df_inputs_prepr['emp_length_int'].unique()\n",
        "# Has only 11 levels: from 0 to 10. Hence, we turn it into a factor with 11 levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRHndcrPDc7G"
      },
      "outputs": [],
      "source": [
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'emp_length_int', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9Eb3woGDeB_"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQE-EqLIDfFf"
      },
      "outputs": [],
      "source": [
        "# We create the following categories: '0', '1', '2 - 4', '5 - 6', '7 - 9', '10'\n",
        "# '0' will be the reference category\n",
        "df_inputs_prepr['emp_length:0'] = np.where(df_inputs_prepr['emp_length_int'].isin([0]), 1, 0)\n",
        "df_inputs_prepr['emp_length:1'] = np.where(df_inputs_prepr['emp_length_int'].isin([1]), 1, 0)\n",
        "df_inputs_prepr['emp_length:2-4'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(2, 5)), 1, 0)\n",
        "df_inputs_prepr['emp_length:5-6'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(5, 7)), 1, 0)\n",
        "df_inputs_prepr['emp_length:7-9'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(7, 10)), 1, 0)\n",
        "df_inputs_prepr['emp_length:10'] = np.where(df_inputs_prepr['emp_length_int'].isin([10]), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sm911_cDgsk"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr['mths_since_issue_d'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Cmgt0XvDh3P"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr['mths_since_issue_d_factor'] = pd.cut(df_inputs_prepr['mths_since_issue_d'], 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yaci8cshDjW8"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr['mths_since_issue_d_factor']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbXnZhmuDkQ9"
      },
      "outputs": [],
      "source": [
        "# mths_since_issue_d\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'mths_since_issue_d_factor', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK9KU0rpDloy"
      },
      "outputs": [],
      "source": [
        "# !!!!!!!!!\n",
        "#df_temp['mths_since_issue_d_factor'] = np.array(df_temp.mths_since_issue_d_factor.apply(str))\n",
        "#df_temp['mths_since_issue_d_factor'] = list(df_temp.mths_since_issue_d_factor.apply(str))\n",
        "#df_temp['mths_since_issue_d_factor'] = tuple(df_temp.mths_since_issue_d_factor.apply(str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuWUWI2zDm3F"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlJhdhaqDoJp"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_0QyTmQDpLB"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp.iloc[3: , : ], 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihbp7iE5DqA-"
      },
      "outputs": [],
      "source": [
        "# We create the following categories:\n",
        "# < 38, 38 - 39, 40 - 41, 42 - 48, 49 - 52, 53 - 64, 65 - 84, > 84.\n",
        "df_inputs_prepr['mths_since_issue_d:<38'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(38)), 1, 0)\n",
        "df_inputs_prepr['mths_since_issue_d:38-39'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(38, 40)), 1, 0)\n",
        "df_inputs_prepr['mths_since_issue_d:40-41'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(40, 42)), 1, 0)\n",
        "df_inputs_prepr['mths_since_issue_d:42-48'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(42, 49)), 1, 0)\n",
        "df_inputs_prepr['mths_since_issue_d:49-52'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(49, 53)), 1, 0)\n",
        "df_inputs_prepr['mths_since_issue_d:53-64'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(53, 65)), 1, 0)\n",
        "df_inputs_prepr['mths_since_issue_d:65-84'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(65, 85)), 1, 0)\n",
        "df_inputs_prepr['mths_since_issue_d:>84'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(85, int(df_inputs_prepr['mths_since_issue_d'].max()))), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8htoDzTsDsO1"
      },
      "outputs": [],
      "source": [
        "# int_rate\n",
        "df_inputs_prepr['int_rate_factor'] = pd.cut(df_inputs_prepr['int_rate'], 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RorXYHgjDtoU"
      },
      "outputs": [],
      "source": [
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'int_rate_factor', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3CsAdHRDuix"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M6nt5FaDvi3"
      },
      "outputs": [],
      "source": [
        "# '< 9.548', '9.548 - 12.025', '12.025 - 15.74', '15.74 - 20.281', '> 20.281'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBFmdn1nDwWa"
      },
      "outputs": [],
      "source": [
        "#loan_data.loc[loan_data['int_rate'] < 5.8, 'int_rate:<5.8'] = 1\n",
        "#(loan_data['int_rate'] > 5.8) & (loan_data['int_rate'] <= 8.64)\n",
        "#loan_data['int_rate:<5.8'] = np.where(loan_data['int_rate'] < 5.8, 1, 0)\n",
        "#loan_data[(loan_data['int_rate'] > 5.8) & (loan_data['int_rate'] <= 8.64)]\n",
        "#loan_data['int_rate'][(np.where((loan_data['int_rate'] > 5.8) & (loan_data['int_rate'] <= 8.64)))]\n",
        "#loan_data.loc[(loan_data['int_rate'] > 5.8) & (loan_data['int_rate'] <= 8.64), 'int_rate:<5.8'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J2R2SwoEAzh"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr['int_rate:<9.548'] = np.where((df_inputs_prepr['int_rate'] <= 9.548), 1, 0)\n",
        "df_inputs_prepr['int_rate:9.548-12.025'] = np.where((df_inputs_prepr['int_rate'] > 9.548) & (df_inputs_prepr['int_rate'] <= 12.025), 1, 0)\n",
        "df_inputs_prepr['int_rate:12.025-15.74'] = np.where((df_inputs_prepr['int_rate'] > 12.025) & (df_inputs_prepr['int_rate'] <= 15.74), 1, 0)\n",
        "df_inputs_prepr['int_rate:15.74-20.281'] = np.where((df_inputs_prepr['int_rate'] > 15.74) & (df_inputs_prepr['int_rate'] <= 20.281), 1, 0)\n",
        "df_inputs_prepr['int_rate:>20.281'] = np.where((df_inputs_prepr['int_rate'] > 20.281), 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWmosiow3rD6"
      },
      "source": [
        "### PD model: Data Preparation: Continuous Variables, Part 1: Homework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRRdCkrBEFFM"
      },
      "outputs": [],
      "source": [
        "# mths_since_earliest_cr_line\n",
        "df_inputs_prepr['mths_since_earliest_cr_line_factor'] = pd.cut(df_inputs_prepr['mths_since_earliest_cr_line'], 50)\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'mths_since_earliest_cr_line_factor', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6823aRJIEGFe"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzQA8Op5EHNY"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp.iloc[6: , : ], 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTET5HFEEIUv"
      },
      "outputs": [],
      "source": [
        "# We create the following categories:\n",
        "# < 140, # 141 - 164, # 165 - 247, # 248 - 270, # 271 - 352, # > 352\n",
        "df_inputs_prepr['mths_since_earliest_cr_line:<140'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(140)), 1, 0)\n",
        "df_inputs_prepr['mths_since_earliest_cr_line:141-164'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(140, 165)), 1, 0)\n",
        "df_inputs_prepr['mths_since_earliest_cr_line:165-247'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(165, 248)), 1, 0)\n",
        "df_inputs_prepr['mths_since_earliest_cr_line:248-270'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(248, 271)), 1, 0)\n",
        "df_inputs_prepr['mths_since_earliest_cr_line:271-352'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(271, 353)), 1, 0)\n",
        "df_inputs_prepr['mths_since_earliest_cr_line:>352'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(353, int(df_inputs_prepr['mths_since_earliest_cr_line'].max()))), 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KFEFbnUHqiq"
      },
      "source": [
        "# REFERENCE CATEGORY!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOIaZNJyHtRo"
      },
      "outputs": [],
      "source": [
        "# delinq_2yrs\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'delinq_2yrs', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F1ThKecHuvs"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnXvzb9ZHyBw"
      },
      "outputs": [],
      "source": [
        "# Categories: 0, 1-3, >=4\n",
        "df_inputs_prepr['delinq_2yrs:0'] = np.where((df_inputs_prepr['delinq_2yrs'] == 0), 1, 0)\n",
        "df_inputs_prepr['delinq_2yrs:1-3'] = np.where((df_inputs_prepr['delinq_2yrs'] >= 1) & (df_inputs_prepr['delinq_2yrs'] <= 3), 1, 0)\n",
        "df_inputs_prepr['delinq_2yrs:>=4'] = np.where((df_inputs_prepr['delinq_2yrs'] >= 9), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_JERBPKHziH"
      },
      "outputs": [],
      "source": [
        "# inq_last_6mths\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'inq_last_6mths', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSOS0R4LH150"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rdPJfKVH66z"
      },
      "outputs": [],
      "source": [
        "# Categories: 0, 1 - 2, 3 - 6, > 6\n",
        "df_inputs_prepr['inq_last_6mths:0'] = np.where((df_inputs_prepr['inq_last_6mths'] == 0), 1, 0)\n",
        "df_inputs_prepr['inq_last_6mths:1-2'] = np.where((df_inputs_prepr['inq_last_6mths'] >= 1) & (df_inputs_prepr['inq_last_6mths'] <= 2), 1, 0)\n",
        "df_inputs_prepr['inq_last_6mths:3-6'] = np.where((df_inputs_prepr['inq_last_6mths'] >= 3) & (df_inputs_prepr['inq_last_6mths'] <= 6), 1, 0)\n",
        "df_inputs_prepr['inq_last_6mths:>6'] = np.where((df_inputs_prepr['inq_last_6mths'] > 6), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AccQZSAoH-Ei"
      },
      "outputs": [],
      "source": [
        "# open_acc\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'open_acc', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nom76GQZIAKZ"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xZMBNbzIC3j"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp.iloc[ : 40, :], 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h5vmOVBIGx2"
      },
      "outputs": [],
      "source": [
        "# Categories: '0', '1-3', '4-12', '13-17', '18-22', '23-25', '26-30', '>30'\n",
        "df_inputs_prepr['open_acc:0'] = np.where((df_inputs_prepr['open_acc'] == 0), 1, 0)\n",
        "df_inputs_prepr['open_acc:1-3'] = np.where((df_inputs_prepr['open_acc'] >= 1) & (df_inputs_prepr['open_acc'] <= 3), 1, 0)\n",
        "df_inputs_prepr['open_acc:4-12'] = np.where((df_inputs_prepr['open_acc'] >= 4) & (df_inputs_prepr['open_acc'] <= 12), 1, 0)\n",
        "df_inputs_prepr['open_acc:13-17'] = np.where((df_inputs_prepr['open_acc'] >= 13) & (df_inputs_prepr['open_acc'] <= 17), 1, 0)\n",
        "df_inputs_prepr['open_acc:18-22'] = np.where((df_inputs_prepr['open_acc'] >= 18) & (df_inputs_prepr['open_acc'] <= 22), 1, 0)\n",
        "df_inputs_prepr['open_acc:23-25'] = np.where((df_inputs_prepr['open_acc'] >= 23) & (df_inputs_prepr['open_acc'] <= 25), 1, 0)\n",
        "df_inputs_prepr['open_acc:26-30'] = np.where((df_inputs_prepr['open_acc'] >= 26) & (df_inputs_prepr['open_acc'] <= 30), 1, 0)\n",
        "df_inputs_prepr['open_acc:>=31'] = np.where((df_inputs_prepr['open_acc'] >= 31), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1Ky8XYqIIiy"
      },
      "outputs": [],
      "source": [
        "# pub_rec\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'pub_rec', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJHW2RYzIKIN"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pijy-GPOIMDW"
      },
      "outputs": [],
      "source": [
        "# Categories '0-2', '3-4', '>=5'\n",
        "df_inputs_prepr['pub_rec:0-2'] = np.where((df_inputs_prepr['pub_rec'] >= 0) & (df_inputs_prepr['pub_rec'] <= 2), 1, 0)\n",
        "df_inputs_prepr['pub_rec:3-4'] = np.where((df_inputs_prepr['pub_rec'] >= 3) & (df_inputs_prepr['pub_rec'] <= 4), 1, 0)\n",
        "df_inputs_prepr['pub_rec:>=5'] = np.where((df_inputs_prepr['pub_rec'] >= 5), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9sLSrxFINox"
      },
      "outputs": [],
      "source": [
        "# total_acc\n",
        "df_inputs_prepr['total_acc_factor'] = pd.cut(df_inputs_prepr['total_acc'], 50)\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'total_acc_factor', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDQDhUa5IOwx"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxBbpOcKIPt9"
      },
      "outputs": [],
      "source": [
        "# Categories: '<=27', '28-51', '>51'\n",
        "df_inputs_prepr['total_acc:<=27'] = np.where((df_inputs_prepr['total_acc'] <= 27), 1, 0)\n",
        "df_inputs_prepr['total_acc:28-51'] = np.where((df_inputs_prepr['total_acc'] >= 28) & (df_inputs_prepr['total_acc'] <= 51), 1, 0)\n",
        "df_inputs_prepr['total_acc:>=52'] = np.where((df_inputs_prepr['total_acc'] >= 52), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV9VzDyOIRGg"
      },
      "outputs": [],
      "source": [
        "# acc_now_delinq\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'acc_now_delinq', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwAVf9xOISUP"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WopOYts8ITkE"
      },
      "outputs": [],
      "source": [
        "# Categories: '0', '>=1'\n",
        "df_inputs_prepr['acc_now_delinq:0'] = np.where((df_inputs_prepr['acc_now_delinq'] == 0), 1, 0)\n",
        "df_inputs_prepr['acc_now_delinq:>=1'] = np.where((df_inputs_prepr['acc_now_delinq'] >= 1), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDZm6oi2IUj7"
      },
      "outputs": [],
      "source": [
        "# total_rev_hi_lim\n",
        "df_inputs_prepr['total_rev_hi_lim_factor'] = pd.cut(df_inputs_prepr['total_rev_hi_lim'], 2000)\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'total_rev_hi_lim_factor', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4B5S78oIWSU"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp.iloc[: 50, : ], 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07_PiUfAIYVT"
      },
      "outputs": [],
      "source": [
        "# Categories\n",
        "# '<=5K', '5K-10K', '10K-20K', '20K-30K', '30K-40K', '40K-55K', '55K-95K', '>95K'\n",
        "df_inputs_prepr['total_rev_hi_lim:<=5K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] <= 5000), 1, 0)\n",
        "df_inputs_prepr['total_rev_hi_lim:5K-10K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 5000) & (df_inputs_prepr['total_rev_hi_lim'] <= 10000), 1, 0)\n",
        "df_inputs_prepr['total_rev_hi_lim:10K-20K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 10000) & (df_inputs_prepr['total_rev_hi_lim'] <= 20000), 1, 0)\n",
        "df_inputs_prepr['total_rev_hi_lim:20K-30K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 20000) & (df_inputs_prepr['total_rev_hi_lim'] <= 30000), 1, 0)\n",
        "df_inputs_prepr['total_rev_hi_lim:30K-40K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 30000) & (df_inputs_prepr['total_rev_hi_lim'] <= 40000), 1, 0)\n",
        "df_inputs_prepr['total_rev_hi_lim:40K-55K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 40000) & (df_inputs_prepr['total_rev_hi_lim'] <= 55000), 1, 0)\n",
        "df_inputs_prepr['total_rev_hi_lim:55K-95K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 55000) & (df_inputs_prepr['total_rev_hi_lim'] <= 95000), 1, 0)\n",
        "df_inputs_prepr['total_rev_hi_lim:>95K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 95000), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SXHLO2tIa5E"
      },
      "outputs": [],
      "source": [
        "# PD model: Data Preparation: Continuous Variables, Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu7BW41NIdEe"
      },
      "outputs": [],
      "source": [
        "# annual_inc\n",
        "df_inputs_prepr['annual_inc_factor'] = pd.cut(df_inputs_prepr['annual_inc'], 50)\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'annual_inc_factor', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AiyCW2BIeE-"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr['annual_inc_factor'] = pd.cut(df_inputs_prepr['annual_inc'], 100)\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'annual_inc_factor', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mboger8jIfxc"
      },
      "outputs": [],
      "source": [
        "# Initial examination shows that there are too few individuals with large income and too many with small income.\n",
        "# Hence, we are going to have one category for more than 150K, and we are going to apply our approach to determine\n",
        "# the categories of everyone with 140k or less."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdxRxBdAIg6Y"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr_temp = df_inputs_prepr.loc[df_inputs_prepr['annual_inc'] <= 140000, : ]\n",
        "#loan_data_temp = loan_data_temp.reset_index(drop = True)\n",
        "#df_inputs_prepr_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeuVpFTRIiLe"
      },
      "outputs": [],
      "source": [
        "#pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_p9xOCHIjlK"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr_temp[\"annual_inc_factor\"] = pd.cut(df_inputs_prepr_temp['annual_inc'], 50)\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'annual_inc_factor', df_targets_prepr[df_inputs_prepr_temp.index])\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqLIOGQWImOT"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhAnywYqInkM"
      },
      "outputs": [],
      "source": [
        "# WoE is monotonically decreasing with income, so we split income in 10 equal categories, each with width of 15k.\n",
        "df_inputs_prepr['annual_inc:<20K'] = np.where((df_inputs_prepr['annual_inc'] <= 20000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:20K-30K'] = np.where((df_inputs_prepr['annual_inc'] > 20000) & (df_inputs_prepr['annual_inc'] <= 30000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:30K-40K'] = np.where((df_inputs_prepr['annual_inc'] > 30000) & (df_inputs_prepr['annual_inc'] <= 40000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:40K-50K'] = np.where((df_inputs_prepr['annual_inc'] > 40000) & (df_inputs_prepr['annual_inc'] <= 50000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:50K-60K'] = np.where((df_inputs_prepr['annual_inc'] > 50000) & (df_inputs_prepr['annual_inc'] <= 60000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:60K-70K'] = np.where((df_inputs_prepr['annual_inc'] > 60000) & (df_inputs_prepr['annual_inc'] <= 70000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:70K-80K'] = np.where((df_inputs_prepr['annual_inc'] > 70000) & (df_inputs_prepr['annual_inc'] <= 80000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:80K-90K'] = np.where((df_inputs_prepr['annual_inc'] > 80000) & (df_inputs_prepr['annual_inc'] <= 90000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:90K-100K'] = np.where((df_inputs_prepr['annual_inc'] > 90000) & (df_inputs_prepr['annual_inc'] <= 100000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:100K-120K'] = np.where((df_inputs_prepr['annual_inc'] > 100000) & (df_inputs_prepr['annual_inc'] <= 120000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:120K-140K'] = np.where((df_inputs_prepr['annual_inc'] > 120000) & (df_inputs_prepr['annual_inc'] <= 140000), 1, 0)\n",
        "df_inputs_prepr['annual_inc:>140K'] = np.where((df_inputs_prepr['annual_inc'] > 140000), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfnNEY5oIo5O"
      },
      "outputs": [],
      "source": [
        "# dti\n",
        "df_inputs_prepr['dti_factor'] = pd.cut(df_inputs_prepr['dti'], 100)\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'dti_factor', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db-a8IW6IqRP"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QT-q636mIqfO"
      },
      "outputs": [],
      "source": [
        "# Similarly to income, initial examination shows that most values are lower than 200.\n",
        "# Hence, we are going to have one category for more than 35, and we are going to apply our approach to determine\n",
        "# the categories of everyone with 150k or less."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj4T9b9VItts"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr_temp = df_inputs_prepr.loc[df_inputs_prepr['dti'] <= 35, : ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCP6GlPLIvHL"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr_temp['dti_factor'] = pd.cut(df_inputs_prepr_temp['dti'], 50)\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'dti_factor', df_targets_prepr[df_inputs_prepr_temp.index])\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfSxFNCmIwQ2"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKvM7hQdIx-H"
      },
      "outputs": [],
      "source": [
        "# Categories:\n",
        "df_inputs_prepr['dti:<=1.4'] = np.where((df_inputs_prepr['dti'] <= 1.4), 1, 0)\n",
        "df_inputs_prepr['dti:1.4-3.5'] = np.where((df_inputs_prepr['dti'] > 1.4) & (df_inputs_prepr['dti'] <= 3.5), 1, 0)\n",
        "df_inputs_prepr['dti:3.5-7.7'] = np.where((df_inputs_prepr['dti'] > 3.5) & (df_inputs_prepr['dti'] <= 7.7), 1, 0)\n",
        "df_inputs_prepr['dti:7.7-10.5'] = np.where((df_inputs_prepr['dti'] > 7.7) & (df_inputs_prepr['dti'] <= 10.5), 1, 0)\n",
        "df_inputs_prepr['dti:10.5-16.1'] = np.where((df_inputs_prepr['dti'] > 10.5) & (df_inputs_prepr['dti'] <= 16.1), 1, 0)\n",
        "df_inputs_prepr['dti:16.1-20.3'] = np.where((df_inputs_prepr['dti'] > 16.1) & (df_inputs_prepr['dti'] <= 20.3), 1, 0)\n",
        "df_inputs_prepr['dti:20.3-21.7'] = np.where((df_inputs_prepr['dti'] > 20.3) & (df_inputs_prepr['dti'] <= 21.7), 1, 0)\n",
        "df_inputs_prepr['dti:21.7-22.4'] = np.where((df_inputs_prepr['dti'] > 21.7) & (df_inputs_prepr['dti'] <= 22.4), 1, 0)\n",
        "df_inputs_prepr['dti:22.4-35'] = np.where((df_inputs_prepr['dti'] > 22.4) & (df_inputs_prepr['dti'] <= 35), 1, 0)\n",
        "df_inputs_prepr['dti:>35'] = np.where((df_inputs_prepr['dti'] > 35), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGp_2b06IzT7"
      },
      "outputs": [],
      "source": [
        "# mths_since_last_delinq\n",
        "# We have to create one category for missing values and do fine and coarse classing for the rest.\n",
        "#loan_data_temp = loan_data[np.isfinite(loan_data['mths_since_last_delinq'])]\n",
        "df_inputs_prepr_temp = df_inputs_prepr[pd.notnull(df_inputs_prepr['mths_since_last_delinq'])]\n",
        "#sum(loan_data_temp['mths_since_last_delinq'].isnull())\n",
        "df_inputs_prepr_temp['mths_since_last_delinq_factor'] = pd.cut(df_inputs_prepr_temp['mths_since_last_delinq'], 50)\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'mths_since_last_delinq_factor', df_targets_prepr[df_inputs_prepr_temp.index])\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lfkESBiI0zq"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh5kOai1I1xA"
      },
      "outputs": [],
      "source": [
        "# Categories: Missing, 0-3, 4-30, 31-56, >=57\n",
        "df_inputs_prepr['mths_since_last_delinq:Missing'] = np.where((df_inputs_prepr['mths_since_last_delinq'].isnull()), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_delinq:0-3'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 0) & (df_inputs_prepr['mths_since_last_delinq'] <= 3), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_delinq:4-30'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 4) & (df_inputs_prepr['mths_since_last_delinq'] <= 30), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_delinq:31-56'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 31) & (df_inputs_prepr['mths_since_last_delinq'] <= 56), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_delinq:>=57'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 57), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX_Kriw2I23X"
      },
      "outputs": [],
      "source": [
        "# mths_since_last_record\n",
        "# We have to create one category for missing values and do fine and coarse classing for the rest.\n",
        "df_inputs_prepr_temp = df_inputs_prepr[pd.notnull(df_inputs_prepr['mths_since_last_record'])]\n",
        "#sum(loan_data_temp['mths_since_last_record'].isnull())\n",
        "df_inputs_prepr_temp['mths_since_last_record_factor'] = pd.cut(df_inputs_prepr_temp['mths_since_last_record'], 50)\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'mths_since_last_record_factor', df_targets_prepr[df_inputs_prepr_temp.index])\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KvNQXaNI4Ng"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEe_OiLhI5A0"
      },
      "outputs": [],
      "source": [
        "# Categories: 'Missing', '0-2', '3-20', '21-31', '32-80', '81-86', '>86'\n",
        "df_inputs_prepr['mths_since_last_record:Missing'] = np.where((df_inputs_prepr['mths_since_last_record'].isnull()), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_record:0-2'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 0) & (df_inputs_prepr['mths_since_last_record'] <= 2), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_record:3-20'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 3) & (df_inputs_prepr['mths_since_last_record'] <= 20), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_record:21-31'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 21) & (df_inputs_prepr['mths_since_last_record'] <= 31), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_record:32-80'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 32) & (df_inputs_prepr['mths_since_last_record'] <= 80), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_record:81-86'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 81) & (df_inputs_prepr['mths_since_last_record'] <= 86), 1, 0)\n",
        "df_inputs_prepr['mths_since_last_record:>=86'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 86), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6S7pTpSI7Bc"
      },
      "outputs": [],
      "source": [
        "df_inputs_prepr['mths_since_last_delinq:Missing'].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V9HgALdI88H"
      },
      "outputs": [],
      "source": [
        "# display inputs_train, inputs_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIumWOBsI-vt"
      },
      "outputs": [],
      "source": [
        "# funded_amnt\n",
        "df_inputs_prepr['funded_amnt_factor'] = pd.cut(df_inputs_prepr['funded_amnt'], 50)\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'funded_amnt_factor', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZCNe_xuJAo0"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT-zakXWJByl"
      },
      "outputs": [],
      "source": [
        "# WON'T USE because there is no clear trend, even if segments of the whole range are considered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx7GQIhzJDiL"
      },
      "outputs": [],
      "source": [
        "# installment\n",
        "df_inputs_prepr['installment_factor'] = pd.cut(df_inputs_prepr['installment'], 50)\n",
        "df_temp = woe_ordered_continuous(df_inputs_prepr, 'installment_factor', df_targets_prepr)\n",
        "df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r0_QtmWJL4x"
      },
      "outputs": [],
      "source": [
        "plot_by_woe(df_temp, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fALgPEbFJNNu"
      },
      "outputs": [],
      "source": [
        "# WON'T USE because there is no clear trend, even if segments of the whole range are considered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZaNncOOJPu_"
      },
      "source": [
        "### Preprocessing the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmwZnxVNJQhM"
      },
      "outputs": [],
      "source": [
        "#####\n",
        "#loan_data_inputs_train = df_inputs_prepr\n",
        "#####\n",
        "#loan_data_inputs_test = df_inputs_prepr\n",
        "######\n",
        "loan_data_inputs_2015 = df_inputs_prepr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8hrY_59JTIv"
      },
      "outputs": [],
      "source": [
        "loan_data_targets_2015 = df_targets_prepr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9fzrd9pJUtn"
      },
      "outputs": [],
      "source": [
        "#loan_data_inputs_train.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGgKvj7MJV0r"
      },
      "outputs": [],
      "source": [
        "#loan_data_inputs_test.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLMHVAWmJWsx"
      },
      "outputs": [],
      "source": [
        "#loan_data_inputs_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUYBp02IJYK8"
      },
      "outputs": [],
      "source": [
        "#loan_data_targets_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wfUGlchJZdo"
      },
      "outputs": [],
      "source": [
        "#loan_data_inputs_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBlKIRV9JcK0"
      },
      "outputs": [],
      "source": [
        "#loan_data_targets_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsvw9YroJdqj"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_2015.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szCEjBt3Je6l"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_2015.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSXvisjOJgAo"
      },
      "outputs": [],
      "source": [
        "loan_data_targets_2015.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL0BzCB6JhPw"
      },
      "outputs": [],
      "source": [
        "#loan_data_inputs_train.to_csv('loan_data_inputs_train.csv')\n",
        "#loan_data_targets_train.to_csv('loan_data_targets_train.csv')\n",
        "#loan_data_inputs_test.to_csv('loan_data_inputs_test.csv')\n",
        "#loan_data_targets_test.to_csv('loan_data_targets_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkYV54fBJiV3"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_2015.to_csv('loan_data_inputs_2015.csv')\n",
        "loan_data_targets_2015.to_csv('loan_data_targets_2015.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsYhN8AQJkgq"
      },
      "source": [
        ">>> The code up to here, from the other line starting with '>>>' is copied from the Data Preparation notebook, with minor adjustments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8ranf06Jm_d"
      },
      "outputs": [],
      "source": [
        "inputs_train_with_ref_cat = pd.read_csv('inputs_train_with_ref_cat.csv', index_col = 0)\n",
        "# We import the dataset with old data, i.e. \"expected\" data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO_tD6b9JqFt"
      },
      "outputs": [],
      "source": [
        "# From the dataframe with new, \"actual\" data, we keep only the relevant columns.\n",
        "inputs_2015_with_ref_cat = loan_data_inputs_2015.loc[: , ['grade:A',\n",
        "'grade:B',\n",
        "'grade:C',\n",
        "'grade:D',\n",
        "'grade:E',\n",
        "'grade:F',\n",
        "'grade:G',\n",
        "'home_ownership:RENT_OTHER_NONE_ANY',\n",
        "'home_ownership:OWN',\n",
        "'home_ownership:MORTGAGE',\n",
        "'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
        "'addr_state:NM_VA',\n",
        "'addr_state:NY',\n",
        "'addr_state:OK_TN_MO_LA_MD_NC',\n",
        "'addr_state:CA',\n",
        "'addr_state:UT_KY_AZ_NJ',\n",
        "'addr_state:AR_MI_PA_OH_MN',\n",
        "'addr_state:RI_MA_DE_SD_IN',\n",
        "'addr_state:GA_WA_OR',\n",
        "'addr_state:WI_MT',\n",
        "'addr_state:TX',\n",
        "'addr_state:IL_CT',\n",
        "'addr_state:KS_SC_CO_VT_AK_MS',\n",
        "'addr_state:WV_NH_WY_DC_ME_ID',\n",
        "'verification_status:Not Verified',\n",
        "'verification_status:Source Verified',\n",
        "'verification_status:Verified',\n",
        "'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
        "'purpose:credit_card',\n",
        "'purpose:debt_consolidation',\n",
        "'purpose:oth__med__vacation',\n",
        "'purpose:major_purch__car__home_impr',\n",
        "'initial_list_status:f',\n",
        "'initial_list_status:w',\n",
        "'term:36',\n",
        "'term:60',\n",
        "'emp_length:0',\n",
        "'emp_length:1',\n",
        "'emp_length:2-4',\n",
        "'emp_length:5-6',\n",
        "'emp_length:7-9',\n",
        "'emp_length:10',\n",
        "'mths_since_issue_d:<38',\n",
        "'mths_since_issue_d:38-39',\n",
        "'mths_since_issue_d:40-41',\n",
        "'mths_since_issue_d:42-48',\n",
        "'mths_since_issue_d:49-52',\n",
        "'mths_since_issue_d:53-64',\n",
        "'mths_since_issue_d:65-84',\n",
        "'mths_since_issue_d:>84',\n",
        "'int_rate:<9.548',\n",
        "'int_rate:9.548-12.025',\n",
        "'int_rate:12.025-15.74',\n",
        "'int_rate:15.74-20.281',\n",
        "'int_rate:>20.281',\n",
        "'mths_since_earliest_cr_line:<140',\n",
        "'mths_since_earliest_cr_line:141-164',\n",
        "'mths_since_earliest_cr_line:165-247',\n",
        "'mths_since_earliest_cr_line:248-270',\n",
        "'mths_since_earliest_cr_line:271-352',\n",
        "'mths_since_earliest_cr_line:>352',\n",
        "'inq_last_6mths:0',\n",
        "'inq_last_6mths:1-2',\n",
        "'inq_last_6mths:3-6',\n",
        "'inq_last_6mths:>6',\n",
        "'acc_now_delinq:0',\n",
        "'acc_now_delinq:>=1',\n",
        "'annual_inc:<20K',\n",
        "'annual_inc:20K-30K',\n",
        "'annual_inc:30K-40K',\n",
        "'annual_inc:40K-50K',\n",
        "'annual_inc:50K-60K',\n",
        "'annual_inc:60K-70K',\n",
        "'annual_inc:70K-80K',\n",
        "'annual_inc:80K-90K',\n",
        "'annual_inc:90K-100K',\n",
        "'annual_inc:100K-120K',\n",
        "'annual_inc:120K-140K',\n",
        "'annual_inc:>140K',\n",
        "'dti:<=1.4',\n",
        "'dti:1.4-3.5',\n",
        "'dti:3.5-7.7',\n",
        "'dti:7.7-10.5',\n",
        "'dti:10.5-16.1',\n",
        "'dti:16.1-20.3',\n",
        "'dti:20.3-21.7',\n",
        "'dti:21.7-22.4',\n",
        "'dti:22.4-35',\n",
        "'dti:>35',\n",
        "'mths_since_last_delinq:Missing',\n",
        "'mths_since_last_delinq:0-3',\n",
        "'mths_since_last_delinq:4-30',\n",
        "'mths_since_last_delinq:31-56',\n",
        "'mths_since_last_delinq:>=57',\n",
        "'mths_since_last_record:Missing',\n",
        "'mths_since_last_record:0-2',\n",
        "'mths_since_last_record:3-20',\n",
        "'mths_since_last_record:21-31',\n",
        "'mths_since_last_record:32-80',\n",
        "'mths_since_last_record:81-86',\n",
        "'mths_since_last_record:>=86',\n",
        "]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7GvDeKrJr6I"
      },
      "outputs": [],
      "source": [
        "inputs_train_with_ref_cat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WnqAAGuJs1b"
      },
      "outputs": [],
      "source": [
        "inputs_2015_with_ref_cat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoQZLPNvJtxK"
      },
      "outputs": [],
      "source": [
        "df_scorecard = pd.read_csv('df_scorecard.csv', index_col = 0)\n",
        "# We import the scorecard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPHOpNFkJuxI"
      },
      "outputs": [],
      "source": [
        "df_scorecard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T90z6q7MJwQv"
      },
      "outputs": [],
      "source": [
        "inputs_train_with_ref_cat_w_intercept = inputs_train_with_ref_cat\n",
        "inputs_train_with_ref_cat_w_intercept.insert(0, 'Intercept', 1)\n",
        "# We insert a column in the dataframe, with an index of 0, that is, in the beginning of the dataframe.\n",
        "# The name of that column is 'Intercept', and its values are 1s.\n",
        "inputs_train_with_ref_cat_w_intercept = inputs_train_with_ref_cat_w_intercept[df_scorecard['Feature name'].values]\n",
        "# Here, from the 'inputs_train_with_ref_cat_w_intercept' dataframe, we keep only the columns with column names,\n",
        "# exactly equal to the row values of the 'Feature name' column from the 'df_scorecard' dataframe.\n",
        "inputs_train_with_ref_cat_w_intercept.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sTZ7jrGJxte"
      },
      "outputs": [],
      "source": [
        "inputs_2015_with_ref_cat_w_intercept = inputs_2015_with_ref_cat\n",
        "inputs_2015_with_ref_cat_w_intercept.insert(0, 'Intercept', 1)\n",
        "# We insert a column in the dataframe, with an index of 0, that is, in the beginning of the dataframe.\n",
        "# The name of that column is 'Intercept', and its values are 1s.\n",
        "inputs_2015_with_ref_cat_w_intercept = inputs_2015_with_ref_cat_w_intercept[df_scorecard['Feature name'].values]\n",
        "# Here, from the 'inputs_train_with_ref_cat_w_intercept' dataframe, we keep only the columns with column names,\n",
        "# exactly equal to the row values of the 'Feature name' column from the 'df_scorecard' dataframe.\n",
        "inputs_2015_with_ref_cat_w_intercept.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSEvH8jQJzFg"
      },
      "outputs": [],
      "source": [
        "scorecard_scores = df_scorecard['Score - Final']\n",
        "scorecard_scores = scorecard_scores.values.reshape(102, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3uB5Wq7J7ut"
      },
      "outputs": [],
      "source": [
        "y_scores_train = inputs_train_with_ref_cat_w_intercept.dot(scorecard_scores)\n",
        "# Here we multiply the values of each row of the dataframe by the values of each column of the variable,\n",
        "# which is an argument of the 'dot' method, and sum them. It's essentially the sum of the products.\n",
        "y_scores_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYmvIVNgJ822"
      },
      "outputs": [],
      "source": [
        "y_scores_2015 = inputs_2015_with_ref_cat_w_intercept.dot(scorecard_scores)\n",
        "# Here we multiply the values of each row of the dataframe by the values of each column of the variable,\n",
        "# which is an argument of the 'dot' method, and sum them. It's essentially the sum of the products.\n",
        "y_scores_2015.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXT0k8YgJ-QH"
      },
      "outputs": [],
      "source": [
        "inputs_train_with_ref_cat_w_intercept = pd.concat([inputs_train_with_ref_cat_w_intercept, y_scores_train], axis = 1)\n",
        "inputs_2015_with_ref_cat_w_intercept = pd.concat([inputs_2015_with_ref_cat_w_intercept, y_scores_2015], axis = 1)\n",
        "# Here we concatenate the scores we calculated with the rest of the variables in the two dataframes:\n",
        "# the one with old (\"expected\") data and the one with new (\"actual\") data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PE90JkxOJ_X6"
      },
      "outputs": [],
      "source": [
        "inputs_train_with_ref_cat_w_intercept.columns.values[inputs_train_with_ref_cat_w_intercept.shape[1] - 1] = 'Score'\n",
        "inputs_2015_with_ref_cat_w_intercept.columns.values[inputs_2015_with_ref_cat_w_intercept.shape[1] - 1] = 'Score'\n",
        "# Here we rename the columns containing scores to \"Score\" in both dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjmmZKSwKAmQ"
      },
      "outputs": [],
      "source": [
        "inputs_2015_with_ref_cat_w_intercept.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTZ-e_27KBlE"
      },
      "outputs": [],
      "source": [
        "inputs_train_with_ref_cat_w_intercept['Score:300-350'] = np.where((inputs_train_with_ref_cat_w_intercept['Score'] >= 300) & (inputs_train_with_ref_cat_w_intercept['Score'] < 350), 1, 0)\n",
        "inputs_train_with_ref_cat_w_intercept['Score:350-400'] = np.where((inputs_train_with_ref_cat_w_intercept['Score'] >= 350) & (inputs_train_with_ref_cat_w_intercept['Score'] < 400), 1, 0)\n",
        "inputs_train_with_ref_cat_w_intercept['Score:400-450'] = np.where((inputs_train_with_ref_cat_w_intercept['Score'] >= 400) & (inputs_train_with_ref_cat_w_intercept['Score'] < 450), 1, 0)\n",
        "inputs_train_with_ref_cat_w_intercept['Score:450-500'] = np.where((inputs_train_with_ref_cat_w_intercept['Score'] >= 450) & (inputs_train_with_ref_cat_w_intercept['Score'] < 500), 1, 0)\n",
        "inputs_train_with_ref_cat_w_intercept['Score:500-550'] = np.where((inputs_train_with_ref_cat_w_intercept['Score'] >= 500) & (inputs_train_with_ref_cat_w_intercept['Score'] < 550), 1, 0)\n",
        "inputs_train_with_ref_cat_w_intercept['Score:550-600'] = np.where((inputs_train_with_ref_cat_w_intercept['Score'] >= 550) & (inputs_train_with_ref_cat_w_intercept['Score'] < 600), 1, 0)\n",
        "inputs_train_with_ref_cat_w_intercept['Score:600-650'] = np.where((inputs_train_with_ref_cat_w_intercept['Score'] >= 600) & (inputs_train_with_ref_cat_w_intercept['Score'] < 650), 1, 0)\n",
        "inputs_train_with_ref_cat_w_intercept['Score:650-700'] = np.where((inputs_train_with_ref_cat_w_intercept['Score'] >= 650) & (inputs_train_with_ref_cat_w_intercept['Score'] < 700), 1, 0)\n",
        "inputs_train_with_ref_cat_w_intercept['Score:700-750'] = np.where((inputs_train_with_ref_cat_w_intercept['Score'] >= 700) & (inputs_train_with_ref_cat_w_intercept['Score'] < 750), 1, 0)\n",
        "inputs_train_with_ref_cat_w_intercept['Score:750-800'] = np.where((inputs_train_with_ref_cat_w_intercept['Score'] >= 750) & (inputs_train_with_ref_cat_w_intercept['Score'] < 800), 1, 0)\n",
        "inputs_train_with_ref_cat_w_intercept['Score:800-850'] = np.where((inputs_train_with_ref_cat_w_intercept['Score'] >= 800) & (inputs_train_with_ref_cat_w_intercept['Score'] <= 850), 1, 0)\n",
        "# We create dummy variables for score intervals in the dataframe with old (\"expected\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUpSjCTlKDSJ"
      },
      "outputs": [],
      "source": [
        "inputs_2015_with_ref_cat_w_intercept['Score:300-350'] = np.where((inputs_2015_with_ref_cat_w_intercept['Score'] >= 300) & (inputs_2015_with_ref_cat_w_intercept['Score'] < 350), 1, 0)\n",
        "inputs_2015_with_ref_cat_w_intercept['Score:350-400'] = np.where((inputs_2015_with_ref_cat_w_intercept['Score'] >= 350) & (inputs_2015_with_ref_cat_w_intercept['Score'] < 400), 1, 0)\n",
        "inputs_2015_with_ref_cat_w_intercept['Score:400-450'] = np.where((inputs_2015_with_ref_cat_w_intercept['Score'] >= 400) & (inputs_2015_with_ref_cat_w_intercept['Score'] < 450), 1, 0)\n",
        "inputs_2015_with_ref_cat_w_intercept['Score:450-500'] = np.where((inputs_2015_with_ref_cat_w_intercept['Score'] >= 450) & (inputs_2015_with_ref_cat_w_intercept['Score'] < 500), 1, 0)\n",
        "inputs_2015_with_ref_cat_w_intercept['Score:500-550'] = np.where((inputs_2015_with_ref_cat_w_intercept['Score'] >= 500) & (inputs_2015_with_ref_cat_w_intercept['Score'] < 550), 1, 0)\n",
        "inputs_2015_with_ref_cat_w_intercept['Score:550-600'] = np.where((inputs_2015_with_ref_cat_w_intercept['Score'] >= 550) & (inputs_2015_with_ref_cat_w_intercept['Score'] < 600), 1, 0)\n",
        "inputs_2015_with_ref_cat_w_intercept['Score:600-650'] = np.where((inputs_2015_with_ref_cat_w_intercept['Score'] >= 600) & (inputs_2015_with_ref_cat_w_intercept['Score'] < 650), 1, 0)\n",
        "inputs_2015_with_ref_cat_w_intercept['Score:650-700'] = np.where((inputs_2015_with_ref_cat_w_intercept['Score'] >= 650) & (inputs_2015_with_ref_cat_w_intercept['Score'] < 700), 1, 0)\n",
        "inputs_2015_with_ref_cat_w_intercept['Score:700-750'] = np.where((inputs_2015_with_ref_cat_w_intercept['Score'] >= 700) & (inputs_2015_with_ref_cat_w_intercept['Score'] < 750), 1, 0)\n",
        "inputs_2015_with_ref_cat_w_intercept['Score:750-800'] = np.where((inputs_2015_with_ref_cat_w_intercept['Score'] >= 750) & (inputs_2015_with_ref_cat_w_intercept['Score'] < 800), 1, 0)\n",
        "inputs_2015_with_ref_cat_w_intercept['Score:800-850'] = np.where((inputs_2015_with_ref_cat_w_intercept['Score'] >= 800) & (inputs_2015_with_ref_cat_w_intercept['Score'] <= 850), 1, 0)\n",
        "# We create dummy variables for score intervals in the dataframe with new (\"actual\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq-Tbt5GKF5G"
      },
      "source": [
        "# Population Stability Index: Calculation and Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCArtiZgKG4o"
      },
      "outputs": [],
      "source": [
        "PSI_calc_train = inputs_train_with_ref_cat_w_intercept.sum() / inputs_train_with_ref_cat_w_intercept.shape[0]\n",
        "# We create a dataframe with proportions of observations for each dummy variable for the old (\"expected\") data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcGXJx-7KKyt"
      },
      "outputs": [],
      "source": [
        "PSI_calc_2015 = inputs_2015_with_ref_cat_w_intercept.sum() / inputs_2015_with_ref_cat_w_intercept.shape[0]\n",
        "# We create a dataframe with proportions of observations for each dummy variable for the new (\"actual\") data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zzfrh4MKL9t"
      },
      "outputs": [],
      "source": [
        "PSI_calc = pd.concat([PSI_calc_train, PSI_calc_2015], axis = 1)\n",
        "# We concatenate the two dataframes along the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZz5NYFaKM_B"
      },
      "outputs": [],
      "source": [
        "PSI_calc = PSI_calc.reset_index()\n",
        "# We reset the index of the dataframe. The index becomes from 0 to the total number of rows less one.\n",
        "# The old index, which is the dummy variable name, becomes a column, named 'index'.\n",
        "PSI_calc['Original feature name'] = PSI_calc['index'].str.split(':').str[0]\n",
        "# We create a new column, called 'Original feature name', which contains the value of the 'Feature name' column,\n",
        "# up to the column symbol.\n",
        "PSI_calc.columns = ['index', 'Proportions_Train', 'Proportions_New', 'Original feature name']\n",
        "# We change the names of the columns of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ9KZZPeKOUs"
      },
      "outputs": [],
      "source": [
        "PSI_calc = PSI_calc[np.array(['index', 'Original feature name', 'Proportions_Train', 'Proportions_New'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY3bZ2ENKPy2"
      },
      "outputs": [],
      "source": [
        "PSI_calc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIBsiXDlKRHV"
      },
      "outputs": [],
      "source": [
        "PSI_calc = PSI_calc[(PSI_calc['index'] != 'Intercept') & (PSI_calc['index'] != 'Score')]\n",
        "# We remove the rows with values in the 'index' column 'Intercept' and 'Score'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnGNcQZ-KSRo"
      },
      "outputs": [],
      "source": [
        "PSI_calc['Contribution'] = np.where((PSI_calc['Proportions_Train'] == 0) | (PSI_calc['Proportions_New'] == 0), 0, (PSI_calc['Proportions_New'] - PSI_calc['Proportions_Train']) * np.log(PSI_calc['Proportions_New'] / PSI_calc['Proportions_Train']))\n",
        "# We calculate the contribution of each dummy variable to the PSI of each original variable it comes from.\n",
        "# If either the proportion of old data or the proportion of new data are 0, the contribution is 0.\n",
        "# Otherwise, we apply the PSI formula for each contribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY7JKBPRKTYY"
      },
      "outputs": [],
      "source": [
        "PSI_calc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTns4skxKUjQ"
      },
      "outputs": [],
      "source": [
        "PSI_calc.groupby('Original feature name')['Contribution'].sum()\n",
        "# Finally, we sum all contributions for each original independent variable and the 'Score' variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXgMalehLcqP"
      },
      "source": [
        "LGD & EAD MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yomgwojK-_Z"
      },
      "outputs": [],
      "source": [
        "# Import data.\n",
        "loan_data_preprocessed_backup = pd.read_csv('loan_data_2007_2014_preprocessed.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UG6nrI-LAhi"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed = loan_data_preprocessed_backup.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjuAKQzFLBuK"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed.columns.values\n",
        "# Displays all column names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqsMR8UqLCzq"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEn0JiflLDyg"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhpBOwaELFWb"
      },
      "outputs": [],
      "source": [
        "loan_data_defaults = loan_data_preprocessed[loan_data_preprocessed['loan_status'].isin(['Charged Off','Does not meet the credit policy. Status:Charged Off'])]\n",
        "# Here we take only the accounts that were charged-off (written-off)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDLHwxpULGZ2"
      },
      "outputs": [],
      "source": [
        "loan_data_defaults.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJVAJHeULHZ2"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_rows = None\n",
        "# Sets the pandas dataframe options to display all columns/ rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWlyclljLIaq"
      },
      "outputs": [],
      "source": [
        "loan_data_defaults.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HlMlNEmLNkw"
      },
      "source": [
        "# Independent Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e622TTpfLPU-"
      },
      "outputs": [],
      "source": [
        "loan_data_defaults['mths_since_last_delinq'].fillna(0, inplace = True)\n",
        "# We fill the missing values with zeroes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceA4FgA6LQh1"
      },
      "outputs": [],
      "source": [
        "#loan_data_defaults['mths_since_last_delinq'].fillna(loan_data_defaults['mths_since_last_delinq'].max() + 12, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJASTL3oLRzJ"
      },
      "outputs": [],
      "source": [
        "loan_data_defaults['mths_since_last_record'].fillna(0, inplace=True)\n",
        "# We fill the missing values with zeroes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oggko-ooMBDQ"
      },
      "source": [
        "# Dependent Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vtpftsLMCsj"
      },
      "outputs": [],
      "source": [
        "loan_data_defaults['recovery_rate'] = loan_data_defaults['recoveries'] / loan_data_defaults['funded_amnt']\n",
        "# We calculate the dependent variable for the LGD model: recovery rate.\n",
        "# It is the ratio of recoveries and funded amount."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnr200yWMEfw"
      },
      "outputs": [],
      "source": [
        "loan_data_defaults['recovery_rate'].describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgP4VdjtMFh5"
      },
      "outputs": [],
      "source": [
        "loan_data_defaults['recovery_rate'] = np.where(loan_data_defaults['recovery_rate'] > 1, 1, loan_data_defaults['recovery_rate'])\n",
        "loan_data_defaults['recovery_rate'] = np.where(loan_data_defaults['recovery_rate'] < 0, 0, loan_data_defaults['recovery_rate'])\n",
        "# We set recovery rates that are greater than 1 to 1 and recovery rates that are less than 0 to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3Btd0SwMGiL"
      },
      "outputs": [],
      "source": [
        "loan_data_defaults['recovery_rate'].describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTYM6v5mMIKX"
      },
      "outputs": [],
      "source": [
        "loan_data_defaults['CCF'] = (loan_data_defaults['funded_amnt'] - loan_data_defaults['total_rec_prncp']) / loan_data_defaults['funded_amnt']\n",
        "# We calculate the dependent variable for the EAD model: credit conversion factor.\n",
        "# It is the ratio of the difference of the amount used at the moment of default to the total funded amount."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9RUmaIPMJPc"
      },
      "outputs": [],
      "source": [
        "loan_data_defaults['CCF'].describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWhZXRmDMLZv"
      },
      "outputs": [],
      "source": [
        "loan_data_defaults.to_csv('loan_data_defaults.csv')\n",
        "# We save the data to a CSV file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ffuvsPzMqmR"
      },
      "source": [
        "# Explore Dependent Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2nfUjbBMs5_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ8CfhbFMv3Y"
      },
      "outputs": [],
      "source": [
        "plt.hist(loan_data_defaults['recovery_rate'], bins = 100)\n",
        "# We plot a histogram of a variable with 100 bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEpCUA_wMy39"
      },
      "outputs": [],
      "source": [
        "plt.hist(loan_data_defaults['recovery_rate'], bins = 50)\n",
        "# We plot a histogram of a variable with 50 bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nnsvge77Mzv3"
      },
      "outputs": [],
      "source": [
        "plt.hist(loan_data_defaults['CCF'], bins = 100)\n",
        "# We plot a histogram of a variable with 100 bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VKBl5ICM06X"
      },
      "outputs": [],
      "source": [
        "loan_data_defaults['recovery_rate_0_1'] = np.where(loan_data_defaults['recovery_rate'] == 0, 0, 1)\n",
        "# We create a new variable which is 0 if recovery rate is 0 and 1 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPb_YzbBM181"
      },
      "outputs": [],
      "source": [
        "loan_data_defaults['recovery_rate_0_1']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BFuywFNM3iA"
      },
      "source": [
        "# LGD Model\n",
        "### Splitting Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7Kf7LuPM79O"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IahSFKSzM9l3"
      },
      "outputs": [],
      "source": [
        "# LGD model stage 1 datasets: recovery rate 0 or greater than 0.\n",
        "lgd_inputs_stage_1_train, lgd_inputs_stage_1_test, lgd_targets_stage_1_train, lgd_targets_stage_1_test = train_test_split(loan_data_defaults.drop(['good_bad', 'recovery_rate','recovery_rate_0_1', 'CCF'], axis = 1), loan_data_defaults['recovery_rate_0_1'], test_size = 0.2, random_state = 42)\n",
        "# Takes a set of inputs and a set of targets as arguments. Splits the inputs and the targets into four dataframes:\n",
        "# Inputs - Train, Inputs - Test, Targets - Train, Targets - Test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7tsR5dGM_za"
      },
      "source": [
        "### Preparing the Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEslXDHQNA56"
      },
      "outputs": [],
      "source": [
        "features_all = ['grade:A',\n",
        "'grade:B',\n",
        "'grade:C',\n",
        "'grade:D',\n",
        "'grade:E',\n",
        "'grade:F',\n",
        "'grade:G',\n",
        "'home_ownership:MORTGAGE',\n",
        "'home_ownership:NONE',\n",
        "'home_ownership:OTHER',\n",
        "'home_ownership:OWN',\n",
        "'home_ownership:RENT',\n",
        "'verification_status:Not Verified',\n",
        "'verification_status:Source Verified',\n",
        "'verification_status:Verified',\n",
        "'purpose:car',\n",
        "'purpose:credit_card',\n",
        "'purpose:debt_consolidation',\n",
        "'purpose:educational',\n",
        "'purpose:home_improvement',\n",
        "'purpose:house',\n",
        "'purpose:major_purchase',\n",
        "'purpose:medical',\n",
        "'purpose:moving',\n",
        "'purpose:other',\n",
        "'purpose:renewable_energy',\n",
        "'purpose:small_business',\n",
        "'purpose:vacation',\n",
        "'purpose:wedding',\n",
        "'initial_list_status:f',\n",
        "'initial_list_status:w',\n",
        "'term_int',\n",
        "'emp_length_int',\n",
        "'mths_since_issue_d',\n",
        "'mths_since_earliest_cr_line',\n",
        "'funded_amnt',\n",
        "'int_rate',\n",
        "'installment',\n",
        "'annual_inc',\n",
        "'dti',\n",
        "'delinq_2yrs',\n",
        "'inq_last_6mths',\n",
        "'mths_since_last_delinq',\n",
        "'mths_since_last_record',\n",
        "'open_acc',\n",
        "'pub_rec',\n",
        "'total_acc',\n",
        "'acc_now_delinq',\n",
        "'total_rev_hi_lim']\n",
        "# List of all independent variables for the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVQw-g2MNBFo"
      },
      "outputs": [],
      "source": [
        "features_reference_cat = ['grade:G',\n",
        "'home_ownership:RENT',\n",
        "'verification_status:Verified',\n",
        "'purpose:credit_card',\n",
        "'initial_list_status:f']\n",
        "# List of the dummy variable reference categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8FiAfvbND07"
      },
      "outputs": [],
      "source": [
        "lgd_inputs_stage_1_train = lgd_inputs_stage_1_train[features_all]\n",
        "# Here we keep only the variables we need for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXosE14rNE1K"
      },
      "outputs": [],
      "source": [
        "lgd_inputs_stage_1_train = lgd_inputs_stage_1_train.drop(features_reference_cat, axis = 1)\n",
        "# Here we remove the dummy variable reference categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7dSHK3gNF7v"
      },
      "outputs": [],
      "source": [
        "lgd_inputs_stage_1_train.isnull().sum()\n",
        "# Check for missing values. We check whether the value of each row for each column is missing or not,\n",
        "# then sum accross columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWKq_-wWNIkg"
      },
      "outputs": [],
      "source": [
        "### Estimating the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb5mF70iNKM5"
      },
      "outputs": [],
      "source": [
        "# P values for sklearn logistic regression.\n",
        "\n",
        "# Class to display p-values for logistic regression in sklearn.\n",
        "\n",
        "from sklearn import linear_model\n",
        "import scipy.stats as stat\n",
        "\n",
        "class LogisticRegression_with_p_values:\n",
        "\n",
        "    def __init__(self,*args,**kwargs):#,**kwargs):\n",
        "        self.model = linear_model.LogisticRegression(*args,**kwargs)#,**args)\n",
        "\n",
        "    def fit(self,X,y):\n",
        "        self.model.fit(X,y)\n",
        "\n",
        "        #### Get p-values for the fitted model ####\n",
        "        denom = (2.0 * (1.0 + np.cosh(self.model.decision_function(X))))\n",
        "        denom = np.tile(denom,(X.shape[1],1)).T\n",
        "        F_ij = np.dot((X / denom).T,X) ## Fisher Information Matrix\n",
        "        Cramer_Rao = np.linalg.inv(F_ij) ## Inverse Information Matrix\n",
        "        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n",
        "        z_scores = self.model.coef_[0] / sigma_estimates # z-score for eaach model coefficient\n",
        "        p_values = [stat.norm.sf(abs(x)) * 2 for x in z_scores] ### two tailed test for p-values\n",
        "\n",
        "        self.coef_ = self.model.coef_\n",
        "        self.intercept_ = self.model.intercept_\n",
        "        #self.z_scores = z_scores\n",
        "        self.p_values = p_values\n",
        "        #self.sigma_estimates = sigma_estimates\n",
        "        #self.F_ij = F_ij"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BPgXY7RNLvA"
      },
      "outputs": [],
      "source": [
        "reg_lgd_st_1 = LogisticRegression_with_p_values()\n",
        "# We create an instance of an object from the 'LogisticRegression' class.\n",
        "reg_lgd_st_1.fit(lgd_inputs_stage_1_train, lgd_targets_stage_1_train)\n",
        "# Estimates the coefficients of the object from the 'LogisticRegression' class\n",
        "# with inputs (independent variables) contained in the first dataframe\n",
        "# and targets (dependent variables) contained in the second dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGUUSucANMy6"
      },
      "outputs": [],
      "source": [
        "feature_name = lgd_inputs_stage_1_train.columns.values\n",
        "# Stores the names of the columns of a dataframe in a variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8Uu0jduNNz3"
      },
      "outputs": [],
      "source": [
        "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
        "# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\n",
        "summary_table['Coefficients'] = np.transpose(reg_lgd_st_1.coef_)\n",
        "# Creates a new column in the dataframe, called 'Coefficients',\n",
        "# with row values the transposed coefficients from the 'LogisticRegression' object.\n",
        "summary_table.index = summary_table.index + 1\n",
        "# Increases the index of every row of the dataframe with 1.\n",
        "summary_table.loc[0] = ['Intercept', reg_lgd_st_1.intercept_[0]]\n",
        "# Assigns values of the row with index 0 of the dataframe.\n",
        "summary_table = summary_table.sort_index()\n",
        "# Sorts the dataframe by index.\n",
        "p_values = reg_lgd_st_1.p_values\n",
        "# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\n",
        "p_values = np.append(np.nan,np.array(p_values))\n",
        "# We add the value 'NaN' in the beginning of the variable with p-values.\n",
        "summary_table['p_values'] = p_values\n",
        "# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\n",
        "summary_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kORyUz9NPS9"
      },
      "outputs": [],
      "source": [
        "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
        "summary_table['Coefficients'] = np.transpose(reg_lgd_st_1.coef_)\n",
        "summary_table.index = summary_table.index + 1\n",
        "summary_table.loc[0] = ['Intercept', reg_lgd_st_1.intercept_[0]]\n",
        "summary_table = summary_table.sort_index()\n",
        "p_values = reg_lgd_st_1.p_values\n",
        "p_values = np.append(np.nan,np.array(p_values))\n",
        "summary_table['p_values'] = p_values\n",
        "summary_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaEEZVSuNR3P"
      },
      "source": [
        "### Testing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Plie3biJOkIx"
      },
      "outputs": [],
      "source": [
        "lgd_inputs_stage_1_test = lgd_inputs_stage_1_test[features_all]\n",
        "# Here we keep only the variables we need for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctQKMXCrOljd"
      },
      "outputs": [],
      "source": [
        "lgd_inputs_stage_1_test = lgd_inputs_stage_1_test.drop(features_reference_cat, axis = 1)\n",
        "# Here we remove the dummy variable reference categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Qe2Db4POmqM"
      },
      "outputs": [],
      "source": [
        "y_hat_test_lgd_stage_1 = reg_lgd_st_1.model.predict(lgd_inputs_stage_1_test)\n",
        "# Calculates the predicted values for the dependent variable (targets)\n",
        "# based on the values of the independent variables (inputs) supplied as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDlE2FTYOoPp"
      },
      "outputs": [],
      "source": [
        "y_hat_test_lgd_stage_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxXQD56YOqbM"
      },
      "outputs": [],
      "source": [
        "y_hat_test_proba_lgd_stage_1 = reg_lgd_st_1.model.predict_proba(lgd_inputs_stage_1_test)\n",
        "# Calculates the predicted probability values for the dependent variable (targets)\n",
        "# based on the values of the independent variables (inputs) supplied as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZlAATtWOqR3"
      },
      "outputs": [],
      "source": [
        "y_hat_test_proba_lgd_stage_1\n",
        "# This is an array of arrays of predicted class probabilities for all classes.\n",
        "# In this case, the first value of every sub-array is the probability for the observation to belong to the first class, i.e. 0,\n",
        "# and the second value is the probability for the observation to belong to the first class, i.e. 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-t9WoeuOvkp"
      },
      "outputs": [],
      "source": [
        "y_hat_test_proba_lgd_stage_1 = y_hat_test_proba_lgd_stage_1[: ][: , 1]\n",
        "# Here we take all the arrays in the array, and from each array, we take all rows, and only the element with index 1,\n",
        "# that is, the second element.\n",
        "# In other words, we take only the probabilities for being 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zelk3BfSOzD5"
      },
      "outputs": [],
      "source": [
        "y_hat_test_proba_lgd_stage_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvfMrlUAOxYM"
      },
      "outputs": [],
      "source": [
        "lgd_targets_stage_1_test_temp = lgd_targets_stage_1_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH5MPiB0O4SY"
      },
      "outputs": [],
      "source": [
        "lgd_targets_stage_1_test_temp.reset_index(drop = True, inplace = True)\n",
        "# We reset the index of a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig1EC0F8O5pq"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs = pd.concat([lgd_targets_stage_1_test_temp, pd.DataFrame(y_hat_test_proba_lgd_stage_1)], axis = 1)\n",
        "# Concatenates two dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEMKjAfGO7dL"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs.columns = ['lgd_targets_stage_1_test', 'y_hat_test_proba_lgd_stage_1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jbpmhiGPAq2"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs.index = lgd_inputs_stage_1_test.index\n",
        "# Makes the index of one dataframe equal to the index of another dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOTAstEGPB6x"
      },
      "outputs": [],
      "source": [
        "df_actual_predicted_probs.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHm2q_FIOjpJ"
      },
      "source": [
        "### Estimating the Аccuracy of the Мodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHQBPnbePITU"
      },
      "outputs": [],
      "source": [
        "tr = 0.5\n",
        "# We create a new column with an indicator,\n",
        "# where every observation that has predicted probability greater than the threshold has a value of 1,\n",
        "# and every observation that has predicted probability lower than the threshold has a value of 0.\n",
        "df_actual_predicted_probs['y_hat_test_lgd_stage_1'] = np.where(df_actual_predicted_probs['y_hat_test_proba_lgd_stage_1'] > tr, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY8Yu0Y3PKVF"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_lgd_stage_1'], rownames = ['Actual'], colnames = ['Predicted'])\n",
        "# Creates a cross-table where the actual values are displayed by rows and the predicted values by columns.\n",
        "# This table is known as a Confusion Matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3Q73Gp3PLj7"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_lgd_stage_1'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]\n",
        "# Here we divide each value of the table by the total number of observations,\n",
        "# thus getting percentages, or, rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH7q_8uDPMfm"
      },
      "outputs": [],
      "source": [
        "(pd.crosstab(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_lgd_stage_1'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[0, 0] + (pd.crosstab(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_lgd_stage_1'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[1, 1]\n",
        "# Here we calculate Accuracy of the model, which is the sum of the diagonal rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsCAQlK_PN-Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGXh5FCGPPE_"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = roc_curve(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_proba_lgd_stage_1'])\n",
        "# Returns the Receiver Operating Characteristic (ROC) Curve from a set of actual values and their predicted probabilities.\n",
        "# As a result, we get three arrays: the false positive rates, the true positive rates, and the thresholds.\n",
        "# we store each of the three arrays in a separate variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uoh_3rAcPQC1"
      },
      "outputs": [],
      "source": [
        "plt.plot(fpr, tpr)\n",
        "# We plot the false positive rate along the x-axis and the true positive rate along the y-axis,\n",
        "# thus plotting the ROC curve.\n",
        "plt.plot(fpr, fpr, linestyle = '--', color = 'k')\n",
        "# We plot a seconary diagonal line, with dashed line style and black color.\n",
        "plt.xlabel('False positive rate')\n",
        "# We name the x-axis \"False positive rate\".\n",
        "plt.ylabel('True positive rate')\n",
        "# We name the x-axis \"True positive rate\".\n",
        "plt.title('ROC curve')\n",
        "# We name the graph \"ROC curve\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0rS3F4_PRvV"
      },
      "outputs": [],
      "source": [
        "AUROC = roc_auc_score(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_proba_lgd_stage_1'])\n",
        "# Calculates the Area Under the Receiver Operating Characteristic Curve (AUROC)\n",
        "# from a set of actual values and their predicted probabilities.\n",
        "AUROC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fxVFWTVPUKT"
      },
      "source": [
        "*italicised text*### Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAgDw06YPVgz"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "051Zj9vtPXyQ"
      },
      "outputs": [],
      "source": [
        "pickle.dump(reg_lgd_st_1, open('lgd_model_stage_1.sav', 'wb'))\n",
        "# Here we export our model to a 'SAV' file with file name 'lgd_model_stage_1.sav'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19BZ8n7WPZz3"
      },
      "source": [
        "### Stage 2 – Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J1qT-yVPe64"
      },
      "outputs": [],
      "source": [
        "lgd_stage_2_data = loan_data_defaults[loan_data_defaults['recovery_rate_0_1'] == 1]\n",
        "# Here we take only rows where the original recovery rate variable is greater than one,\n",
        "# i.e. where the indicator variable we created is equal to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NegVlyJAPf-x"
      },
      "outputs": [],
      "source": [
        "# LGD model stage 2 datasets: how much more than 0 is the recovery rate\n",
        "lgd_inputs_stage_2_train, lgd_inputs_stage_2_test, lgd_targets_stage_2_train, lgd_targets_stage_2_test = train_test_split(lgd_stage_2_data.drop(['good_bad', 'recovery_rate','recovery_rate_0_1', 'CCF'], axis = 1), lgd_stage_2_data['recovery_rate'], test_size = 0.2, random_state = 42)\n",
        "# Takes a set of inputs and a set of targets as arguments. Splits the inputs and the targets into four dataframes:\n",
        "# Inputs - Train, Inputs - Test, Targets - Train, Targets - Test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy2eHLKzPhQC"
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCBpPKvYPiZP"
      },
      "outputs": [],
      "source": [
        "# Since the p-values are obtained through certain statistics, we need the 'stat' module from scipy.stats\n",
        "import scipy.stats as stat\n",
        "\n",
        "# Since we are using an object oriented language such as Python, we can simply define our own\n",
        "# LinearRegression class (the same one from sklearn)\n",
        "# By typing the code below we will ovewrite a part of the class with one that includes p-values\n",
        "# Here's the full source code of the ORIGINAL class: https://github.com/scikit-learn/scikit-learn/blob/7b136e9/sklearn/linear_model/base.py#L362\n",
        "\n",
        "\n",
        "class LinearRegression(linear_model.LinearRegression):\n",
        "    \"\"\"\n",
        "    LinearRegression class after sklearn's, but calculate t-statistics\n",
        "    and p-values for model coefficients (betas).\n",
        "    Additional attributes available after .fit()\n",
        "    are `t` and `p` which are of the shape (y.shape[1], X.shape[1])\n",
        "    which is (n_features, n_coefs)\n",
        "    This class sets the intercept to 0 by default, since usually we include it\n",
        "    in X.\n",
        "    \"\"\"\n",
        "\n",
        "    # nothing changes in __init__\n",
        "    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n",
        "                 n_jobs=1):\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.normalize = normalize\n",
        "        self.copy_X = copy_X\n",
        "        self.n_jobs = n_jobs\n",
        "\n",
        "\n",
        "    def fit(self, X, y, n_jobs=1):\n",
        "        self = super(LinearRegression, self).fit(X, y, n_jobs)\n",
        "\n",
        "        # Calculate SSE (sum of squared errors)\n",
        "        # and SE (standard error)\n",
        "        sse = np.sum((self.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n",
        "        se = np.array([np.sqrt(np.diagonal(sse * np.linalg.inv(np.dot(X.T, X))))])\n",
        "\n",
        "        # compute the t-statistic for each feature\n",
        "        self.t = self.coef_ / se\n",
        "        # find the p-value for each feature\n",
        "        self.p = np.squeeze(2 * (1 - stat.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1])))\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lc0laZjuPj74"
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stat\n",
        "\n",
        "class LinearRegression(linear_model.LinearRegression):\n",
        "    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n",
        "                 n_jobs=1):\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.normalize = normalize\n",
        "        self.copy_X = copy_X\n",
        "        self.n_jobs = n_jobs\n",
        "    def fit(self, X, y, n_jobs=1):\n",
        "        self = super(LinearRegression, self).fit(X, y, n_jobs)\n",
        "        sse = np.sum((self.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n",
        "        se = np.array([np.sqrt(np.diagonal(sse * np.linalg.inv(np.dot(X.T, X))))])\n",
        "        self.t = self.coef_ / se\n",
        "        self.p = np.squeeze(2 * (1 - stat.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1])))\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJf7Eno-PlkO"
      },
      "outputs": [],
      "source": [
        "lgd_inputs_stage_2_train = lgd_inputs_stage_2_train[features_all]\n",
        "# Here we keep only the variables we need for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxXt9dnfPnCD"
      },
      "outputs": [],
      "source": [
        "lgd_inputs_stage_2_train = lgd_inputs_stage_2_train.drop(features_reference_cat, axis = 1)\n",
        "# Here we remove the dummy variable reference categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0NdKr3xPn9K"
      },
      "outputs": [],
      "source": [
        "reg_lgd_st_2 = LinearRegression()\n",
        "# We create an instance of an object from the 'LogisticRegression' class.\n",
        "reg_lgd_st_2.fit(lgd_inputs_stage_2_train, lgd_targets_stage_2_train)\n",
        "# Estimates the coefficients of the object from the 'LogisticRegression' class\n",
        "# with inputs (independent variables) contained in the first dataframe\n",
        "# and targets (dependent variables) contained in the second dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vFBTxsvPqe9"
      },
      "outputs": [],
      "source": [
        "feature_name = lgd_inputs_stage_2_train.columns.values\n",
        "# Stores the names of the columns of a dataframe in a variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRxnevSeProp"
      },
      "outputs": [],
      "source": [
        "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
        "# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\n",
        "summary_table['Coefficients'] = np.transpose(reg_lgd_st_2.coef_)\n",
        "# Creates a new column in the dataframe, called 'Coefficients',\n",
        "# with row values the transposed coefficients from the 'LogisticRegression' object.\n",
        "summary_table.index = summary_table.index + 1\n",
        "# Increases the index of every row of the dataframe with 1.\n",
        "summary_table.loc[0] = ['Intercept', reg_lgd_st_2.intercept_]\n",
        "# Assigns values of the row with index 0 of the dataframe.\n",
        "summary_table = summary_table.sort_index()\n",
        "# Sorts the dataframe by index.\n",
        "p_values = reg_lgd_st_2.p\n",
        "# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\n",
        "p_values = np.append(np.nan,np.array(p_values))\n",
        "# We add the value 'NaN' in the beginning of the variable with p-values.\n",
        "summary_table['p_values'] = p_values.round(3)\n",
        "# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\n",
        "summary_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcExPkQCPrc4"
      },
      "outputs": [],
      "source": [
        "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
        "summary_table['Coefficients'] = np.transpose(reg_lgd_st_2.coef_)\n",
        "summary_table.index = summary_table.index + 1\n",
        "summary_table.loc[0] = ['Intercept', reg_lgd_st_2.intercept_]\n",
        "summary_table = summary_table.sort_index()\n",
        "p_values = reg_lgd_st_2.p\n",
        "p_values = np.append(np.nan,np.array(p_values))\n",
        "summary_table['p_values'] = p_values.round(3)\n",
        "summary_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWaZoE-4Pu5l"
      },
      "source": [
        "### Stage 2 – Linear Regression Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giCbojc8PxcQ"
      },
      "outputs": [],
      "source": [
        "lgd_inputs_stage_2_test = lgd_inputs_stage_2_test[features_all]\n",
        "# Here we keep only the variables we need for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dPZC4H_PyiN"
      },
      "outputs": [],
      "source": [
        "lgd_inputs_stage_2_test = lgd_inputs_stage_2_test.drop(features_reference_cat, axis = 1)\n",
        "# Here we remove the dummy variable reference categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwVAlsQnPzf-"
      },
      "outputs": [],
      "source": [
        "lgd_inputs_stage_2_test.columns.values\n",
        "# Calculates the predicted values for the dependent variable (targets)\n",
        "# based on the values of the independent variables (inputs) supplied as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eclCbPdP0kh"
      },
      "outputs": [],
      "source": [
        "y_hat_test_lgd_stage_2 = reg_lgd_st_2.predict(lgd_inputs_stage_2_test)\n",
        "# Calculates the predicted values for the dependent variable (targets)\n",
        "# based on the values of the independent variables (inputs) supplied as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5ZRYI2aP1ts"
      },
      "outputs": [],
      "source": [
        "lgd_targets_stage_2_test_temp = lgd_targets_stage_2_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ-Hb0aLP3Kn"
      },
      "outputs": [],
      "source": [
        "lgd_targets_stage_2_test_temp = lgd_targets_stage_2_test_temp.reset_index(drop = True)\n",
        "# We reset the index of a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm5S6Cv9P4Mj"
      },
      "outputs": [],
      "source": [
        "pd.concat([lgd_targets_stage_2_test_temp, pd.DataFrame(y_hat_test_lgd_stage_2)], axis = 1).corr()\n",
        "# We calculate the correlation between actual and predicted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D32kjwFeP5n8"
      },
      "outputs": [],
      "source": [
        "sns.distplot(lgd_targets_stage_2_test - y_hat_test_lgd_stage_2)\n",
        "# We plot the distribution of the residuals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyWORXggP6v1"
      },
      "outputs": [],
      "source": [
        "pickle.dump(reg_lgd_st_2, open('lgd_model_stage_2.sav', 'wb'))\n",
        "# Here we export our model to a 'SAV' file with file name 'lgd_model_stage_1.sav'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDMU-y6XP8rF"
      },
      "source": [
        "### Combining Stage 1 and Stage 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKXUJB10P9_1"
      },
      "outputs": [],
      "source": [
        "y_hat_test_lgd_stage_2_all = reg_lgd_st_2.predict(lgd_inputs_stage_1_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mp4OxNjPP_Ro"
      },
      "outputs": [],
      "source": [
        "y_hat_test_lgd_stage_2_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhB6UB6iQAK8"
      },
      "outputs": [],
      "source": [
        "y_hat_test_lgd = y_hat_test_lgd_stage_1 * y_hat_test_lgd_stage_2_all\n",
        "# Here we combine the predictions of the models from the two stages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEhjJaE9QA_I"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(y_hat_test_lgd).describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl6I7dZJQB3s"
      },
      "outputs": [],
      "source": [
        "y_hat_test_lgd = np.where(y_hat_test_lgd < 0, 0, y_hat_test_lgd)\n",
        "y_hat_test_lgd = np.where(y_hat_test_lgd > 1, 1, y_hat_test_lgd)\n",
        "# We set predicted values that are greater than 1 to 1 and predicted values that are less than 0 to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kkFYDJjQC6E"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(y_hat_test_lgd).describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsihj5B9Qh6J"
      },
      "source": [
        "EAD MODEL , EVALUATION , ESTIMATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4Nx938dQnHG"
      },
      "outputs": [],
      "source": [
        "# EAD model datasets\n",
        "ead_inputs_train, ead_inputs_test, ead_targets_train, ead_targets_test = train_test_split(loan_data_defaults.drop(['good_bad', 'recovery_rate','recovery_rate_0_1', 'CCF'], axis = 1), loan_data_defaults['CCF'], test_size = 0.2, random_state = 42)\n",
        "# Takes a set of inputs and a set of targets as arguments. Splits the inputs and the targets into four dataframes:\n",
        "# Inputs - Train, Inputs - Test, Targets - Train, Targets - Test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vm999vAQotU"
      },
      "outputs": [],
      "source": [
        "ead_inputs_train.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvhxOP_4Qpj8"
      },
      "outputs": [],
      "source": [
        "ead_inputs_train = ead_inputs_train[features_all]\n",
        "# Here we keep only the variables we need for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gIeHy2FQsKf"
      },
      "outputs": [],
      "source": [
        "ead_inputs_train = ead_inputs_train.drop(features_reference_cat, axis = 1)\n",
        "# Here we remove the dummy variable reference categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJlpWk1FQtRW"
      },
      "outputs": [],
      "source": [
        "reg_ead = LinearRegression()\n",
        "# We create an instance of an object from the 'LogisticRegression' class.\n",
        "reg_ead.fit(ead_inputs_train, ead_targets_train)\n",
        "# Estimates the coefficients of the object from the 'LogisticRegression' class\n",
        "# with inputs (independent variables) contained in the first dataframe\n",
        "# and targets (dependent variables) contained in the second dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFTUAJw2Qu2K"
      },
      "outputs": [],
      "source": [
        "feature_name = ead_inputs_train.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nGbNjRvQvRJ"
      },
      "outputs": [],
      "source": [
        "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
        "# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\n",
        "summary_table['Coefficients'] = np.transpose(reg_ead.coef_)\n",
        "# Creates a new column in the dataframe, called 'Coefficients',\n",
        "# with row values the transposed coefficients from the 'LogisticRegression' object.\n",
        "summary_table.index = summary_table.index + 1\n",
        "# Increases the index of every row of the dataframe with 1.\n",
        "summary_table.loc[0] = ['Intercept', reg_ead.intercept_]\n",
        "# Assigns values of the row with index 0 of the dataframe.\n",
        "summary_table = summary_table.sort_index()\n",
        "# Sorts the dataframe by index.\n",
        "p_values = reg_ead.p\n",
        "# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\n",
        "p_values = np.append(np.nan,np.array(p_values))\n",
        "# We add the value 'NaN' in the beginning of the variable with p-values.\n",
        "summary_table['p_values'] = p_values\n",
        "# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\n",
        "summary_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Por2iyXcSTIC"
      },
      "outputs": [],
      "source": [
        "### Model Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g1TFrCzSVIe"
      },
      "outputs": [],
      "source": [
        "ead_inputs_test = ead_inputs_test[features_all]\n",
        "# Here we keep only the variables we need for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vq0Y-Bb3SWhY"
      },
      "outputs": [],
      "source": [
        "ead_inputs_test = ead_inputs_test.drop(features_reference_cat, axis = 1)\n",
        "# Here we remove the dummy variable reference categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G7FKbA5SXrL"
      },
      "outputs": [],
      "source": [
        "ead_inputs_test.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_Q6cZvgSY75"
      },
      "outputs": [],
      "source": [
        "y_hat_test_ead = reg_ead.predict(ead_inputs_test)\n",
        "# Calculates the predicted values for the dependent variable (targets)\n",
        "# based on the values of the independent variables (inputs) supplied as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2M2OIcrSaad"
      },
      "outputs": [],
      "source": [
        "ead_targets_test_temp = ead_targets_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8ae3nYvSbuF"
      },
      "outputs": [],
      "source": [
        "ead_targets_test_temp = ead_targets_test_temp.reset_index(drop = True)\n",
        "# We reset the index of a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jH-aQ7uhSdFg"
      },
      "outputs": [],
      "source": [
        "pd.concat([ead_targets_test_temp, pd.DataFrame(y_hat_test_ead)], axis = 1).corr()\n",
        "# We calculate the correlation between actual and predicted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOtsnYWcSeNZ"
      },
      "outputs": [],
      "source": [
        "sns.distplot(ead_targets_test - y_hat_test_ead)\n",
        "# We plot the distribution of the residuals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaFuxzG6SfQ5"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(y_hat_test_ead).describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lXdNdMLSg6n"
      },
      "outputs": [],
      "source": [
        "y_hat_test_ead = np.where(y_hat_test_ead < 0, 0, y_hat_test_ead)\n",
        "y_hat_test_ead = np.where(y_hat_test_ead > 1, 1, y_hat_test_ead)\n",
        "# We set predicted values that are greater than 1 to 1 and predicted values that are less than 0 to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rs7k55d-Sh9j"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(y_hat_test_ead).describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RxNCR9USUpW"
      },
      "source": [
        "#EExpected Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vqs06TR0T5b7"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyesMPH9T6dl"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed['mths_since_last_delinq'].fillna(0, inplace = True)\n",
        "# We fill the missing values with zeroes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq1Em0DgT7f7"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed['mths_since_last_record'].fillna(0, inplace = True)\n",
        "# We fill the missing values with zeroes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFj3Yox3T8oN"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed_lgd_ead = loan_data_preprocessed[features_all]\n",
        "# Here we keep only the variables we need for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPogVY4gT-ET"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed_lgd_ead = loan_data_preprocessed_lgd_ead.drop(features_reference_cat, axis = 1)\n",
        "# Here we remove the dummy variable reference categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkhChDLYT-61"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed['recovery_rate_st_1'] = reg_lgd_st_1.model.predict(loan_data_preprocessed_lgd_ead)\n",
        "# We apply the stage 1 LGD model and calculate predicted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eioAa1LlT_zn"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed['recovery_rate_st_2'] = reg_lgd_st_2.predict(loan_data_preprocessed_lgd_ead)\n",
        "# We apply the stage 2 LGD model and calculate predicted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irZABZoFUAqx"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed['recovery_rate'] = loan_data_preprocessed['recovery_rate_st_1'] * loan_data_preprocessed['recovery_rate_st_2']\n",
        "# We combine the predicted values from the stage 1 predicted model and the stage 2 predicted model\n",
        "# to calculate the final estimated recovery rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R02llsCQUCCM"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed['recovery_rate'] = np.where(loan_data_preprocessed['recovery_rate'] < 0, 0, loan_data_preprocessed['recovery_rate'])\n",
        "loan_data_preprocessed['recovery_rate'] = np.where(loan_data_preprocessed['recovery_rate'] > 1, 1, loan_data_preprocessed['recovery_rate'])\n",
        "# We set estimated recovery rates that are greater than 1 to 1 and  estimated recovery rates that are less than 0 to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuoBbeeIUDXz"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed['LGD'] = 1 - loan_data_preprocessed['recovery_rate']\n",
        "# We calculate estimated LGD. Estimated LGD equals 1 - estimated recovery rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKzdHgEGUEpy"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed['LGD'].describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dneiONL5UF6u"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed['CCF'] = reg_ead.predict(loan_data_preprocessed_lgd_ead)\n",
        "# We apply the EAD model to calculate estimated credit conversion factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwQooMSPUHPR"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed['CCF'] = np.where(loan_data_preprocessed['CCF'] < 0, 0, loan_data_preprocessed['CCF'])\n",
        "loan_data_preprocessed['CCF'] = np.where(loan_data_preprocessed['CCF'] > 1, 1, loan_data_preprocessed['CCF'])\n",
        "# We set estimated CCF that are greater than 1 to 1 and  estimated CCF that are less than 0 to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vv3q-ZqhUIaA"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed['EAD'] = loan_data_preprocessed['CCF'] * loan_data_preprocessed_lgd_ead['funded_amnt']\n",
        "# We calculate estimated EAD. Estimated EAD equals estimated CCF multiplied by funded amount."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px22GZuwUJl-"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed['EAD'].describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeDQJZPoUK2e"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olRq4DYLUL4Y"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_train = pd.read_csv('loan_data_inputs_train.csv')\n",
        "# We import data to apply the PD model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAJT1HZMUNcC"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_test = pd.read_csv('loan_data_inputs_test.csv')\n",
        "# We import data to apply the PD model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiYXLyfnURj0"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_pd = pd.concat([loan_data_inputs_train, loan_data_inputs_test], axis = 0)\n",
        "# We concatenate the two dataframes along the rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj2QxBw1USqN"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_pd.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWr4pI2gUUAs"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_pd.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zI3mC0hUWRp"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_pd = loan_data_inputs_pd.set_index('Unnamed: 0')\n",
        "# We set the index of the dataframe to the values of a specific column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXvcZpGvUXcW"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_pd.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQVfK08BUY16"
      },
      "outputs": [],
      "source": [
        "features_all_pd = ['grade:A',\n",
        "'grade:B',\n",
        "'grade:C',\n",
        "'grade:D',\n",
        "'grade:E',\n",
        "'grade:F',\n",
        "'grade:G',\n",
        "'home_ownership:RENT_OTHER_NONE_ANY',\n",
        "'home_ownership:OWN',\n",
        "'home_ownership:MORTGAGE',\n",
        "'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
        "'addr_state:NM_VA',\n",
        "'addr_state:NY',\n",
        "'addr_state:OK_TN_MO_LA_MD_NC',\n",
        "'addr_state:CA',\n",
        "'addr_state:UT_KY_AZ_NJ',\n",
        "'addr_state:AR_MI_PA_OH_MN',\n",
        "'addr_state:RI_MA_DE_SD_IN',\n",
        "'addr_state:GA_WA_OR',\n",
        "'addr_state:WI_MT',\n",
        "'addr_state:TX',\n",
        "'addr_state:IL_CT',\n",
        "'addr_state:KS_SC_CO_VT_AK_MS',\n",
        "'addr_state:WV_NH_WY_DC_ME_ID',\n",
        "'verification_status:Not Verified',\n",
        "'verification_status:Source Verified',\n",
        "'verification_status:Verified',\n",
        "'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
        "'purpose:credit_card',\n",
        "'purpose:debt_consolidation',\n",
        "'purpose:oth__med__vacation',\n",
        "'purpose:major_purch__car__home_impr',\n",
        "'initial_list_status:f',\n",
        "'initial_list_status:w',\n",
        "'term:36',\n",
        "'term:60',\n",
        "'emp_length:0',\n",
        "'emp_length:1',\n",
        "'emp_length:2-4',\n",
        "'emp_length:5-6',\n",
        "'emp_length:7-9',\n",
        "'emp_length:10',\n",
        "'mths_since_issue_d:<38',\n",
        "'mths_since_issue_d:38-39',\n",
        "'mths_since_issue_d:40-41',\n",
        "'mths_since_issue_d:42-48',\n",
        "'mths_since_issue_d:49-52',\n",
        "'mths_since_issue_d:53-64',\n",
        "'mths_since_issue_d:65-84',\n",
        "'mths_since_issue_d:>84',\n",
        "'int_rate:<9.548',\n",
        "'int_rate:9.548-12.025',\n",
        "'int_rate:12.025-15.74',\n",
        "'int_rate:15.74-20.281',\n",
        "'int_rate:>20.281',\n",
        "'mths_since_earliest_cr_line:<140',\n",
        "'mths_since_earliest_cr_line:141-164',\n",
        "'mths_since_earliest_cr_line:165-247',\n",
        "'mths_since_earliest_cr_line:248-270',\n",
        "'mths_since_earliest_cr_line:271-352',\n",
        "'mths_since_earliest_cr_line:>352',\n",
        "'inq_last_6mths:0',\n",
        "'inq_last_6mths:1-2',\n",
        "'inq_last_6mths:3-6',\n",
        "'inq_last_6mths:>6',\n",
        "'acc_now_delinq:0',\n",
        "'acc_now_delinq:>=1',\n",
        "'annual_inc:<20K',\n",
        "'annual_inc:20K-30K',\n",
        "'annual_inc:30K-40K',\n",
        "'annual_inc:40K-50K',\n",
        "'annual_inc:50K-60K',\n",
        "'annual_inc:60K-70K',\n",
        "'annual_inc:70K-80K',\n",
        "'annual_inc:80K-90K',\n",
        "'annual_inc:90K-100K',\n",
        "'annual_inc:100K-120K',\n",
        "'annual_inc:120K-140K',\n",
        "'annual_inc:>140K',\n",
        "'dti:<=1.4',\n",
        "'dti:1.4-3.5',\n",
        "'dti:3.5-7.7',\n",
        "'dti:7.7-10.5',\n",
        "'dti:10.5-16.1',\n",
        "'dti:16.1-20.3',\n",
        "'dti:20.3-21.7',\n",
        "'dti:21.7-22.4',\n",
        "'dti:22.4-35',\n",
        "'dti:>35',\n",
        "'mths_since_last_delinq:Missing',\n",
        "'mths_since_last_delinq:0-3',\n",
        "'mths_since_last_delinq:4-30',\n",
        "'mths_since_last_delinq:31-56',\n",
        "'mths_since_last_delinq:>=57',\n",
        "'mths_since_last_record:Missing',\n",
        "'mths_since_last_record:0-2',\n",
        "'mths_since_last_record:3-20',\n",
        "'mths_since_last_record:21-31',\n",
        "'mths_since_last_record:32-80',\n",
        "'mths_since_last_record:81-86',\n",
        "'mths_since_last_record:>=86']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-33TZfLUa_n"
      },
      "outputs": [],
      "source": [
        "ref_categories_pd = ['grade:G',\n",
        "'home_ownership:RENT_OTHER_NONE_ANY',\n",
        "'addr_state:ND_NE_IA_NV_FL_HI_AL',\n",
        "'verification_status:Verified',\n",
        "'purpose:educ__sm_b__wedd__ren_en__mov__house',\n",
        "'initial_list_status:f',\n",
        "'term:60',\n",
        "'emp_length:0',\n",
        "'mths_since_issue_d:>84',\n",
        "'int_rate:>20.281',\n",
        "'mths_since_earliest_cr_line:<140',\n",
        "'inq_last_6mths:>6',\n",
        "'acc_now_delinq:0',\n",
        "'annual_inc:<20K',\n",
        "'dti:>35',\n",
        "'mths_since_last_delinq:0-3',\n",
        "'mths_since_last_record:0-2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCMZ5e9wUcih"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_pd_temp = loan_data_inputs_pd[features_all_pd]\n",
        "# Here we keep only the variables we need for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpGVVCZQUeiy"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_pd_temp = loan_data_inputs_pd_temp.drop(ref_categories_pd, axis = 1)\n",
        "# Here we remove the dummy variable reference categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAWBfG2kUgKf"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_pd_temp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdnh5eB_UinD"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu51QoTJUiag"
      },
      "outputs": [],
      "source": [
        "reg_pd = pickle.load(open('pd_model.sav', 'rb'))\n",
        "# We import the PD model, stored in the 'pd_model.sav' file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WscUXvelUlRg"
      },
      "outputs": [],
      "source": [
        "reg_pd.model.predict_proba(loan_data_inputs_pd_temp)[: ][: , 0]\n",
        "# We apply the PD model to caclulate estimated default probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28SLlPYQUmfC"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_pd['PD'] = reg_pd.model.predict_proba(loan_data_inputs_pd_temp)[: ][: , 0]\n",
        "# We apply the PD model to caclulate estimated default probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMzwBcbiUn4V"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_pd['PD'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr7vP19PUqlX"
      },
      "outputs": [],
      "source": [
        "loan_data_inputs_pd['PD'].describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "speo9c7NUqaK"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed_new = pd.concat([loan_data_preprocessed, loan_data_inputs_pd], axis = 1)\n",
        "# We concatenate the dataframes where we calculated LGD and EAD and the dataframe where we calculated PD along the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO4TWXQ4UtBE"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO2lSIEAUs0b"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed_new.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI-bbNOpUvH8"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed_new['EL'] = loan_data_preprocessed_new['PD'] * loan_data_preprocessed_new['LGD'] * loan_data_preprocessed_new['EAD']\n",
        "# We calculate Expected Loss. EL = PD * LGD * EAD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPPGXM80Uw-X"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed_new['EL'].describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnuAwWz9UyOr"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed_new[['funded_amnt', 'PD', 'LGD', 'EAD', 'EL']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLYu3ZRbUz09"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed_new['funded_amnt'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThPu9v1PU1Lz"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed_new['EL'].sum()\n",
        "# Total Expected Loss for all loans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE6ua4jdU26X"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed_new['funded_amnt'].sum()\n",
        "# Total funded amount for all loans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rof8NMHwU4WY"
      },
      "outputs": [],
      "source": [
        "loan_data_preprocessed_new['EL'].sum() / loan_data_preprocessed_new['funded_amnt'].sum()\n",
        "# Total Expected Loss as a proportion of total funded amount for all loans.\n",
        "####\n",
        "####\n",
        "####\n",
        "# THE END."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idBqw9tjUBcl"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}